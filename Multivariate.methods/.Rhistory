cfact2 = cfa(model2, data=coi_dosen2, std.lv=TRUE, auto.cov.lv.x=FALSE)
summary(cfact2, fit.measures = T, standardized = T)
semPaths(cfact2, 'path', 'std', style = 'lisrel',
edge.color = 'black', intercepts = F)
coi_dosen3 = read_excel("DATA - responden COI.clean.xlsx",  sheet = "DOSEN (filter)")
library(readxl)
coi_dosen3 = read_excel("DATA - responden COI.clean.xlsx",  sheet = "DOSEN (filter)")
View(coi_dosen3)
#Chi square
chisq.test(coi_dosen3$`Pengalaman mengikuti MOOCs sebelumnya`, coi_dosen3$Kat_komp)
chisq.test(coi_dosen3$`Pengalaman mengikuti MOOCs sebelumnya`, coi_dosen3$Kat_sos)
chisq.test(coi_dosen3$`Pengalaman mengikuti MOOCs sebelumnya`, coi_dosen3$Kat_komu)
chisq.test(coi_dosen3$`Pengalaman mengikuti MOOCs sebelumnya`, coi_dosen3$Kat_MOOCS)
chisq.test(coi_dosen3$`Pengalaman mengikuti MOOCs sebelumnya`, coi_dosen3$Kat_komp)
kruskal.test(coi_dosen$`Kompetensi Komputer`~coi_dosen3$`Pengalaman mengikuti MOOCs sebelumnya`)
chisq.test(coi_dosen3$`Pengalaman mengikuti MOOCs sebelumnya`, coi_dosen3$Kat_daring)
chisq.test(coi_dosen3$`Pengalaman memfasilitasi pembelajaran daring di MOOCs sebelumnya`, coi_dosen3$Kat_komp)
chisq.test(coi_dosen3$`Pengalaman memfasilitasi pembelajaran daring di MOOCs sebelumnya`, coi_dosen3$Kat_sos)
chisq.test(coi_dosen3$`Pengalaman memfasilitasi pembelajaran daring di MOOCs sebelumnya`, coi_dosen3$Kat_komu)
chisq.test(coi_dosen3$`Pengalaman memfasilitasi pembelajaran daring di MOOCs sebelumnya`, coi_dosen3$Kat_MOOCS)
chisq.test(coi_dosen3$`Pengalaman memfasilitasi pembelajaran daring di MOOCs sebelumnya`, coi_dosen3$Kat_daring)
###multivariate normal distribution###
library(mvtnorm)
library(MVN)
library(JWileymisc)
library(MASS)
library(dplyr)
library(ellipse)
library(rgl)
##Simulating mutivariate normal (MN) distribution based on correlation/covariance matrix, means, and standard deviation
set.seed(100) #setting seed for pseudo-random initiation
#in a case where we have the covariance matrix, it is then become a straightforward method to simulate
#MN distribution. Meanwhile in case we have correlation matrix, we use the previous equation to transform
#correlation matrix into covariance matrix.
#library jwileymisc provide the function to simplify this transformation
v = matrix(c(1,.152,.096,.043,.109,
.152,1,.400,-.016,.297,
.096,.400,1,.092,.382,
.043,-.016,.092,1,.103,
.109,.297,.382,.103,1),5,5) #correlation matrix
sigma = c(.4421,1.0880,8.5073,.4700,1.1249) #vector of standard deviation
cov.matrix = cor2cov(v, sigma) #convert correlation into covariance matrix
#after getting the covariance matrix, we can use mvrnorm to simulate the MN distribution based on
#previous covariance matrix
#in addition we also have to provide the means vector
mu = c(.7337,2.7300,46.9970,2.6002,1.7491)
sim.mn.data = data.frame(mvrnorm(n = 500, Sigma = cov.matrix, mu = mu)) #simulated data with total 500 observations
str(sim.mn.data)
sim.mn.data = rename(sim.mn.data, low = X1, high = X2, throw = X3, dodge = X4, make = X5)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#since we are limited in 3dimensional representation, we are limited to only use until first 3 variables
#of the simulated data from before.
#it is guaranteed that marginal distributions from MN is always normal(see Methods_of_Multivariate_Analysis-_3rd_Edition Rencher & Christensen )
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,3], n = 150) #calculate kernel density estimate
image(sim.mn.kernel) #heatmap based on the kernel estimate
contour(sim.mn.kernel, add = T) #contour plot added above the heatmap
#next is the "clean" version of bivariate normal distribution which using ellipse function
#in addition, confidence intervals is also added
rho = cor(sim.mn.data[,1:2])
y_on_x = lm(high ~ low, sim.mn.data)    # Regression Y ~ X
x_on_y = lm(low ~ high, sim.mn.data)    # Regression X ~ Y
plot_legend = c("99% CI green", "95% CI red","90% CI blue",
"Y on X black", "X on Y brown")
plot(sim.mn.data[,1:2], xlab = "low", ylab = "high",
col = "dark blue",
main = "Bivariate Normal with Confidence Intervals")
lines(ellipse(rho), col="red")       # ellipse() from ellipse package
lines(ellipse(rho, level = .99), col="green")
lines(ellipse(rho, level = .90), col="blue")
abline(y_on_x)
abline(x_on_y, col="brown")
legend(3,1,legend=plot_legend,cex = .5, bty = "n")
#3d representation
persp(sim.mn.kernel, phi = 45, theta = 30, shade = .1, border = NA) # from base graphics package
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#Visualizing the simulated data can be ilustratedusing the following 1 dimension subset of previously generated matrix data.
hist(sim.mn.data[,1])
##Simulating mutivariate normal (MN) distribution based on correlation/covariance matrix, means, and standard deviation
set.seed(100) #setting seed for pseudo-random initiation
sim.mn.data = data.frame(mvrnorm(n = 5000, Sigma = cov.matrix, mu = mu)) #simulated data with total 5000 observations
str(sim.mn.data)
sim.mn.data = rename(sim.mn.data, low = X1, high = X2, throw = X3, dodge = X4, make = X5)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#Visualizing the simulated data can be ilustratedusing the following 1 dimension subset of previously generated matrix data.
hist(sim.mn.data[,1])
hist(sim.mn.data$high)
hist(sim.mn.data$throw)
View(sim.mn.data)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#Visualizing the simulated data can be ilustratedusing the following 1 dimension subset of previously generated matrix data.
hist(sim.mn.data$low)#var 1
hist(sim.mn.data$high)#var 2
hist(sim.mn.data$throw)#var 3
hist(sim.mn.data$dodge)#var 4
hist(sim.mn.data$make)#var 5
#since we are limited in 3dimensional representation, we are limited to only use until first 3 variables
#of the simulated data from before.
#it is guaranteed that marginal distributions from MN is always normal(see Methods_of_Multivariate_Analysis-_3rd_Edition Rencher & Christensen )
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,3], n = 150) #calculate kernel density estimate
image(sim.mn.kernel) #heatmap based on the kernel estimate
contour(sim.mn.kernel, add = T) #contour plot added above the heatmap
plot_legend = c("99% CI green", "95% CI red","90% CI blue",
"Y on X black", "X on Y brown")
plot(sim.mn.data[,1:2], xlab = "low", ylab = "high",
col = "dark blue",
main = "Bivariate Normal with Confidence Intervals")
lines(ellipse(rho), col="red")       # ellipse() from ellipse package
lines(ellipse(rho, level = .99), col="green")
lines(ellipse(rho, level = .90), col="blue")
#3d representation
persp(sim.mn.kernel, phi = 45, theta = 30, shade = .1, border = NA) # from base graphics package
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#Visualizing the simulated data can be ilustratedusing the following 1 dimension subset of previously generated matrix data.
hist(sim.mn.data$low)#var 1
##To give an introduction, lets do a quick review on normal univariate distrivution
#Suppose we have a normally distributed random variable X with some mean mu and standard deviance sigma
mu = 1 #Set mu = 1
sigma = 0 #Set sigma = 0
rnorm(1, mean = mu, sigma)
rnorm(1, mean = mu, sigma)
rnorm(1, mean = mu, sigma) #randomize a number x1 (realization of variable X)
##To give an introduction, lets do a quick review on normal univariate distrivution
#Suppose we have a normally distributed random variable X with some mean mu and standard deviance sigma
mu = 0 #Set mu = 0
sigma = 1 #Set sigma = 1
rnorm(1, mean = mu, sigma) #randomize a number x1 (realization of variable X)
#Thus in R, to make sure we get the same output on a randomization process, we are setting a "pseudo" random number
#using function set.seed()
set.seed(11)
rnorm(1, mean = mu, sigma)
#Thus in R, to make sure we get the same output on a randomization process, we are setting a "pseudo" random number
#using function set.seed()
set.seed(11)
rnorm(1, mean = mu, sigma)
#Thus in R, to make sure we get the same output on a randomization process, we are setting a "pseudo" random number
#using function set.seed()
set.seed(11)
x1 = rnorm(1, mean = mu, sigma)
#Suppose we want to get more than 1 realization number, we can either set the number of sampling in the function
#of randomization or define an independent sampling method.
#Lets say, we want to get a sample with size of 1000
set.seed(11)
x_samp1 = rnorm(1000, mean = mu, sigma)
x_samp2 = rep(NA, 1000)#We set a "void" vector to be used as the independent sampling vector
n = 1000 #Set number of observations
set.seed(11)
for(i in 1:n){
x_samp2[i] = rnorm(1, mean = mu, sigma)#Independently sampling each realization value of X_i
}
x_samp1 == x_samp2
hist(x_samp1)
hist(x_samp2)
##Testing univariate normality
#With the null hypothesis of the data comes from normal distribution, each of this test can be used to the hypothesis
shapiro.test(x_samp1)
ks.test(x_samp1)
ks.test(x_samp1, "pnormal")
library(nortest)
ad.test(x_samp1)
lillie.test(x_samp1)
cvm.test(x_samp1)
library(MASS)
library(stests)
library(ggplot2)
library(rgl)
library(car)
library(RColorBrewer)
library(stats)
library(gridExtra)
library(tidyverse)
library(DescTools)
library(MVN)
##Testing when the covariance matrix is known
#similar to univariate case, lets start with testing means when the covariance is known.
#simulate the samples
meanvect = c(13,5,1250)
cov_m = matrix(c(124,123,47,
123,203,162,
47,162,3257), ncol = 3)
#define a function to sample from a MN dist
set.seed(123)
gen.data = function(mu, sigma, n) {
require(MASS)
dt = mvrnorm(n, mu=mu, Sigma=sigma) #samples
meandt = mean(dt)
return(dt)
}
gen.samples = gen.data(meanvect, cov_m, 300)
colnames(gen.samples) = c("v1", "v2", "v3")
plot3d(gen.samples)
#Testing for one sample
#Lets test the generated sample gen.samples with the null hypothesis mu* = mu = c(10,5,1200)
#Known covariance matrix (we use the covariance of the previous normal distribution)
n = dim(gen.samples)[1]
mu.sam = apply(gen.samples, 2, mean)
mu = c(10, 5, 1200)
z2 = n*t(mu.sam-mu)%*%solve(cov_m)%*%(mu.sam-mu)
p.z2 = dchisq(z2, 3) #Z2 is distributed as chisq(p), p : degree of freedom if null is true
p.z2
p.z2 = pchisq(z2, 3, lower.tail = F) #Z2 is distributed as chisq(p), p : degree of freedom if null is true
p.z2
#Using hotellingT2 function from desctools package for unknown covariance matrix
HotellingsT2Test(gen.samples, mu = mu, cov_m)
#assumption test
mvn(gen.samples) #the result is expected since we draw the sample from a normal distribution
#But there is an interesting finding, where the univariate normality of the 3rd variable is rejected
mshapiro.test(t(gen.samples))
library(RVAideMemoire)
#But there is an interesting finding, where the univariate normality of the 3rd variable is rejected
mshapiro.test(t(gen.samples))#From library RVAideMemoire
det(cov(gen.samples))
chol(cov(gen.samples))
#But there is an interesting finding, where the univariate normality of the 3rd variable is rejected
mshapiro.test(t(gen.samples))#From library RVAideMemoire
#But there is an interesting finding, where the univariate normality of the 3rd variable is rejected
mshapiro.test((gen.samples))#From library RVAideMemoire
#Compare the result with univariate case
#H0 : (1) mu1 = 10; (2) mu2 = 5; (3) mu3 = 1200
z1 = (mean(gen.samples[,1])-10)/124
pnorm(z1, 0, 1, lower.tail = F)
z.test(gen.samples[,1], sigma2 = 124, mu = 10)
Ztest(gen.samples[,1], sigma2 = 124, mu = 10)
ZTest(gen.samples[,1], sigma2 = 124, mu = 10)
install.packages("BSDA")
library(BSDA)
z.test(gen.samples[,1], sigma.x = 124, mu = 10)
#Compare the result with univariate case
#H0 : (1) mu1 = 10; (2) mu2 = 5; (3) mu3 = 1200
z1 = (mean(gen.samples[,1])-10)/124
pnorm(z1, 0, 1, lower.tail = F)
z.test()
z.test
#Compare the result with univariate case
#H0 : (1) mu1 = 10; (2) mu2 = 5; (3) mu3 = 1200
z1 = (mean(gen.samples[,1])-10)/(124/length(gen.samples[,1]))
pnorm(z1, 0, 1, lower.tail = F)
#Compare the result with univariate case
#H0 : (1) mu1 = 10; (2) mu2 = 5; (3) mu3 = 1200
z1 = (mean(gen.samples[,1])-10)/(124/sqrt(length(gen.samples[,1])))
pnorm(z1, 0, 1, lower.tail = F)
2*pnorm(z1, 0, 1, lower.tail = F)#P value for testing 2 way hypothesis with alternative h1 : mu1 =/= 10
z.test(gen.samples[,1], sigma.x = 124, mu = 10)
z.2 = n*t(mu.sam-mu)%*%solve(cov_m)%*%(mu.sam-mu)
p.z2 = pchisq(z.2, 3, lower.tail = F) #Z2 is distributed as chisq(p), p : degree of freedom if null is true
p.z2
#Using hotellingT2 function from desctools package for unknown covariance matrix
HotellingsT2Test(gen.samples, mu = mu, cov_m)
#Using hotellingT2 function from desctools package for unknown covariance matrix
HotellingsT2Test(gen.samples, mu = mu, cov(gen.samples))
HotellingsT2Test
"HotellingsT2Test"
#Using hotellingT2 function from desctools package for unknown covariance matrix
t2 = n*t(mu.sam-mu)%*%solve(cov(gen.samples))%*%(mu.sam-mu)
HotellingsT2Test(gen.samples, mu = mu, cov(gen.samples))
f.eq = (297/(3*299))*t2
library(Hotelling)
hotelling.test(gen.samples)
#Using hotellingT2 function from desctools package for unknown covariance matrix
t2 = n*t(mu.sam-mu)%*%solve(cov(gen.samples))%*%(mu.sam-mu)
f.eq = (297/(3*299))*t2
HotellingsT2Test(gen.samples, mu = mu, cov(gen.samples))
View(f.eq)
#Using hotellingT2 function from desctools package for unknown covariance matrix
t2 = n*t(mu.sam-mu)%*%solve(cov(gen.samples))%*%(mu.sam-mu)
HotellingsT2Test(gen.samples, mu = mu, cov(gen.samples))
172/398
297/598
297/(3*299)
HotellingsT2Test(gen.samples, mu = mu, cov(gen.samples))
mu
HotellingsT2Test(gen.samples, mu = mu, cov(gen.samples))
HotellingsT2Test(gen.samples, mu = mu, cov(gen.samples), test = "f")
HotellingsT2Test(gen.samples, mu = mu, cov(gen.samples), test = "chi")
z.2 = n*t(mu.sam-mu)%*%solve(cov_m)%*%(mu.sam-mu)
#We conclude from the pvalue above that there is not enough evidence for the null hypothesis
#mu* = mu = c(10,5,1200) to be accepted
HotellingsT2Test(gen.samples, mu = mu, cov_m, test = "chi")
HotellingsT2Test(gen.samples, mu = mu, cov(gen.samples), test = "f")
#Testing mu for unknown covariance matrix
t2 = n*t(mu.sam-mu)%*%solve(cov(gen.samples))%*%(mu.sam-mu)#T2 statistics,
f.eq = (297/(3*299))*t2
HotellingsT2Test(gen.samples, mu = mu, cov(gen.samples), test = "f")
f.eq = (297/(2*300))*t2
f.eq = (297/(2*297))*t2
f.eq = (297/(2*298))*t2
#Testing mu for unknown covariance matrix
t2 = ((n^2)/(n+n))*t(mu.sam-mu)%*%solve(cov(gen.samples))%*%(mu.sam-mu)#T2 statistics,
f.eq = (297/(3*299))*t2
HotellingsT2Test(gen.samples, mu = mu, cov(gen.samples), test = "f")
#Testing for one sample
#Lets test the generated sample gen.samples with the null hypothesis mu* = mu = c(10,5,1200)
#Known covariance matrix (we use the covariance of the previous normal distribution)
n = dim(gen.samples)[1]
mu.sam = apply(gen.samples, 2, mean)
mu = c(10, 5, 1200)
z.2 = n*t(mu.sam-mu)%*%solve(cov_m)%*%(mu.sam-mu)
p.z2 = pchisq(z.2, 3, lower.tail = F) #Z2 is distributed as chisq(p), p : degree of freedom if null is true
p.z2
#We conclude from the pvalue above that there is not enough evidence for the null hypothesis
#mu* = mu = c(10,5,1200) to be accepted
HotellingsT2Test(gen.samples, mu = mu, cov_m, test = "chi")
math.teach <- data.frame(
teacher = factor(rep(1:2, c(3, 6))),
satis = c(1, 3, 2, 4, 6, 6, 5, 5, 4),
know = c(3, 7, 2, 6, 8, 8, 10, 10, 6))
with(math.teach,
HotellingsT2Test(cbind(satis, know) ~ teacher))
z.test(gen.samples[,1], sigma.x = 124, mu = 10)
#Compare the result with univariate case
#H0 : (1) mu1 = 10; (2) mu2 = 5; (3) mu3 = 1200
#Known Variance using the covariance matrix from multivariate normal distribution above
cov_m
#2nd variable
z2 = (mean(gen.samples[,2])-5)/(203/sqrt(length(gen.samples[,2])))
2*pnorm(z2, 0, 1, lower.tail = F)
z.test(gen.samples[,2], sigma.x = 203, mu = 5)
z3 = (mean(gen.samples[,3])-1200)/(3257/sqrt(length(gen.samples[,3])))
z3
2*pnorm(z3, 0, 1, lower.tail = F)
z.test(gen.samples[,3], sigma2 = 3257, mu = 1200)
z.test(gen.samples[,3], sigma.x = 3257, mu = 1200)
#Unknown variance
z.test(gen.samples[,3])
#Unknown variance
t.test(gen.samples[,3])
#Unknown variance
t.test(gen.samples[,3], mu = 1200)
z.test(gen.samples[,3], sigma.x = 3257, mu = 1200)
#Rounding error, leads to pvalue > 1
z.test(gen.samples[,2], sigma.x = 203, mu = 5)
z.test(gen.samples[,1], sigma.x = 124, mu = 10)
#Unknown variance
t.test(gen.samples[,1], mu = 10)
t.test(gen.samples[,2], mu = 5)
t.test(gen.samples[,3], mu = 1200)
#Unknown variance
t.test(gen.samples[,1], mu = 10)[1]
#Unknown variance
t.test(gen.samples[,1], mu = 10)[2]
#Unknown variance
t.test(gen.samples[,1], mu = 10)[3]
#Unknown variance
p.val_test = rep(NA, 3)
for(i in 1:3){
p.val_test[i] = t.test(gen.samples[,i], mu = mu[i])
}
#Unknown variance
p.val_test = rep(NA, 3)
for(i in 1:3){
p.val_test[i] = as.numeric(t.test(gen.samples[,i], mu = mu[i])[3])
}
p.val_test
#Unknown variance
p.val_test = rep(NA, 3)
for(i in 1:3){
p.val_test[i] = (t.test(gen.samples[,i], mu = mu[i])[3])
}
p.val_test
p.val_test = data.frame(p.val_test)
p.val_test
#Unknown variance
p.val_test = rep(NA, 3)
for(i in 1:3){
p.val_test[i] = as.numeric(t.test(gen.samples[,i], mu = mu[i])[3])
}
#Unknown variance
p.val_test = data.frame(pv.test1 = NA, pv.test2 = NA, pv.test3 = NA)
View(p.val_test)
p.val_test[1]
for(i in 1:3){
p.val_test[i] = as.numeric(t.test(gen.samples[,i], mu = mu[i])[3])
}
p.val_test
#All individual univariate tests have the same conclusion, that is acceptance of null hypothesis
ztest.ind = data.frame(var1 = c(NA, NA), var2 = c(NA,NA), var3 = c(NA,NA))
ztest.ind
rownames(ztest.ind) = c("z stat", "2 way-p value")
ztest.ind
z.test(gen.samples[,3], sigma.x = 3257, mu = 1200)
z.test(gen.samples[,3], sigma.x = 3257, mu = 1200)[1]
for(i in 1:2){
for(j in 1:3){
ztest.ind[i,j] = as.numeric(z.test(gen.samples[,3], sigma.x = cov_m[j,j], mu = mu[j])[1])
}
ztest.ind[i,j] = as.numeric(z.test(gen.samples[,3], sigma.x = cov_m[j,j], mu = mu[j])[3])
}
View(ztest.ind)
#All individual univariate tests have the same conclusion, that is acceptance of null hypothesis
ztest.ind = data.frame(var1 = c(NA, NA), var2 = c(NA,NA), var3 = c(NA,NA))
rownames(ztest.ind) = c("z stat", "2 way-p value")
for(i in 1:2){
for(j in 1:3){
ztest.ind[i,j] = as.numeric(z.test(gen.samples[,j], sigma.x = cov_m[j,j], mu = mu[j])[1])
}
ztest.ind[i,j] = as.numeric(z.test(gen.samples[,j], sigma.x = cov_m[j,j], mu = mu[j])[3])
}
#All individual univariate tests have the same conclusion, that is acceptance of null hypothesis
ztest.ind = data.frame(var1 = c(NA, NA), var2 = c(NA,NA), var3 = c(NA,NA))
rownames(ztest.ind) = c("z stat", "2 way-p value")
for(i in 1:2){
for(j in 1:3){
ztest.ind[i,j] = as.numeric(z.test(gen.samples[,j], sigma.x = cov_m[j,j], mu = mu[j])[1])
ztest.ind[i,j] = as.numeric(z.test(gen.samples[,j], sigma.x = cov_m[j,j], mu = mu[j])[3])
}
}
#All individual univariate tests have the same conclusion, that is acceptance of null hypothesis
ztest.ind = data.frame(var1 = c(NA, NA), var2 = c(NA,NA), var3 = c(NA,NA))
rownames(ztest.ind) = c("z stat", "2 way-p value")
for(i in 1:2){
for(j in 1:3){
ztest.ind[i,j] = as.numeric(z.test(gen.samples[,j], sigma.x = cov_m[j,j], mu = mu[j])[1])
}
}
#All individual univariate tests have the same conclusion, that is acceptance of null hypothesis
ztest.ind = data.frame(var1 = c(NA, NA), var2 = c(NA,NA), var3 = c(NA,NA))
rownames(ztest.ind) = c("z stat", "2 way-p value")
for(j in 1:3){
ztest.ind[1,j] = as.numeric(z.test(gen.samples[,j], sigma.x = cov_m[j,j], mu = mu[j])[1])
ztest.ind[2,j] = as.numeric(z.test(gen.samples[,j], sigma.x = cov_m[j,j], mu = mu[j])[3])
}
for(j in 1:3){
ztest.ind[1,j] = as.numeric(z.test(gen.samples[,j], sigma.x = cov_m[j,j], mu = mu[j])[1])
}
for(j in 1:3){
ztest.ind[1,j] = as.numeric(z.test(gen.samples[,j], sigma.x = cov_m[j,j], mu = mu[j])[1])
ztest.ind[2,j] = as.numeric(z.test(gen.samples[,j], sigma.x = cov_m[j,j], mu = mu[j])[1])
}
#Unknown variance
p.val_test = data.frame(pv.test1 = NA, pv.test2 = NA, pv.test3 = NA)
for(i in 1:3){
p.val_test[i] = as.numeric(t.test(gen.samples[,i], mu = mu[i])[3])
}
for(j in 1:3){
ztest.ind[1,j] = as.numeric(z.test(gen.samples[,j], sigma.x = cov_m[j,j], mu = mu[j])[1])
ztest.ind[2,j] = as.numeric(z.test(gen.samples[,j], sigma.x = cov_m[j,j], mu = mu[j])[3])
}
z.test(gen.samples[,3], sigma.x = 3257, mu = 1200)[3]
z.test(gen.samples[,3], sigma.x = 3257, mu = 1200)[2]
#All individual univariate tests have the same conclusion, that is acceptance of null hypothesis
ztest.ind = data.frame(var1 = c(NA, NA), var2 = c(NA,NA), var3 = c(NA,NA))
rownames(ztest.ind) = c("z stat", "2 way-p value")
for(j in 1:3){
ztest.ind[1,j] = as.numeric(z.test(gen.samples[,j], sigma.x = cov_m[j,j], mu = mu[j])[1])
ztest.ind[2,j] = as.numeric(z.test(gen.samples[,j], sigma.x = cov_m[j,j], mu = mu[j])[2])
}
gen.samples
view(gen.samples)
rnorm(1, mean = mu, sigma) #randomize a number x1 (realization of variable X)
##To give an introduction, lets do a quick review on normal univariate distrivution
#Suppose we have a normally distributed random variable X with some mean mu and standard deviance sigma
mu = 0 #Set mu = 0
sigma = 1 #Set sigma = 1
rnorm(1, mean = mu, sigma) #randomize a number x1 (realization of variable X)
rnorm(1, mean = mu, sigma) #If we do another run on the same code, the value of x1 will change
#Thus in R, to make sure we get the same output on a randomization process, we are setting a "pseudo" random number
#using function set.seed()
set.seed(11)
x1 = rnorm(1, mean = mu, sigma)
rnorm(1, mean = mu, sigma) #randomize a number x1 (realization of variable X)
rnorm(1, mean = mu, sigma) #If we do another run on the same code, the value of x1 will change
rnorm(1, mean = mu, sigma) #randomize a number x1 (realization of variable X)
rnorm(1, mean = mu, sigma) #If we do another run on the same code, the value of x1 will change
#Thus in R, to make sure we get the same output on a randomization process, we are setting a "pseudo" random number
#using function set.seed()
set.seed(11)
rnorm(1, mean = mu, sigma)
#Thus in R, to make sure we get the same output on a randomization process, we are setting a "pseudo" random number
#using function set.seed()
set.seed(11)
rnorm(1, mean = mu, sigma)
#Thus in R, to make sure we get the same output on a randomization process, we are setting a "pseudo" random number
#using function set.seed()
set.seed(11)
rnorm(1, mean = mu, sigma)
rnorm(1, mean = mu, sigma) #randomize a number x1 (realization of variable X)
rnorm(1, mean = mu, sigma) #randomize a number x1 (realization of variable X)
rnorm(1, mean = mu, sigma) #randomize a number x1 (realization of variable X)
#Testing for one sample
#Lets test the generated sample gen.samples with the null hypothesis mu* = mu = c(10,5,1200)
#Known covariance matrix (we use the covariance of the previous normal distribution)
n = dim(gen.samples)[1]
mu.sam = apply(gen.samples, 2, mean)
mu = c(10, 5, 1200)
z.2 = n*t(mu.sam-mu)%*%solve(cov_m)%*%(mu.sam-mu)
p.z2 = pchisq(z.2, 3, lower.tail = F) #Z2 is distributed as chisq(p), p : degree of freedom if null is true
p.z2
#We conclude from the pvalue above that there is not enough evidence for the null hypothesis
#mu* = mu = c(10,5,1200) to be accepted
HotellingsT2Test(gen.samples, mu = mu, cov_m, test = "chi")#Using function from desctools package
z.test(gen.samples[,3], sigma.x = 3257, mu = 1200)[2]
#Rounding error, leads to pvalue > 1
z.test(gen.samples[,2], sigma.x = 203, mu = 5)
#We conclude from the pvalue above that there is not enough evidence for the null hypothesis
#mu* = mu = c(10,5,1200) to be accepted
HotellingsT2Test(gen.samples, mu = mu, cov_m, test = "chi")#Using function from desctools package
#We conclude from the pvalue above that there is not enough evidence for the null hypothesis
#mu* = mu = c(10,5,1200) to be accepted
HotellingsT2Test(gen.samples, mu = mu, cov_m, test = "chi")#Using function from desctools package
#assumption test
mvn(gen.samples) #the result is expected since we draw the sample from a normal distribution
#1st variable
z1 = (mean(gen.samples[,1])-10)/(124/sqrt(length(gen.samples[,1])))#Z statistics
z1
2*pnorm(z1, 0, 1, lower.tail = F)#P value for testing 2 way hypothesis with alternative h1 : mu1 =/= 10
z.test(gen.samples[,1], sigma.x = 124, mu = 10)

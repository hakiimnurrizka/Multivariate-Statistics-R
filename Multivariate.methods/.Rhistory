View(SAQ1)
corrplot(SAQ1)
corrplot(SAQ1[,1:2])
corrplot(cor(SAQ1))
## CFA Using lavaan
library(lavaan)
library(corrplot)
corrplot(cor(SAQ1))
## One factor - 3 items
fac1_3 = 'f =~ q03+q04+q05'
mfac1_3 = cfa(fac1_3, data = SAQ1)
summary(mfac1_3)
#we can set marker method to fixed other parameter by defining NA*parameter
fac1_3 = 'f =~ q03+q04+NA*q05'
mfac1_3 = cfa(fac1_3, data = SAQ1)
summary(mfac1_3)
#we can set marker method to fixed other parameter by defining NA*parameter
fac1_3 = 'f =~ q03+q04+NA*q05
q05 ~ 1'
mfac1_3 = cfa(fac1_3, data = SAQ1)
summary(mfac1_3)
#we can set marker method to fixed other parameter by defining NA*parameter
fac1_3 = 'f =~ q03+q04+1*q05'
mfac1_3 = cfa(fac1_3, data = SAQ1)
summary(mfac1_3)
#we can set marker method to fixed other parameter by defining NA*parameter
fac1_3 = 'f =~ NA*q03+q04+1*q05'
mfac1_3 = cfa(fac1_3, data = SAQ1)
summary(mfac1_3)
#For better understanding of the factor, we can add standardize in the summary function
summary(mfac1_3, standardized = TRUE)
## One factor - 2 items
#To fit such model, we need variance standardization that is to set the variance of factor to one and equate
#both factor loadings.
fac1_2 = 'f1 =~ a*q04 + a*q05'
mfac1_2 = cfa(fac1_2, data = SAQ1)
summary(mfac1_2, standardized = TRUE)
mfac1_2 = cfa(fac1_2, data = SAQ1, std.lv = TRUE)
summary(mfac1_2, standardized = TRUE)
## One factor - >3 items
fac1 = 'f =~ q01 + q02 + q03 + q04 + q05 + q06 + q07 + q08'
mfac1 = cfa(fac1, data =SAQ1, std.lv = TRUE)
summary(mfac1, fit.measures = TRUE, standardized = TRUE)
### Clustering ###
library(factoextra)
library(cluster)
library(ggplot2)
library(dplyr)
<<<<<<< Updated upstream
<<<<<<< Updated upstream
### CFA Using lavaan
library(lavaan)
library(haven)
library(corrplot)
summary(mfac1, fit.measures = TRUE, standardized = TRUE)
## Two or more factors
#For case where we assume there are more than 1 factor, there are 3 conditions to consider : (1)uncorrelated
#factors, (2)correlated factors, and (3)hierarchy/order of factors.
#For the (1) case we simply assume that each factor is uncorrelated with each other. Lets say for previous
#anxiety data we have 2 uncorrelated factors, thus the model :
fac2 = 'f1 =~ q01 +  q03 + q04 + q05 + q08
f2 =~ a*q06 + a*q07
f1 ~~ 0*f2'
mfac2 = cfa(fac2, data = SAQ1, std.lv = TRUE)
summary(mfac2, fit.measures = TRUE, standardized = TRUE)
#From the result above, model fit measurements show that 2 factors model with uncorrelated factors poorly fits
#the data. It is also has higher test statistics of chi-square and RMSEA than the previous one factor model.
#We can then try 2 factors model with correlated factors.
#Case (2) we assume there is strong correlation between 2 factors behind the anxiety data. The model for
#2 correlated factors is :
fac2_c = 'f1 =~ q01 +  q03 + q04 + q05 + q08
f2 =~ q06 + q07'
mfac2_c = cfa(fac2_c, data = SAQ1, std.lv = TRUE)
summary(mfac2_c, fit.measures = TRUE, std.lv = TRUE)
summary(mfac2_c, fit.measures = TRUE, standardized = TRUE)
#The result above shows some contradiction on the test statistics where RMSEA decide that the model is a close
#fitting for the data while chi square decide that the model is not a close fit. This is a normal occurrence
#since the chi square statistics tends to reject null hypothesis easily when the number observations is high.
#But all in all, when we also consider the fit measures from CFI and TLI, this 2 correlated factors is by far
#the best model for anxiety data compared to previous models.
#For the last case (3), we may consider that instead of 2 factors being correlated, there is another factor
#that affects both factors. In other words, we have a structure that is ordered from : exogenous factor which
#affects the endogenous factors which then these endogenous factors affects our item/data.
#The model is simply adding another layer of factor equation on the endogenous factors.
fac2_o = 'f1 =~ q01 +  q03 + q04 + q05 + q08
f2 =~ q06 + q07
f3 =~ a*f1 + a*f2'
mfac2_o = cfa(fac2_o, data = SAQ1, std.lv = TRUE)
summary(mfac2_o, fit.measures = TRUE, standardized = TRUE)
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
pokemon <- read.csv("D:/My Drive/Github/Multivariate-Statistics-R/pokemon.csv", header=FALSE)
View(pokemon)
pokemon <- read.csv("D:/My Drive/Github/Multivariate-Statistics-R/pokemon.csv", row.names=1)
View(pokemon)
library(readxl)
pokemon <- read_excel("D:/My Drive/Github/Multivariate-Statistics-R/pokemon.xlsx")
View(pokemon)
pc_poke = prcomp(pokemon[,18])
library(factoextra)
fviz_eig(pc_poke)
pc_poke = prcomp(pokemon[,1:18])
fviz_eig(pc_poke)
rm(pokemon)
data("iris")
attach(iris)
View(iris)
###manova###
###one way
##test statistics
#test with manova, when the null hypothesis is accepted, proceed with anova
man.iris = manova(cbind(Sepal.Width, Sepal.Length, Petal.Length) ~ Species)
summary(man.iris)
lm1.iris = lm(Sepal.Width ~ Species)
anova1 = anova(lm1.iris)
anova1
lm2.iris = lm(Sepal.Length ~ Species)
anova2 = anova(lm2.iris)
anova2
lm3.iris = lm(Petal.Length ~ Species)
anova3 = anova(lm3.iris)
anova3
##assumption
# H0: normality
mshapiro.test(man.iris$residuals)
mvn(man.iris$residuals)
library(biotools)
library(MVN)
library(RVAideMemoire)
##assumption
# H0: normality
mshapiro.test(man.iris$residuals)
mvn(man.iris$residuals)
boxM(man.iris$residuals, iris[,5])
boxM(man.iris$residuals, Species)
#for independence, need information about the sampling method
summary(man.iris) # default Pillai
summary(man.iris, test = 'Wilks')
summary(man.iris, test = 'Hotelling-Lawley')
summary(man.iris, test = 'Roy')
man.iris2 = manova(cbind(Sepal.Width, Sepal.Length, Petal.Length) ~ Species*soil)
summary(man.iris2)
###two way
#suppose we are to test whether the iris data is significantly affected by two independent
#variables that is the previous species and a new variable soil.
#first the model will include an interaction term to accommodate in case of both
#independent variables affect the dependent variables simultanously
iris$soil = rep(c("dry", "wet", "sand"), each = 5, times = 10)
attach(iris)
man.iris2 = manova(cbind(Sepal.Width, Sepal.Length, Petal.Length) ~ Species*soil)
summary(man.iris2)
#the conventional term-selection method dictate to eliminate the term that
#isn't significant starting from the highest order.
man.iris2 = manova(cbind(Sepal.Width, Sepal.Length, Petal.Length) ~ Species+soil)
summary(man.iris2)
### PCA ###
pca_train1 = prcomp(client_train1[,-13], scale = TRUE)
View(head(pca_train1$x,7))
fviz_eig(pca_train1)
library(factoextra)
fviz_eig(pca_train1)
summary(pca_train1)
fviz_pca_ind(pca_train1, col.ind = "cos2", # Color by the quality of representation
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = T)
head(cricket)
library(corrplot)
corrplot(matcrick, method = "number")
T3_9_GLUCOSE <- read.table("D:/My Drive/Github/Multivariate-Statistics-R/T3_9_GLUCOSE.DAT", quote="\"", comment.char="")
View(T3_9_GLUCOSE)
##Data used is glucose.dat from Rencher, 2012
rm(T3_9_GLUCOSE)
glucose = read.table("D:/My Drive/Github/Multivariate-Statistics-R/T3_9_GLUCOSE.DAT", quote="\"", comment.char="")
#we consider it has 2 sets of variables with each set has 3 variables each
gluc_1 = glucose[,1:3]
gluc_2 = glucose[,4:6]
View(glucose)
library(psych)
#We'll visualize the correlation within each set between each pair
pairs.panels(gluc_1)
pairs.panels(gluc_2)
#We'll visualize the correlation within each set between each pair
pairs.panels(gluc_1) + cor(gluc_1)
pairs.panels(glucose)
#Correlation above used pearson correlation coefficient, which means it is only appropriate IF
#the data has normal distribution
shapiro.test(gluc_1)
#Correlation above used pearson correlation coefficient, which means it is only appropriate IF
#the data has normal distribution
shapiro.test(gluc_1$V1)
library(mvn)
library(MVN)
#Let's check normality for each variables AND multivariate normality for each set
mvn(gluc_1)
mvn(gluc_2) #multivariate normality and 2 individuals normality is not accepted
mvn(glucose)
cor(gluc_1)
cor(gluc_1, method = "spearman")
##Canonical correlation
#First we'll use the function from default and psych then we proceed with breaking down the formula.
cancor(gluc_1, gluc_2)
cc(gluc_1, gluc_2)
install.packages("CCA")
library(CCA)
cc(gluc_1, gluc_2)
plt.cc(gluc_1, gluc_2)
can.gluc = cc(gluc_1, gluc_2)
plt.cc(can.gluc)
plt.cc(can.gluc)
plt.cc(can.gluc, var.label = TRUE)
can.gluc$cor
#The plots above show the closeness for each variable and individual
#The next section, the correlations are tested for their significances
comput(gluc_1, gluc_2, can.gluc)
library(CCP)
install.packages("CCP")
library(CCP)
#The plots above show the closeness for each variable and individual
#The next section, the correlations are tested for their significances
p.asym(can.gluc$cor, dim(gluc_1)[1], length(gluc_1), length(gluc_2))#Inputs are : correlation coef,
#The plots above show the closeness for each variable and individual
#The next section, the correlations are tested for their significances
p.asym(can.gluc$cor, dim(gluc_1)[1], length(gluc_1), length(gluc_2),
tstat = "Wilks")#Inputs are : correlation coef,
#The plots above show the closeness for each variable and individual
#The next section, the correlations are tested for their significances
p.asym(can.gluc$cor, dim(gluc_1)[1], length(gluc_1), length(gluc_2),
tstat = "Hotelling")#Inputs are : correlation coef, num of observations+variables
#The result above shows that only when canonical correlation is built upon 3 dimensions, the correlations
#are significance
det(cor(glucose))
#The result above shows that only when canonical correlation is built upon 3 dimensions, the correlations
#are significance
det(cor(glucose))/(det(gluc_1)*det(gluc_2))
#The result above shows that only when canonical correlation is built upon 3 dimensions, the correlations
#are significance
det(cor(glucose))/(det(cor(gluc_1))*det(cor(gluc_2))
#The result above shows that only when canonical correlation is built upon 3 dimensions, the correlations
#are significance
det(cor(glucose))/(det(cor(gluc_1))*det(cor(gluc_2)))
#The result above shows that only when canonical correlation is built upon 3 dimensions, the correlations
#are significance
det(cor(glucose))/(det(cor(gluc_1))*det(cor(gluc_2)))
#The result above shows that only when canonical correlation is built upon 3 dimensions, the correlations
#are significance. This is equivalent with test for overall significance using 3 dimensions
#Now we discuss about the coefficient for canonical correlation
can.gluc$xcoef
can.gluc$ycoef
##Canonical correlation
#First we'll use the function from default and CCA then we proceed with breaking down the formula.
cancor(gluc_1, gluc_2) #default function from stats library
##Canonical correlation
#First we'll use the function from default and CCA then we proceed with breaking down the formula.
cancor(gluc_1, gluc_2, xcenter = FALSE, ycenter = FALSE) #default function from stats library
##Canonical correlation
#First we'll use the function from default and CCA then we proceed with breaking down the formula.
cancor(gluc_1, gluc_2) #default function from stats library
#The result above shows that only when canonical correlation is built upon 3 dimensions, the correlations
#are significance. This is equivalent with test for overall significance using 3 dimensions
#Now we discuss about the coefficient for canonical correlation
can.gluc$xcoef
can.gluc$ycoef
#Interpretationfor above can be seen similar to that of regression.
#So for this case, we have 2 sets of regression equations (which is actually the canonical variate) represented
#by table. For each set, we have 3 equations represented by collumns while row is our predictor (in this case
#is the variables themselves). Thus, we can interpret each value just like regression (i.e : each unit
#change of V1 will decrease first equation of set 1 of canonical variate by 0.065)
#We can also used the standardized version of canonical coefficient to see the relative effect of each variable
#and removing the difference in size/scale between variables.
diag(sqrt(diag(cov(gluc_1))))
#Interpretationfor above can be seen similar to that of regression.
#So for this case, we have 2 sets of regression equations (which is actually the canonical variate) represented
#by table. For each set, we have 3 equations represented by collumns while row is our predictor (in this case
#is the variables themselves). Thus, we can interpret each value just like regression (i.e : each unit
#change of V1 will decrease first equation of set 1 of canonical variate by 0.065)
#We can also used the standardized version of canonical coefficient to see the relative effect of each variable
#and removing the difference in size/scale between variables.
diag(sqrt(diag(cov(gluc_1))))%*%can.gluc$xcoef
barplot(can.gluc$cor, main = "Canonical correlation", col ="blue" )
par(mfrow = c(1,1))
barplot(can.gluc$cor, main = "Canonical correlation", col ="blue" )
library(ggplot2)
can.coef = data.frame(rho = can.gluc$cor, dim = c("rho1", "rho2", "rho3"))
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar()
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity") + labs(title = "Canonical correlation")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "bin") + labs(title = "Canonical correlation")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity", fill = "blue") + labs(title = "Canonical correlation")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity", fill = "steel blue") + labs(title = "Canonical correlation")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity", width = 0.9, fill = "steel blue") +
labs(title = "Canonical correlation")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity", width = 0.5, fill = "steel blue") +
labs(title = "Canonical correlation")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity", width = 0.5, fill = "steel blue") +
labs(title = "Canonical correlation", col = "red")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity", width = 0.5, fill = "steel blue") +
labs(title = "Canonical correlation", colour = "cylinders")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity", width = 0.5, fill = "steel blue") +
labs(title = "Canonical correlation", colour = "red")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity", width = 0.5, fill = "steel blue") +
labs(colour = "red")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity", width = 0.5, fill = "steel blue") +
labs(title = "Canonical correlation")
#Interpretationfor above can be seen similar to that of regression.
#So for this case, we have 2 sets of regression equations (which is actually the canonical variate) represented
#by table. For each set, we have 3 equations represented by collumns while row is our predictor (in this case
#is the variables themselves). Thus, we can interpret each value just like regression (i.e : each unit
#change of V1 will decrease first variate of set 1 of canonical variate by 0.065)
#We can also used the standardized version of canonical coefficient to see the relative effect of each variable
#and removing the difference in size/scale between variables.
diag(sqrt(diag(cov(gluc_1))))%*%can.gluc$xcoef
##Using matrix operation to get canonical correlation
#Next, lets break down the formula.
#First, we get the correlation matrix of the data
cor(gluc_1)
cor(gluc_2)
rxy = cor(gluc_1, gluc_2)
ryx = cor(gluc_2, gluc_1)
##Using matrix operation to get canonical correlation
#Next, lets break down the formula.
#First, we get the correlation matrix of the data
rxx = cor(gluc_1)
ryy = cor(gluc_2)
cor(glucose)
rxy
#Following formula from Rencher (2012), we are going to compute multiplication of inverse and the partition
#matrix above. That is : Rxx^-1 * Rxy * Ryy^-1 * Ryx
rho.mat = rxx^-1
rho.mat
#Following formula from Rencher (2012), we are going to compute multiplication of inverse and the partition
#matrix above. That is : Rxx^-1 * Rxy * Ryy^-1 * Ryx
rho.mat = rxx^-1%*%rxy%*%ryy^-1%*%ryx
rho.mat
#Eigen values of the resulting matrix is the squared canonical correlations that we are interested in
eigen(rho.mat)
#Following formula from Rencher (2012), we are going to compute multiplication of inverse and the partition
#matrix above. That is : Rxx^-1 * Rxy * Ryy^-1 * Ryx
rho.mat = solve(rxx)%*%rxy%*%solve(ryy)%*%ryx
#Eigen values of the resulting matrix is the squared canonical correlations that we are interested in
eigen(rho.mat)
#Eigen values of the resulting matrix is the squared canonical correlations that we are interested in
eigen(rho.mat)$values
#Eigen values of the resulting matrix is the squared canonical correlations that we are interested in
sq.rho = eigen(rho.mat)$values #squared canonical correlations
sqrt(sq.rho)
#Testing overall significance can also be made based only upon correlation matrix
wilk.gl = det(cor(glucose))/(det(cor(gluc_1))*det(cor(gluc_2)))
wf = length(glucose)-(dim(gluc_1)[1]+dim(gluc_2)[1]+3)/2
tf = sqrt((dim(gluc_1)[1]^2*dim(gluc_2)[1]^2-4)/(dim(gluc_1)[1]+dim(gluc_2)[1]-5))
tf = sqrt((dim(gluc_1)[1]^2*dim(gluc_2)[1]^2-4)/(dim(gluc_1)[1]^2+dim(gluc_2)[1]^2-5))
df1 = dim(gluc_1)[1]*dim(gluc_2)[1]
wf = length(glucose)-(dim(gluc_1)[2]+dim(gluc_2)[2]+3)/2
tf = sqrt((dim(gluc_1)[2]^2*dim(gluc_2)[2]^2-4)/(dim(gluc_1)[2]^2+dim(gluc_2)[2]^2-5))
df1 = dim(gluc_1)[2]*dim(gluc_2)[2]
df2 = wf*tf-df1/2+1
length(glucose)
wf = dim(glucose)[1]-(dim(gluc_1)[2]+dim(gluc_2)[2]+3)/2
wf = dim(glucose)[1]-(dim(gluc_1)[2]+dim(gluc_2)[2]+3)/2
tf = sqrt((dim(gluc_1)[2]^2*dim(gluc_2)[2]^2-4)/(dim(gluc_1)[2]^2+dim(gluc_2)[2]^2-5))
df1 = dim(gluc_1)[2]*dim(gluc_2)[2]
df2 = wf*tf-df1/2+1
ap.f = (1-wilk.gl^(1/tf))*df2/(wilk.gl^(1/tf)*df1)
pf(ap.f, df1, df2)
qf(ap.f, df1, df2)
wilk.gl
p.asym(can.gluc$cor, dim(gluc_1)[1], length(gluc_1), length(gluc_2),
tstat = "Wilks")
ap.f = (1-wilk.gl^(1/tf))*df2/(wilk.gl^(1/tf)*df1)
pf(ap.f, df1, df2, lower.tail = FALSE)
#We can also use the rho values instead of correlation matrix
wilk.gl = sum(1-eigen(sq.rho))
1-eigen(sq.rho)
1-eigen(sq.rho)$values
eigen(sq.rho)$values
#We can also use the rho values instead of correlation matrix
wilk.gl = sum(1-sq.rho)
#Testing overall significance can also be made based only upon correlation matrix
wilk.gl = det(cor(glucose))/(det(cor(gluc_1))*det(cor(gluc_2)))
#We can also use the rho values instead of correlation matrix
wilk.gl = prod(1-sq.rho)
#Testing overall significance can also be made based only upon correlation matrix
wilk.gl = det(cor(glucose))/(det(cor(gluc_1))*det(cor(gluc_2)))
pillai.gl = sum(sq.rho)
hotel.gl = sum(sq.rho/(1-sq.rho))
roy.gl = max(sq.rho)
p.asym(can.gluc$cor, dim(gluc_1)[1], length(gluc_1), length(gluc_2),
tstat = "Pillai")
p.asym(can.gluc$cor, dim(gluc_1)[1], length(gluc_1), length(gluc_2),
tstat = "Roy")
#For roy test, it is basically only testing significance with 1st dimension.
#Extension for testing succeeding dimensions can be made by modifying the formula based on eigen values
wilk.gl2 = prod(1-sq.rho[,-1])
#For roy test, it is basically only testing significance with 1st dimension.
#Extension for testing succeeding dimensions can be made by modifying the formula based on eigen values
wilk.gl2 = prod(1-sq.rho[-1])
#From the result above, it can be seen that the highest canonical correlation is close to .5
#The next section, the correlations are tested for their significances
p.asym(can.gluc$cor, dim(gluc_1)[1], length(gluc_1), length(gluc_2),
tstat = "Hotelling")#Inputs are : correlation coef, num of observations+variables
#Lets compare with our previous function using tstat wilks
p.asym(can.gluc$cor, dim(gluc_1)[1], length(gluc_1), length(gluc_2),
tstat = "Wilks")
matrix(c(1,.5,1,.5), byrow = T, ncol = 2)
matrix(c(1,.5,1,.5), ncol = 2)
matrix(c(1,.5,.5,1), ncol = 2)
matrix(c(.4,.3,.2,.25), ncol = 2)
matrix(c(1,.6,.6,1), ncol = 2)
matrix(c(.4,.3,.2,.25), byrow = T, ncol = 2)
### Multivariate Regression ###
#Extension from multiple linear regression where we have more than 1 target/dependent variables
#At this point, regression should have been very familiar for almost everyone so lets jump right into modelling
#We'll model glucose data with first 3 variables as x and the others are the target
mlm.gluc = lm(glucose[,1:3] ~ glucose[,4:6])
### Multivariate Regression ###
#Extension from multiple linear regression where we have more than 1 target/dependent variables
#At this point, regression should have been very familiar for almost everyone so lets jump right into modelling
#We'll model glucose data with first 3 variables as x and the others are the target
mlm.gluc = lm(cbind(glucose[,1:3]) ~ glucose[,4:6])
### Multivariate Regression ###
#Extension from multiple linear regression where we have more than 1 target/dependent variables
#At this point, regression should have been very familiar for almost everyone so lets jump right into modelling
#We'll model glucose data with first 3 variables as x and the others are the target
mlm.gluc = lm(formula = cbind(V1, V2, V3) ~ V4 + V5 + V6, data = glucose)
summary(mlm.gluc)
### Multivariate Regression ###
#Extension from multiple linear regression where we have more than 1 target/dependent variables
#At this point, regression should have been very familiar for almost everyone so lets jump right into modelling
#We'll model glucose data with first 3 variables as x and the others are the target
mlm.gluc = lm(formula = cbind(V4, V5, V6) ~ V1 + V2 + V3, data = glucose)
summary(mlm.gluc)
mlm.gluc$coefficients
#Overall test : alternative hypothesis that at least one element of beta matrix is not equal to 0 (of course
#except the beta zero)
wilk.mlgl = det(t(glucose[,4:6])%*%glucose[,4:6])
t(glucose[,4:6])%*%glucose[,4:6]
#Overall test : alternative hypothesis that at least one element of beta matrix is not equal to 0 (of course
#except the beta zero)
glucose1 = as.matrix(glucose)
wilk.mlgl = det(t(glucose1[,4:6])%*%glucose1[,4:6])
#Overall test : alternative hypothesis that at least one element of beta matrix is not equal to 0 (of course
#except the beta zero)
beta1.mlgl =  as.matrix(mlm.gluc$coefficients[2:4,])
View(beta1.mlgl)
wilk.mlgl = det(t(glucose1[,4:6])%*%glucose1[,4:6]-t(beta1.mlgl)%*%t(glucose1[,1:3]%*%glucose1[,4:6])
wilk.mlgl = det(t(glucose1[,4:6])%*%glucose1[,4:6]-t(beta1.mlgl)%*%t(glucose1[,1:3])%*%glucose1[,4:6])
wilk.mlgl = det(t(glucose1[,4:6])%*%glucose1[,4:6]-t(beta1.mlgl)%*%t(glucose1[,1:3])%*%glucose1[,4:6])
xgl = glucose1[,1:3]
ygl = glucose1[,4:6]
dim(glucose1)
ngl = dim(glucose1)[1]
mean(y)
mean(ygl)
apply(ygl, 2, mean)
ybar.gl = as.matrix(apply(ygl, 2, mean))
wilk.mlgl = det(t(y)%*%y-t(beta1.mlgl)%*%t(x)%*%y)/det(t(y)%*%y-ngl*ybar.gl%*%t(ybar.gl))
wilk.mlgl = det(t(ygl)%*%ygl-t(beta1.mlgl)%*%t(xgl)%*%ygl)/det(t(ygl)%*%ygl-ngl*ybar.gl%*%t(ybar.gl))
wilk.mlgl = det(cov(glucose1))/(det(cov(xgl))*det(cov(ygl)))
solve(t(xgl)%*%xgl)%*%t(xgl)%*%ygl
mlm.gluc$coefficients #beta matrix
chem <- read.table("D:/My Drive/Github/Multivariate-Statistics-R/T10_1_CHEM.DAT", quote="\"", comment.char="")
View(chem)
lm(formula = cbind(V2, V3, V4) ~ V5 + V6 + V7, data = chem)
xche = as.matrix(chem[,5:7])
yche = as.matrix(chem[,2:4])
solve(t(xche)%*%xche)%*%t(xche)%*%yche
xche
xche = as.matrix(1,chem[,5:7])
xche
length(chem)
dim(chem)[1]
xche = as.matrix(cbind(rep(1, dim(chem)[1]), chem[,5:7]))
xche
solve(t(xche)%*%xche)%*%t(xche)%*%yche
lm(formula = cbind(V2, V3, V4) ~ V5 + V6 + V7, data = chem)
xgl = cbind(rep(1, dim(glucose1)[1]), glucose1[,1:3])
View(xgl)
xgl = cbind(1 = rep(1, dim(glucose1)[1]), glucose1[,1:3])
xgl = cbind('1' = rep(1, dim(glucose1)[1]), glucose1[,1:3])
solve(t(xgl)%*%xgl)%*%t(xgl)%*%ygl
summary(mlm.gluc)
mlm.gluc$coefficients #beta matrix
solve(t(xgl)%*%xgl)%*%t(xgl)%*%ygl
#Overall test : alternative hypothesis that at least one element of beta matrix is not equal to 0 (of course
#except the beta zero)
beta1.mlgl =  as.matrix(mlm.gluc$coefficients[2:4,])
ybar.gl = as.matrix(apply(ygl, 2, mean))
ngl = dim(glucose1)[1]
wilk.mlgl = det(t(ygl)%*%ygl-t(beta1.mlgl)%*%t(xgl)%*%ygl)/det(t(ygl)%*%ygl-ngl*ybar.gl%*%t(ybar.gl))
beta1.mlgl
#Overall test : alternative hypothesis that at least one element of beta matrix is not equal to 0 (of course
#except the beta zero)
beta1.mlgl =  as.matrix(mlm.gluc$coefficients[1:4,])
ybar.gl = as.matrix(apply(ygl, 2, mean))
ngl = dim(glucose1)[1]
wilk.mlgl = det(t(ygl)%*%ygl-t(beta1.mlgl)%*%t(xgl)%*%ygl)/det(t(ygl)%*%ygl-ngl*ybar.gl%*%t(ybar.gl))
wilk.mlgl = det(cov(glucose1))/(det(cov(xgl))*det(cov(ygl)))
wilk.mlgl = det(t(ygl)%*%ygl-t(beta1.mlgl)%*%t(xgl)%*%ygl)/det(t(ygl)%*%ygl-ngl*ybar.gl%*%t(ybar.gl))
#Overall test : alternative hypothesis that at least one element of beta matrix is not equal to 0 (of course
#except the beta zero)
beta1.mlgl =  as.matrix(mlm.gluc$coefficients)
ybar.gl = as.matrix(apply(ygl, 2, mean))
ngl = dim(glucose1)[1]
egl = det(t(ygl)%*%ygl-t(beta1.mlgl)%*%t(xgl)%*%ygl)
egl = t(ygl)%*%ygl-t(beta1.mlgl)%*%t(xgl)%*%ygl
hgl = t(beta1.mlgl)%*%t(xgl)%*%ygl-ngl*ybar.gl%*%t(ybar.gl)
eh = solve(egl)%*%hgl
eigen(eh)
prod(1/(1+eigen(eh)$values))
wilk.mlgl = det(cov(glucose1))/(det(cov(xgl[,3:4]))*det(cov(ygl)))
xgl[,3:4]
wilk.mlgl = det(cov(glucose1))/(det(cov(xgl[,2:4]))*det(cov(ygl)))

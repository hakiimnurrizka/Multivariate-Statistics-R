five.samples <- data.frame("A"=rnorm(100), "B"=rnorm(100), "C"=rnorm(100),
"D"=rnorm(100), "E"=rnorm(100))
cor(five.samples)
KMOS(five.samples, use = "pairwise.complete.obs")
install.packages("REdaS")
library(REdaS)
KMOS(five.samples, use = "pairwise.complete.obs")
KMOS(client_train1[,-13])
KMOS(client_train1[,-13]) #Another function for KMO and MSA
cortest.bartlett(client_train1[,-13])
KMO((client_train1[,-13])) #Kaiser-Meyer-Olkin to measure factorability
cortest.bartlett(client_train1[,-13])
det(cor(client_train1[,-13]))
det(cor(client_train1[,-c(6,13)]))
det(cor(client_train1[,-c(6,8,13)]))
det(cor(client_train1[,-c(6,8,11,13)]))
det(cor(client_train1[,-c(6,8,10,11,13)]))
det(cor(client_train1[,-c(8,10,11,13)]))
det(cor(client_train1[,-c(7:13)]))
det(cor(client_train1[,-c(7:11,13)]))
det(cor(client_train1[,-c(1,13)]))
det(cor(client_train1[,-c(6,12,13)]))
det(cor(client_train1[,-c(5,6,11,12,13)]))
det(cor(client_train1[,-13]))
det(cor(client_train1[,-c(6,12,13)]))
view(client_train1[,-c(6,12,13)])
View(client_train1[,-c(6,12,13)])
##Extraction using PCA
pca(client_train1[,-c(6,12,13)])
##Extraction using PCA
library(factoextra)
pc_client = prcomp(client_train1[,-c(6,12,13)], scale. = T)
summary(pc_client)
pc_client$sdev
pc_client$rotation
View(pc_client$rotation)
pc_client$rotation[1,1]^2
sum(pc_client$rotation[1,]^2)
fviz_eig(pc_client)
#Based on PCA, lets move on with the premise that there are 2 factors that make up client data
#We can also determine how many factors using following method
fa_client = fa(client_train1[,-c(6,12,13)], nfactors = ncol(client_train1[,-c(6,12,13)]), rotate = "none")
n_fa = length(fa_clientk$e.values)
n_fa = length(fa_client$e.values)
scree_fa = data.frame(factor_n = as.factor(1:n_fa), eigenvalues = fa_client$e.values)
ggplot(scree_fa, aes(x = factor_n, y = eigenvalues, group = 1))+
geom_point()+geom_line()+xlab("number of factors")+ylab("initial eigenvalue")+
labs(title = "scree plot", subtitle = "(based on unreduced correlation matrix)")
##Apply factor analysis
fa_client =  fa(r = client_train1[,-c(6,12,13)], nfactors = 2, covar = FALSE, SMC = TRUE,
fm = "pa", max.iter = 100, rotate = "varimax")
fa_client
print(fa_client)
factanal(client_train1[,-c(6,12,13)], factors = 2, rotation ="varimax", scores = c("regression"))
factanal(client_train1[,-c(6,12,13)], factors = 2, rotation ="none", scores = c("regression"))
factanal(client_train1[,-c(6,12,13)], factors = 3, rotation ="none", scores = c("regression"))
factanal(client_train1[,-c(6,12,13)], factors = 4, rotation ="none", scores = c("regression"))
factanal(client_train1[,-c(6,12,13)], factors = 5, rotation ="none", scores = c("regression"))
factanal(client_train1[,-c(6,12,13)], factors = 6, rotation ="none", scores = c("regression"))
factanal(client_train1[,-c(6,12,13)], factors = 7, rotation ="none", scores = c("regression"))
factanal(client_train1[,-c(6,12,13)], factors = 5, rotation ="none", scores = c("regression"))
fa.parallel(client_train1[,-c(6,12,13)])
factanal(client_train1[,-c(6,12,13)], factors = 5, rotation ="varimax", scores = c("regression"))
fa_client = factanal(client_train1[,-c(6,12,13)], factors = 5, rotation ="varimax", scores = c("regression"))
fa.diagram(fa_client)
##Apply factor analysis
fa_client =  fa(r = client_train1[,-c(6,12,13)], nfactors = 2, covar = FALSE, SMC = TRUE,
fm = "pa", max.iter = 100, rotate = "varimax")
fa.diagram(fa_client)
client_test.PC = predict(fa_client, newdata = client_test[,13:24])
fa_client$scores
View(client_test)
client_test.PC = predict.psych(fa_client, old.data = client_train1[,-c(6,12,13)], newdata = client_test[,c(13:17,19:23])
client_test[,c(13:17,19:23]
client_test[,c(13:17,19:23)]
client_test.PC = predict.psych(fa_client, old.data = client_train1[,-c(6,12,13)], newdata = client_test[,c(13:17,19:23)])
client_test.PC = predict.psych(fa_client, old.data = client_train1[,-c(6,12,13)], data = client_test[,c(13:17,19:23)])
View(client_test.PC)
client_test.PC = predict(pca_train1, newdata = client_test[,13:24])
client_test.FA = predict.psych(fa_client, old.data = client_train1[,-c(6,12,13)], data = client_test[,c(13:17,19:23)])
View(client_test1)
View(client_train1)
View(client_train_new)
View(client_train)
### Logistic model using FA results
client_train_new1 = as.data.frame(cbind(client_train[,c(1:5,24)], fa_client$scores))
View(client_train_new1)
View(client_train_new)
View(client_train_new1)
client_train_new1[,2] = as.factor(client_train_new1[,2])
client_train_new1[,3] = as.factor(client_train_new1[,3])
client_train_new1[,4] = as.factor(client_train_new1[,4])
client_train_new1[,6] = as.factor(client_train_new1[,6])
client_log1 = glm(default.payment.next.month~0+., data = client_train_new1, family = binomial)
summary(client_log1)
client_log1 = glm(default.payment.next.month~0++LIMIT_BAL+EDUCATION+MARRIAGE+PA1+PA2, data = client_train_new1, family = binomial)
summary(client_log1)
## Check the accuracy using specificity and sensitiviy
predict_client_stat = predict(client_log, newdata = client_train_new1[,c(1,3,4,7,8)], type = "response")
## Check the accuracy using specificity and sensitiviy
predict_client_stat = predict(client_log1, newdata = client_train_new1[,c(1,3,4,7,8)], type = "response")
predictions = prediction(predict_client_stat, client_train_new$default.payment.next.month)
sens = data.frame(x=unlist(performance(predictions, "sens")@x.values),
y=unlist(performance(predictions, "sens")@y.values))
spec = data.frame(x=unlist(performance(predictions, "spec")@x.values),
y=unlist(performance(predictions, "spec")@y.values))
sens %>% ggplot(aes(x,y)) +
geom_line() +
geom_line(data=spec, aes(x,y,col="red")) +
scale_y_continuous(sec.axis = sec_axis(~., name = "Specificity")) +
labs(x='Cutoff', y="Sensitivity") +
theme(axis.title.y.right = element_text(colour = "red"), legend.position="none")
library(dplyr)
sens %>% ggplot(aes(x,y)) +
geom_line() +
geom_line(data=spec, aes(x,y,col="red")) +
scale_y_continuous(sec.axis = sec_axis(~., name = "Specificity")) +
labs(x='Cutoff', y="Sensitivity") +
theme(axis.title.y.right = element_text(colour = "red"), legend.position="none")
#get the optimum threshold
sens = cbind(unlist(performance(predictions, "sens")@x.values), unlist(performance(predictions, "sens")@y.values))
spec = cbind(unlist(performance(predictions, "spec")@x.values), unlist(performance(predictions, "spec")@y.values))
cutoff_opt_client = sens[which.min(apply(sens, 1, function(x) min(colSums(abs(t(spec) - x))))), 1]
View(client_test1)
View(client_test.FA)
client_test2 = as.data.frame(cbind(client_test[,-c(1,13:24)],client_test.FA))
View(client_test1)
View(client_test2)
#Confusion matrix
predict_client_stat = ifelse(predict_client_stat>cutoff_opt_client,1,0)
conf_mat_client = table(predict_client_stat, client_train_new$default.payment.next.month)
specificity(conf_mat_client)
### Logistic model using FA results
library(ROCR)
specificity(conf_mat_client)
library(performance)
specificity(conf_mat_client)
library(caret)
specificity(conf_mat_client)
sensitivity(conf_mat_client)
View(client_train_new1)
# Get the prediction for training data
predict_client_stat = predict(client_log, newdata = client_train_new[,c(1,3,4,7,8)], type = "response")
# Using specificity and sensitivity to decide the response threshold
predictions = prediction(predict_client_stat, client_train_new$default.payment.next.month)
sens = data.frame(x=unlist(performance(predictions, "sens")@x.values),
y=unlist(performance(predictions, "sens")@y.values))
spec = data.frame(x=unlist(performance(predictions, "spec")@x.values),
y=unlist(performance(predictions, "spec")@y.values))
sens %>% ggplot(aes(x,y)) +
geom_line() +
geom_line(data=spec, aes(x,y,col="red")) +
scale_y_continuous(sec.axis = sec_axis(~., name = "Specificity")) +
labs(x='Cutoff', y="Sensitivity") +
theme(axis.title.y.right = element_text(colour = "red"), legend.position="none")
rm(list = ls())
##Prepare the data
client_train = read_excel("~/Github/Machine-Learning/client-data.xlsx",
sheet = "client_train")
### Client Data Analysis using Factor Analysis ###
library(readxl)
##Prepare the data
client_train = read_excel("~/Github/Machine-Learning/client-data.xlsx",
sheet = "client_train")
library(readxl)
client_train <- read_excel("G:/My Drive/Github/Machine-Learning/client-data.xlsx",
sheet = "client_train")
View(client_train)
client_train1 = client_train[,-c(1:11)]
##Correlation and sampling adequation
#This acts as pre-analysis to identify whether factor analysis is a valid method to analyze the data or not
corrplot(cor(client_train1[,-13]), method = "number") #correlation matrix
View(client_train1)
client_train1 = client_train[,-c(1:12)]
View(client_train1)
##Correlation and sampling adequation
#This acts as pre-analysis to identify whether factor analysis is a valid method to analyze the data or not
corrplot(cor(client_train1[,-13]), method = "number") #correlation matrix
KMO(cor(client_train1[,-13])) #Kaiser-Meyer-Olkin to measure factorability
KMOS(client_train1[,-13]) #Another function for KMO and MSA
cortest.bartlett(client_train1[,-13])
det(cor(client_train1[,-13]))
det(cor(client_train1[,-c(6,12,13)]))
pc_client = prcomp(client_train1[,-c(6,12,13)], scale. = T)
View(pc_client$rotation)
fviz_eig(pc_client)
#Based on previous PCA, lets move on with the premise that there are 2 factors that make up client data
#We can also determine how many factors using following method
fa_client = fa(client_train1[,-c(6,12,13)], nfactors = ncol(client_train1[,-c(6,12,13)]), rotate = "none")
n_fa = length(fa_client$e.values)
scree_fa = data.frame(factor_n = as.factor(1:n_fa), eigenvalues = fa_client$e.values)
ggplot(scree_fa, aes(x = factor_n, y = eigenvalues, group = 1))+
geom_point()+geom_line()+xlab("number of factors")+ylab("initial eigenvalue")+
labs(title = "scree plot", subtitle = "(based on unreduced correlation matrix)") #either using elbow method or by setting certain percentage of total variance (calculate using eigen value)
fa.parallel(client_train1[,-c(6,12,13)]) #Paralel analysis
##Apply factor analysis
fa_client =  fa(r = client_train1[,-c(6,12,13)], nfactors = 2, covar = FALSE, SMC = TRUE,
fm = "pa", max.iter = 100, rotate = "varimax")
print(fa_client)
factanal(client_train1[,-c(6,12,13)], factors = 5, rotation ="varimax", scores = c("regression"))
fa.diagram(fa_client)
#The result from factor analysis then can be used to other model such as regression.
#Obviously, we are to use the factor analysis transformation to predict using logistic regression.
#Prepare the test data
client_test = read_excel("G:~/Github/Machine-Learning/client-data.xlsx",
sheet = "client_train")
library(readxl)
client_test <- read_excel("G:/My Drive/Github/Machine-Learning/client-data.xlsx",
sheet = "client_test")
View(client_test)
client_test.FA = predict.psych(fa_client, old.data = client_train1[,-c(6,12,13)], data = client_test[,c(13:17,19:23)])
View(client_test.FA)
client_test2 = as.data.frame(cbind(client_test[,-c(1,13:24)],client_test.FA))
View(client_train1)
client_train_new1 = as.data.frame(cbind(client_train[,c(1:5,24)], fa_client$scores))
client_train_new1[,2] = as.factor(client_train_new1[,2])
client_train_new1[,3] = as.factor(client_train_new1[,3])
client_train_new1[,4] = as.factor(client_train_new1[,4])
client_train_new1[,6] = as.factor(client_train_new1[,6])
client_log1 = glm(default.payment.next.month~0++LIMIT_BAL+EDUCATION+MARRIAGE+PA1+PA2, data = client_train_new1, family = binomial)
View(client_train_new1)
View(client_train)
client_train_new1 = as.data.frame(cbind(client_train[,c(1:5,25)], fa_client$scores))
View(client_train_new1)
client_train_new1[,2] = as.factor(client_train_new1[,2])
client_train_new1[,3] = as.factor(client_train_new1[,3])
client_train_new1[,4] = as.factor(client_train_new1[,4])
client_train_new1[,6] = as.factor(client_train_new1[,6])
client_log1 = glm(default.payment.next.month~0++LIMIT_BAL+EDUCATION+MARRIAGE+PA1+PA2, data = client_train_new1, family = binomial)
summary(client_log1)
View(client_train_new1)
View(client_train)
client_train_new1 = as.data.frame(cbind(client_train[,c(2:6,25)], fa_client$scores))
client_train_new1[,2] = as.factor(client_train_new1[,2])
client_train_new1[,3] = as.factor(client_train_new1[,3])
client_train_new1[,4] = as.factor(client_train_new1[,4])
client_train_new1[,6] = as.factor(client_train_new1[,6])
client_log1 = glm(default.payment.next.month~0++LIMIT_BAL+EDUCATION+MARRIAGE+PA1+PA2, data = client_train_new1, family = binomial)
summary(client_log1)
## Check the accuracy using specificity and sensitiviy
predict_client_stat = predict(client_log1, newdata = client_train_new1[,c(1,3,4,7,8)], type = "response")
predictions = prediction(predict_client_stat, client_train_new$default.payment.next.month)
sens = data.frame(x=unlist(performance(predictions, "sens")@x.values),
y=unlist(performance(predictions, "sens")@y.values))
predictions = prediction(predict_client_stat, client_train_new1$default.payment.next.month)
sens = data.frame(x=unlist(performance(predictions, "sens")@x.values),
y=unlist(performance(predictions, "sens")@y.values))
View(client_train_new1)
View(client_train_new1)
sens = data.frame(x=unlist(performance(predictions, "sens")@x.values),
y=unlist(performance(predictions, "sens")@y.values))
spec = data.frame(x=unlist(performance(predictions, "spec")@x.values),
y=unlist(performance(predictions, "spec")@y.values))
## Check the accuracy using specificity and sensitiviy
predict_client_stat = predict(client_log1, newdata = client_train_new1[,c(1,3,4,7,8)], type = "response")
predictions = prediction(predict_client_stat, client_train_new1$default.payment.next.month)
sens = data.frame(x=unlist(performance(predictions, "sens")@x.values),
y=unlist(performance(predictions, "sens")@y.values))
str(predictions)
performance(predictions, "sens")@x.values
predict_client_stat
rm(list = ls(-c(client_test,client_train)))
rm(list = -c(client_test,client_train))
rm(list = -c("client_test","client_train"))
rm(list = ls())
library(readxl)
client_test <- read_excel("G:/My Drive/Github/Machine-Learning/client-data.xlsx",
sheet = "client_test")
View(client_test)
library(readxl)
client_train <- read_excel("G:/My Drive/Github/Machine-Learning/client-data.xlsx",
sheet = "client_train")
View(client_train)
client_train = read_excel("~/Github/Machine-Learning/client-data.xlsx",
sheet = "client_train")
client_train1 = client_train[,-c(1:11)]
### PCA ###
pca_train1 = prcomp(client_train1[,-13], scale = TRUE)
View(head(pca_train1$x,7))
fviz_eig(pca_train1)
summary(pca_train1)
fviz_pca_ind(pca_train1, col.ind = "cos2", # Color by the quality of representation
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = T)
fviz_pca_var(pca_train1, col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)
client_test.PC = predict(pca_train1, newdata = client_test[,13:24])
View(client_test)
client_test.PC = predict(pca_train1, newdata = client_test[,13:24])
View(client_train1)
View(client_train)
client_train1 = client_train[,-c(2:12)]
View(client_train1)
client_train1 = client_train[,-c(1,2:12)]
### PCA ###
pca_train1 = prcomp(client_train1[,-13], scale = TRUE)
summary(pca_train1)
fviz_eig(pca_train1)
client_test.PC = predict(pca_train1, newdata = client_test[,13:24])
client_test1 = as.data.frame(cbind(client_test[,-c(1,13:24)],client_test.PC[,1:2]))
### Logistic Regression for Predictive Model ###
## Building the logistic regression for predictive study
client_train_new = as.data.frame(cbind(client_train[,-c(6:23)],pca_train1$x[,1:2]))
client_train_new[,2] = as.factor(client_train_new[,2])
client_train_new[,3] = as.factor(client_train_new[,3])
client_train_new[,4] = as.factor(client_train_new[,4])
client_train_new[,6] = as.factor(client_train_new[,6])
client_log = glm(default.payment.next.month~0+LIMIT_BAL+EDUCATION+MARRIAGE+PC1+PC2, data = client_train_new, family = binomial)
summary(client_log)
View(client_train)
View(client_train)
### Logistic Regression for Predictive Model ###
## Building the logistic regression for predictive study
client_train_new = as.data.frame(cbind(client_train[,-c(1,13:24)],pca_train1$x[,1:2]))
View(client_train_new)
client_train_new[,2] = as.factor(client_train_new[,2])
client_train_new[,3] = as.factor(client_train_new[,3])
client_train_new[,4] = as.factor(client_train_new[,4])
client_train_new[,6] = as.factor(client_train_new[,6])
client_log = glm(default.payment.next.month~0+LIMIT_BAL+EDUCATION+MARRIAGE+PC1+PC2, data = client_train_new, family = binomial)
summary(client_log)
View(client_train_new)
View(client_train)
### Logistic Regression for Predictive Model ###
## Building the logistic regression for predictive study
client_train_new = as.data.frame(cbind(client_train[,-c(1,7:24)],pca_train1$x[,1:2]))
client_train_new[,2] = as.factor(client_train_new[,2])
client_train_new[,3] = as.factor(client_train_new[,3])
client_train_new[,4] = as.factor(client_train_new[,4])
client_train_new[,6] = as.factor(client_train_new[,6])
client_log = glm(default.payment.next.month~0+LIMIT_BAL+EDUCATION+MARRIAGE+PC1+PC2, data = client_train_new, family = binomial)
summary(client_log)
View(client_train_new)
# Get the prediction for training data
predict_client_stat = predict(client_log, newdata = client_train_new[,c(1,3,4,7,8)], type = "response")
View(client_train_new)
# Using specificity and sensitivity to decide the response threshold
predictions = prediction(predict_client_stat, client_train_new$default.payment.next.month)
sens = data.frame(x=unlist(performance(predictions, "sens")@x.values),
y=unlist(performance(predictions, "sens")@y.values))
spec = data.frame(x=unlist(performance(predictions, "spec")@x.values),
y=unlist(performance(predictions, "spec")@y.values))
sens %>% ggplot(aes(x,y)) +
geom_line() +
geom_line(data=spec, aes(x,y,col="red")) +
scale_y_continuous(sec.axis = sec_axis(~., name = "Specificity")) +
labs(x='Cutoff', y="Sensitivity") +
theme(axis.title.y.right = element_text(colour = "red"), legend.position="none")
#get the optimum threshold
sens = cbind(unlist(performance(predictions, "sens")@x.values), unlist(performance(predictions, "sens")@y.values))
spec = cbind(unlist(performance(predictions, "spec")@x.values), unlist(performance(predictions, "spec")@y.values))
cutoff_opt_client = sens[which.min(apply(sens, 1, function(x) min(colSums(abs(t(spec) - x))))), 1]
#confusion matrix
predict_client_stat = ifelse(predict_client_stat>cutoff_opt_client,1,0)
library(factoextra)
library(readxl)
library(ggplot2)
library(performance)
library(caret)
library(ROCR)
library(dplyr)
sens = data.frame(x=unlist(performance(predictions, "sens")@x.values),
y=unlist(performance(predictions, "sens")@y.values))
performance(predictions, "sens")
predict_client_stat
predictions
head(predictions,10)
performance(client_log)
performance(client_log, "sens")
performance(client_log, "sens")@x.values
performance(client_log)@x.values
View(predictions)
performance(predictions)
performance_accuracy(predictions)
unlist(predictions)
performance(unlist(predictions))
performance(predict_client_stat)
performance(predictions$predictions)
performance(predictions)
performance(client_log)
prediction(predict_client_stat, client_train_new$default.payment.next.month)
library(factoextra)
library(readxl)
library(ggplot2)
library(caret)
library(ROCR)
library(dplyr)
sens = data.frame(x=unlist(performance(predictions, "sens")@x.values),
y=unlist(performance(predictions, "sens")@y.values))
spec = data.frame(x=unlist(performance(predictions, "spec")@x.values),
y=unlist(performance(predictions, "spec")@y.values))
sens %>% ggplot(aes(x,y)) +
geom_line() +
geom_line(data=spec, aes(x,y,col="red")) +
scale_y_continuous(sec.axis = sec_axis(~., name = "Specificity")) +
labs(x='Cutoff', y="Sensitivity") +
theme(axis.title.y.right = element_text(colour = "red"), legend.position="none")
#get the optimum threshold
sens = cbind(unlist(performance(predictions, "sens")@x.values), unlist(performance(predictions, "sens")@y.values))
spec = cbind(unlist(performance(predictions, "spec")@x.values), unlist(performance(predictions, "spec")@y.values))
cutoff_opt_client = sens[which.min(apply(sens, 1, function(x) min(colSums(abs(t(spec) - x))))), 1]
#confusion matrix
predict_client_stat = ifelse(predict_client_stat>cutoff_opt_client,1,0)
conf_mat_client = table(predict_client_stat, client_train_new$default.payment.next.month)
conf_mat_client
specificity(conf_mat_client)
sensitivity(conf_mat_client)
#Apply model and cutoff on test data
predict_client_test = ifelse(predict(client_log, newdata = client_test1[,c(1,3,4,12,13)], type = "response")>cutoff_opt_client,1,0)
client_test1$predict.payment = predict_client_test
rm(list = ls())
library(readxl)
client_train <- read_excel("G:/My Drive/Github/Machine-Learning/client-data.xlsx",
sheet = "client_train")
View(client_train)
library(readxl)
client_test <- read_excel("G:/My Drive/Github/Machine-Learning/client-data.xlsx",
sheet = "client_test")
View(client_test)
### Client Data Analysis using Factor Analysis ###
library(readxl)
library(psych)
library(corrplot)
library(ggplot2)
library(car)
library(nFactors)
library(REdaS)
library(dplyr)
client_train1 = client_train[,-c(1:12)]
View(client_train1)
##Correlation and sampling adequation
#This acts as pre-analysis to identify whether factor analysis is a valid method to analyze the data or not
corrplot(cor(client_train1[,-13]), method = "number") #correlation matrix
KMO(cor(client_train1[,-13])) #Kaiser-Meyer-Olkin to measure factorability
KMOS(client_train1[,-13]) #Another function for KMO and MSA
cortest.bartlett(client_train1[,-13])
det(cor(client_train1[,-13]))
det(cor(client_train1[,-c(6,12,13)]))
##Extraction using PCA
library(factoextra)
pc_client = prcomp(client_train1[,-c(6,12,13)], scale. = T)
View(pc_client$rotation)
fviz_eig(pc_client)
#Based on previous PCA, lets move on with the premise that there are 2 factors that make up client data
#We can also determine how many factors using following method
fa_client = fa(client_train1[,-c(6,12,13)], nfactors = ncol(client_train1[,-c(6,12,13)]), rotate = "none")
n_fa = length(fa_client$e.values)
scree_fa = data.frame(factor_n = as.factor(1:n_fa), eigenvalues = fa_client$e.values)
ggplot(scree_fa, aes(x = factor_n, y = eigenvalues, group = 1))+
geom_point()+geom_line()+xlab("number of factors")+ylab("initial eigenvalue")+
labs(title = "scree plot", subtitle = "(based on unreduced correlation matrix)") #either using elbow method or by setting certain percentage of total variance (calculate using eigen value)
fa.parallel(client_train1[,-c(6,12,13)]) #Paralel analysis
##Apply factor analysis
fa_client =  fa(r = client_train1[,-c(6,12,13)], nfactors = 2, covar = FALSE, SMC = TRUE,
fm = "pa", max.iter = 100, rotate = "varimax")
print(fa_client)
factanal(client_train1[,-c(6,12,13)], factors = 5, rotation ="varimax", scores = c("regression"))
fa.diagram(fa_client)
#The result from factor analysis then can be used to other model such as regression.
#Obviously, we are to use the factor analysis transformation to predict using logistic regression.
#Prepare the test data
client_test = read_excel("~/Github/Machine-Learning/client-data.xlsx",
sheet = "client_train")
client_test.FA = predict.psych(fa_client, old.data = client_train1[,-c(6,12,13)], data = client_test[,c(13:17,19:23)])
client_test2 = as.data.frame(cbind(client_test[,-c(1,13:24)],client_test.FA))
View(client_test2)
View(client_test)
client_test2 = as.data.frame(cbind(client_test[,-c(1,7:24)],client_test.FA))
### Logistic model using FA results
library(ROCR)
library(caret)
client_train_new1 = as.data.frame(cbind(client_train[,c(2:6,25)], fa_client$scores))
client_train_new1[,2] = as.factor(client_train_new1[,2])
client_train_new1[,3] = as.factor(client_train_new1[,3])
client_train_new1[,4] = as.factor(client_train_new1[,4])
client_train_new1[,6] = as.factor(client_train_new1[,6])
View(client_train_new1)
client_log1 = glm(default.payment.next.month~0++LIMIT_BAL+EDUCATION+MARRIAGE+PA1+PA2, data = client_train_new1, family = binomial)
summary(client_log1)
## Check the accuracy using specificity and sensitiviy
predict_client_stat = predict(client_log1, newdata = client_train_new1[,c(1,3,4,7,8)], type = "response")
predictions = prediction(predict_client_stat, client_train_new1$default.payment.next.month)
sens = data.frame(x=unlist(performance(predictions, "sens")@x.values),
y=unlist(performance(predictions, "sens")@y.values))
spec = data.frame(x=unlist(performance(predictions, "spec")@x.values),
y=unlist(performance(predictions, "spec")@y.values))
sens %>% ggplot(aes(x,y)) +
geom_line() +
geom_line(data=spec, aes(x,y,col="red")) +
scale_y_continuous(sec.axis = sec_axis(~., name = "Specificity")) +
labs(x='Cutoff', y="Sensitivity") +
theme(axis.title.y.right = element_text(colour = "red"), legend.position="none")
#get the optimum threshold
sens = cbind(unlist(performance(predictions, "sens")@x.values), unlist(performance(predictions, "sens")@y.values))
spec = cbind(unlist(performance(predictions, "spec")@x.values), unlist(performance(predictions, "spec")@y.values))
cutoff_opt_client = sens[which.min(apply(sens, 1, function(x) min(colSums(abs(t(spec) - x))))), 1]
#Confusion matrix
predict_client_stat = ifelse(predict_client_stat>cutoff_opt_client,1,0)
conf_mat_client = table(predict_client_stat, client_train_new$default.payment.next.month)
conf_mat_client
specificity(conf_mat_client)
conf_mat_client = table(predict_client_stat, client_train_new1$default.payment.next.month)
conf_mat_client
specificity(conf_mat_client)
sensitivity(conf_mat_client)
#Apply model and cutoff on test data
predict_client_test2 = ifelse(predict(client_log1, newdata = client_test2[,c(1,3,4,12,13)], type = "response")>cutoff_opt_client,1,0)
View(client_test2)
View(client_test2)
#Apply model and cutoff on test data
predict_client_test2 = ifelse(predict(client_log1, newdata = client_test2[,c(1,3,4,6,7)], type = "response")>cutoff_opt_client,1,0)
client_test2$predict.paymentFA = predict_client_test2
View(client_test2)

cent = as.data.frame(cbind(c(2.5,8.75, 3), c(0.1, 0.1, 0.4)))
names(cent) = c("x", "y")
cent$cluster = c(1, 2, 3)
cent %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, colour = cent$cluster, shape = 2, stroke = 2) +
scale_color_manual(values = c('red', 'green', 'blue')) + labs(x='X', y="Y")
cent %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, colour = cent$cluster, shape = 2, stroke = 2) +
scale_color_manual(values = c('red', 'green', 'blue')) + labs(x='X', y="Y")
cent %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, colour = c(2,3,4), shape = 2, stroke = 2) +
scale_color_manual(values = c('red', 'green', 'blue')) + labs(x='X', y="Y")
#Plot the data (circles) and the initial centers (triangles)
my_dummy = my_dummy[,1:2]
my_dummy$cluster = c(1,1,1,1,1,1,1,1,1,1)
km_dummy = as.data.frame(rbind(cent, my_dummy))
km_dummy %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, shape = c(2,2,2,2,1,1,1,1,1,1,1,1,1,1), colour = c(2,3,4,1,1,1,1,1,1,1,1,1,1), stroke = c(2,2,2,2,1,1,1,1,1,1,1,1,1,1)) +
scale_color_manual(values = c('black', 'red', 'green', 'blue')) + labs(x='X', y="Y")
km_dummy %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, shape = c(2,2,2,1,1,1,1,1,1,1,1,1,1), colour = c(2,3,4,1,1,1,1,1,1,1,1,1,1), stroke = c(2,2,2,1,1,1,1,1,1,1,1,1,1)) +
scale_color_manual(values = c('black', 'red', 'green', 'blue')) + labs(x='X', y="Y")
dist(km_dummy)
## K Means clustering
#Lets cluster the dummy into 3 clusters, and choose the initiation for the centers
cent = as.data.frame(cbind(c(2.5,8.75, 2.5), c(0.1, 0.1, 0.4)))
names(cent) = c("x", "y")
cent$cluster = c(1, 2, 3)
cent %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, colour = c(2,3,4), shape = 2, stroke = 2) +
scale_color_manual(values = c('red', 'green', 'blue')) + labs(x='X', y="Y")
#Plot the data (circles) and the initial centers (triangles)
my_dummy = my_dummy[,1:2]
my_dummy$cluster = c(1,1,1,1,1,1,1,1,1,1)
km_dummy = as.data.frame(rbind(cent, my_dummy))
km_dummy %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, shape = c(2,2,2,1,1,1,1,1,1,1,1,1,1), colour = c(2,3,4,1,1,1,1,1,1,1,1,1,1), stroke = c(2,2,2,1,1,1,1,1,1,1,1,1,1)) +
scale_color_manual(values = c('black', 'red', 'green', 'blue')) + labs(x='X', y="Y")
dist(km_dummy)
kmean_1st = kmeans(my_dummy, centers = cent, iter.max = 100)
## K Means clustering
#Lets cluster the dummy into 3 clusters, and choose the initiation for the centers
cent = as.data.frame(cbind(c(2.5,8.75, 1), c(0.1, 0.1, 0.4)))
names(cent) = c("x", "y")
cent$cluster = c(1, 2, 3)
cent %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, colour = c(2,3,4), shape = 2, stroke = 2) +
scale_color_manual(values = c('red', 'green', 'blue')) + labs(x='X', y="Y")
#Plot the data (circles) and the initial centers (triangles)
my_dummy = my_dummy[,1:2]
my_dummy$cluster = c(1,1,1,1,1,1,1,1,1,1)
km_dummy = as.data.frame(rbind(cent, my_dummy))
km_dummy %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, shape = c(2,2,2,1,1,1,1,1,1,1,1,1,1), colour = c(2,3,4,1,1,1,1,1,1,1,1,1,1), stroke = c(2,2,2,1,1,1,1,1,1,1,1,1,1)) +
scale_color_manual(values = c('black', 'red', 'green', 'blue')) + labs(x='X', y="Y")
## K Means clustering
#Lets cluster the dummy into 3 clusters, and choose the initiation for the centers
cent = as.data.frame(cbind(c(5,8.75, 1), c(0.2, 0.1, 0.4)))
names(cent) = c("x", "y")
cent$cluster = c(1, 2, 3)
cent %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, colour = c(2,3,4), shape = 2, stroke = 2) +
scale_color_manual(values = c('red', 'green', 'blue')) + labs(x='X', y="Y")
#Plot the data (circles) and the initial centers (triangles)
my_dummy = my_dummy[,1:2]
my_dummy$cluster = c(1,1,1,1,1,1,1,1,1,1)
km_dummy = as.data.frame(rbind(cent, my_dummy))
km_dummy %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, shape = c(2,2,2,1,1,1,1,1,1,1,1,1,1), colour = c(2,3,4,1,1,1,1,1,1,1,1,1,1), stroke = c(2,2,2,1,1,1,1,1,1,1,1,1,1)) +
scale_color_manual(values = c('black', 'red', 'green', 'blue')) + labs(x='X', y="Y")
dist(km_dummy)
kmean_1st = kmeans(my_dummy, centers = cent, iter.max = 100)
kmean_1st = kmeans(my_dummy, centers = cent, iter.max = 1)
dist(km_dummy)
#Place the each observation to their respective nearest cluster
km_cl1 = c(1,2,3,1,1,2,1,1,2,2,3,1,1)
km_dummy$cluster1 = km_cl1
km_dummy %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, shape = c(2,2,2,1,1,1,1,1,1,1,1,1,1), colour = km_dummy$cluster1+1, stroke = c(2,2,2,1,1,1,1,1,1,1,1,1,1)) +
scale_color_manual(values = c('black', 'red', 'green', 'blue')) + labs(x='X', y="Y")
km_dummy[-1:3,] %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, shape = c(2,2,2,1,1,1,1,1,1,1,1,1,1), colour = km_dummy$cluster1+1, stroke = c(2,2,2,1,1,1,1,1,1,1,1,1,1)) +
scale_color_manual(values = c('black', 'red', 'green', 'blue')) + labs(x='X', y="Y")
km_dummy[-c(1:3),] %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, shape = c(2,2,2,1,1,1,1,1,1,1,1,1,1), colour = km_dummy$cluster1+1, stroke = c(2,2,2,1,1,1,1,1,1,1,1,1,1)) +
scale_color_manual(values = c('black', 'red', 'green', 'blue')) + labs(x='X', y="Y")
km_dummy[4:13,] %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, shape = c(2,2,2,1,1,1,1,1,1,1,1,1,1), colour = km_dummy$cluster1+1, stroke = c(2,2,2,1,1,1,1,1,1,1,1,1,1)) +
scale_color_manual(values = c('black', 'red', 'green', 'blue')) + labs(x='X', y="Y")
km_dummy[4:13,] %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, shape = c(1,1,1,1,1,1,1,1,1,1), colour = km_dummy[4:13,]$cluster1+1, stroke = c(1,1,1,1,1,1,1,1,1,1)) +
scale_color_manual(values = c('black', 'red', 'green', 'blue')) + labs(x='X', y="Y")
km_dummy[4:13,] %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 3, shape = c(1,1,1,1,1,1,1,1,1,1), colour = km_dummy[4:13,]$cluster1+1, stroke = c(1,1,1,1,1,1,1,1,1,1)) +
scale_color_manual(values = c('black', 'red', 'green', 'blue')) + labs(x='X', y="Y")
km_dummy[4:13,] %>% ggplot(aes(x,y)) +
geom_point(alpha = 2, size = 3, shape = c(1,1,1,1,1,1,1,1,1,1), colour = km_dummy[4:13,]$cluster1+1, stroke = c(1,1,1,1,1,1,1,1,1,1)) +
scale_color_manual(values = c('black', 'red', 'green', 'blue')) + labs(x='X', y="Y")
km_dummy[4:13,] %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 1, shape = c(1,1,1,1,1,1,1,1,1,1), colour = km_dummy[4:13,]$cluster1+1, stroke = c(1,1,1,1,1,1,1,1,1,1)) +
scale_color_manual(values = c('black', 'red', 'green', 'blue')) + labs(x='X', y="Y")
km_dummy[4:13,] %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, shape = c(1,1,1,1,1,1,1,1,1,1), colour = km_dummy[4:13,]$cluster1+1, stroke = c(1,1,1,1,1,1,1,1,1,1)) +
scale_color_manual(values = c('black', 'red', 'green', 'blue')) + labs(x='X', y="Y")
#Compute the new centers for each cluster
km_dummy[4:13,] %>% group_by(cluster1)
#Compute the new centers for each cluster
km_dummy[4:13,] %>% group_by(cluster1) %>%
summarise("1" = mean(as.numeric(1)))
#Compute the new centers for each cluster
km_dummy[4:13,] %>% group_by(cluster1) %>%
summarise(x = mean(as.numeric(x)))
#Compute the new centers for each cluster
km_dummy[4:13,] %>% group_by(cluster1) %>%
summarise(x = mean(as.numeric(x)),
y = mean(y))
km_dummy[1:3,1] = c(4.41,8.1,0.278)
km_dummy[1:3,2] = c(0.267, 0.150, .381)
km_dummy %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, shape = c(2,2,2,1,1,1,1,1,1,1,1,1,1), colour = km_dummy$cluster1+1, stroke = c(2,2,2,1,1,1,1,1,1,1,1,1,1)) +
scale_color_manual(values = c('black', 'red', 'green', 'blue')) + labs(x='X', y="Y")
km_dummy %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, shape = c(2,2,2,1,1,1,1,1,1,1,1,1,1), colour = km_dummy$cluster1+1) +
scale_color_manual(values = c('black', 'red', 'green', 'blue')) + labs(x='X', y="Y")
km_dummy %>% ggplot(aes(x,y)) +
geom_point(alpha = 1, size = 2, shape = c(2,2,2,1,1,1,1,1,1,1,1,1,1), colour = km_dummy$cluster1+1, stroke = c(1,1,1,1,1,1,1,1,1,1,1,1,1)) +
scale_color_manual(values = c('black', 'red', 'green', 'blue')) + labs(x='X', y="Y")
kmean_1st = kmeans(my_dummy, centers = cent, iter.max = 1)
kmean_1st$cluster
kmean_1st = kmeans(my_dummy, centers = cent, iter.max = 100)
kmean_1st$cluster
library(cluster)
### Clustering ###
library(factoextra)
#Optimal number of clusters based on several methods
fviz_nbclust(my_dummy, kmeans, method = "wss")
View(my_dummy)
#Optimal number of clusters based on several methods
fviz_nbclust(my_dummy[,1:2], kmeans, method = "wss")
View(my_dummy)
#Optimal number of clusters based on several methods
my_dummy = my_dummy[,1:2]
fviz_nbclust(my_dummy, kmeans, method = "wss")
fviz_nbclust(my_dummy, kmeans)
### Clustering ###
library(factoextra)
fviz_nbclust(my_dummy, kmeans)
library(ggplot2)
library(dplyr)
fviz_nbclust(my_dummy, kmeans, method = "wss", k.max = 5)
# PCA
pca_train1 = prcomp(client_train1[,-13], scale = TRUE)
View(pca_train1)
head(pca_train1$rotation, 4)
head(pca_train1$x, 4)
# Hierarchical
hier_client = hclust(pca_train1$x[,1:2], method = "complete")
pc_client$x
comp_client = pca_train1$x
comp_client = as.data.frame(pca_train1$x)
# Hierarchical
hier_client = hclust(comp_client[,1:2], method = "complete")
comp_client[,1:2]
# Take 1000 from training for example and learn the cluster pattern
set.seed(12345)
examples <- comp_client$PC1 %>%
createDataPartition(p = 0.2, list = FALSE)
library(caret)
examples <- comp_client$PC1 %>%
createDataPartition(p = 0.2, list = FALSE)
exm_comp  = PimaIndiansDiabetes2[examples, ]
exm_comp  = comp_client[examples, ]
comp_client = as.data.frame(pca_train1$x[,1:2])
# Take 1000 from training for example and learn the cluster pattern
set.seed(12345)
examples = comp_client$PC1 %>%
createDataPartition(p = 0.2, list = FALSE)
exm_comp  = comp_client[examples,]
View(exm_comp)
View(exm_comp)
View(comp_client)
# Use Hierarchical to see how the good is 2 cluster in the data
hier_client = hclust(exm_comp, method = "complete")
# Take 1000 from training for example and learn the cluster pattern
set.seed(12345)
examples = comp_client$PC1 %>%
createDataPartition(p = 0.05, list = FALSE)
exm_comp  = comp_client[examples,]
# Use Hierarchical to see how the good is 2 cluster in the data
hier_client = hclust(exm_comp, method = "complete")
# Take 1000 from training for example and learn the cluster pattern
set.seed(12345)
examples = comp_client$PC1 %>%
createDataPartition(p = 0.025, list = FALSE)
exm_comp  = comp_client[examples,]
# Use Hierarchical to see how the good is 2 cluster in the data
hier_client = hclust(exm_comp, method = "complete")
# Take 1000 from training for example and learn the cluster pattern
set.seed(12345)
examples = comp_client$PC1 %>%
createDataPartition(p = 0.00625, list = FALSE)
exm_comp  = comp_client[examples,]
# Use Hierarchical to see how the good is 2 cluster in the data
hier_client = hclust(exm_comp, method = "complete")
# Use Hierarchical to see how the good is 2 cluster in the data
hier_client = hclust(exm_comp[1:10,], method = "complete")
exm_comp %>% mutate_if(is.numeric, ~round(., 3))
exm_comp = exm_comp %>% mutate_if(is.numeric, ~round(., 3))
# Use Hierarchical to see how the good is 2 cluster in the data
hier_client = hclust(exm_comp[1:10,], method = "complete")
exm_comp = na.omit(exm_comp)
# Use Hierarchical to see how the good is 2 cluster in the data
hier_client = hclust(exm_comp, method = "complete")
View(client_train1)
# Use Hierarchical to see how the good is 2 cluster in the data
hier_client = hclust(client_train1[1:1000,1:2], method = "complete")
# PCA
pca_train1 = prcomp(client_train1[,-13], scale = TRUE) #use first 2 components
comp_client = as.data.frame(pca_train1$x[,1:2])
# Take 1000 from training for example and learn the cluster pattern
set.seed(12345)
examples = comp_client$PC1 %>%
createDataPartition(p = 0.00625, list = FALSE)
# Take 1000 from training for example and learn the cluster pattern
set.seed(12345)
examples = comp_client$PC1 %>%
createDataPartition(p = 0.05, list = FALSE)
exm_comp  = comp_client[examples,]
# Use Hierarchical to see how the good is 2 cluster in the data
hier_client = hclust(dist(exm_comp), method = "complete")
plot(hier_client)
# Result from the example doesnt show a good indication that the data can be clustered into 2 clusters
# Lets apply this k means and try to look for optimal k
# Using k means
km_client = kmeans(comp_client, iter.max = 1000L, nstart = 100L)
# Result from the example doesnt show a good indication that the data can be clustered into 2 clusters
# Lets apply this k means and try to look for optimal k
# Using k means
km_client = kmeans(comp_client, iter.max = 1000L)
# Result from the example doesnt show a good indication that the data can be clustered into 2 clusters
# Lets apply this k means and try to look for optimal k
# Using k means
km_client = kmeans(comp_client, iter.max = 1000L, 2)
# Result from the example doesnt show a good indication that the data can be clustered into 2 clusters
# Lets apply this k means and try to look for optimal k
# Using k means
fviz_nbclust(comp_client, kmeans, method = "wss", k.max = 10)
# Result from the example doesnt show a good indication that the data can be clustered into 2 clusters
# Lets apply this k means and try to look for optimal k
# Using k means
fviz_nbclust(comp_client, kmeans, method = "wss", k.max = 6)
# Result from the example doesnt show a good indication that the data can be clustered into 2 clusters
# Lets apply this k means and try to look for optimal k
# Using k means
fviz_nbclust(comp_client, exm_comp, method = "wss", k.max = 6)
# Result from the example doesnt show a good indication that the data can be clustered into 2 clusters
# Lets apply this k means and try to look for optimal k
# Using k means
fviz_nbclust(exm_comp, kmeans, method = "wss", k.max = 6)
# Lets try different quality comparison method
fviz_nbclust(exm_comp, kmeans, method = "silhouette", k.max = 6)
gap_stat = clusGap(exm_comp, FUN = kmeans, nstart = 25,
K.max = 6, B = 50)
library(cluster)
gap_stat = clusGap(exm_comp, FUN = kmeans, nstart = 25,
K.max = 6, B = 50)
fviz_gap_stat(gap_stat)
# We conclude that from the example 2 clusters is the optimal number of cluster
km_client = kmeans(comp_client, iter.max = 1000L, centers = 2, nstart = 100)
fviz_cluster(km_client)
fviz_cluster(km_client, comp_client)
fviz_cluster(km_client, comp_client, stand = F)
fviz_cluster(km_client, comp_client, stand = F, geom = "point")
km_client$cluster
View(client_train1)
# Extract the result and compare to actual label
result_kmcluster = as.data.frame(cbind(client_train$default.payment.next.month, km_client$cluster))
View(result_kmcluster)
# Extract the result and compare to actual label
result_kmcluster = as.data.frame(cbind(client_train$default.payment.next.month, km_client$cluster-1))
## Comparison
# We'll use specificity and sensitivity to measure our accuracy of clustering prediction
table(result_kmcluster)
# Extract the result and compare to actual label
result_kmcluster = as.data.frame(cbind(client_train$default.payment.next.month, km_client$cluster%%2))
names(result_kmcluster) = c("actual label", "cluster")
## Comparison
# We'll use specificity and sensitivity to measure our accuracy of clustering prediction
table(result_kmcluster)
result_kmcluster2 = as.data.frame(cbind(client_train$default.payment.next.month, km_client$cluster-1))
names(result_kmcluster2) = c("actual label", "cluster")
table(result_kmcluster2)
specificity(result_kmcluster)
specificity(table(result_kmcluster))
sensitivity(table(result_kmcluster))
specificity(table(result_kmcluster2))
sensitivity(table(result_kmcluster2))
result_kmcluster2
table(result_kmcluster2)
View(result_kmcluster2)
# For the sake of simplicity as to get the points easier to be understood,
#we'll start to illustrate the methods from the linear discriminant.
# We use iris data
data("iris")
iris
iris = iris[1:100,]
View(iris)
iris = iris[1:100,-c(3:4)]
View(iris)
iris = iris[1:100,-c(2,4)]
# For the sake of simplicity as to get the points easier to be understood,
#we'll start to illustrate the methods from the linear discriminant.
# We use iris data
data("iris")
iris = iris[1:100,-c(2,4)]
plot(iris$Sepal.Length, iris$Petal.Length)
library(dplyr)
library(ggplot2)
iris %>% ggplot(aes(Sepal.Length, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='X', y="Y")
iris$Species = as.factor(iris$Species)
iris %>% ggplot(aes(Sepal.Length, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$Species) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='X', y="Y")
iris$spec = c(rep(1, 50), rep(2,50))
iris %>% ggplot(aes(Sepal.Length, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
install.packages("klaR")
install.packages("devtools")
#Installing a package outside cran
options(repos = c(
fawda123 = 'https://fawda123.r-universe.dev',
CRAN = 'https://cloud.r-project.org'))
# Install ggord
install.packages('ggord')
### Discriminant analysis ###
library(klaR)
library(psych)
library(MASS)
library(ggord)
library(devtools)
# Linear DA
lda_iris = lda(iris$Species ~ iris[,1:2])
# Linear DA
lda_iris = lda(Species ~., data = iris[,-4] )
lda_iris$prior
lda_iris$counts
lda_iris$means
lda_iris$scaling
lda_iris$xlevels
lda_iris
line(iris$Sepal.Length)
plot(line(iris$Sepal.Length))
iris %>% ggplot(aes(Sepal.Length, 0), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
iris %>% ggplot(aes(0, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
iris %>% ggplot(aes(Sepal.Length, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
geom_abline(lda_iris$scaling) + scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
lda_iris$scaling
as.vector(lda_iris$scaling)
lda_iris$lev
lda_iris$svd
lda_iris$call
iris[,1:2]%*%lda_iris$scaling
iris[,1:2]%*%as.vector(lda_iris$scaling)
iris[,1:2]%*%matrix(lda_iris$scaling)
matrix(lda_iris$scaling)
iris[,1:2]
matrix(iris[,1:2])
as.matrix(iris[,1:2])
as.matrix(iris[,1:2])%*%matrix(lda_iris$scaling)
abline(b= c(1,2))
plot(lda_iris)
lda_pred = data.frame(iris[,1:2], y = as.numeric(predict(lda_iris, iris[,1:2])$class))
iris %>% ggplot(aes(x = Sepal.Length, y = Petal.Length, color = Species)) + geom_point() +
stat_contour(iris, aes(x = Sepal.Length, y = Petal.Length , z = Species), data = lda_pred) + ggtitle(“LDA Decision Boundaries”)
iris %>% ggplot(aes(x = Sepal.Length, y = Petal.Length, color = Species)) + geom_point() +
stat_contour(aes(x = iris$Sepal.Length, y = iris$Petal.Length , z = iris$Species), data = lda_pred) +
ggtitle(“LDA Decision Boundaries”)
iris %>% ggplot(aes(x = Sepal.Length, y = Petal.Length, color = Species)) + geom_point() +
stat_contour(aes(x = iris$Sepal.Length, y = iris$Petal.Length , z = iris$Species), data = lda_pred)
View(lda_pred)
lda_pred = data.frame(iris[,1:2], Species = as.numeric(predict(lda_iris, iris[,1:2])$class))
iris %>% ggplot(aes(x = Sepal.Length, y = Petal.Length, color = Species)) + geom_point() +
stat_contour(aes(x = iris$Sepal.Length, y = iris$Petal.Length , z = iris$Species), data = lda_pred)
lda_pred = data.frame(iris[,1:2], Species = as.factor(predict(lda_iris, iris[,1:2])$class))
iris %>% ggplot(aes(x = Sepal.Length, y = Petal.Length, color = Species)) + geom_point() +
stat_contour(aes(x = iris$Sepal.Length, y = iris$Petal.Length , z = iris$Species), data = lda_pred)
iris %>% ggplot(aes(x = Sepal.Length, y = Petal.Length, color = Species)) + geom_point() +
stat_contour(aes(x = iris$Sepal.Length, y = iris$Petal.Length , z = iris$spec), data = lda_pred)
str(iris$spec)
lda_pred = data.frame(iris[,1:2], Species = as.numeric(predict(lda_iris, iris[,1:2])$class))
iris %>% ggplot(aes(x = Sepal.Length, y = Petal.Length, color = Species)) + geom_point() +
stat_contour(aes(x = iris$Sepal.Length, y = iris$Petal.Length , z = iris$spec), data = lda_pred)
iris %>% ggplot(aes(x = Sepal.Length, y = Petal.Length, color = Species)) + geom_point() +
stat_function(aes(x = iris$Sepal.Length, y = iris$Petal.Length , z = iris$spec), data = lda_pred)
#Do prediction to actual data
contour_data = expand.grid(X1 = seq(-8, 8, length = 300), X2 = seq(-8, 8, length = 300))
lda_pred = data.frame(contour_data, Species = as.numeric(predict(lda_iris, iris[,1:2])$class))
iris %>% ggplot(aes(Sepal.Length, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
iris %>% ggplot(aes(x = Sepal.Length, y = Petal.Length, color = Species)) + geom_point() +
stat_(aes(x = iris$Sepal.Length, y = iris$Petal.Length , z = iris$spec), data = lda_pred)
iris %>% ggplot(aes(x = Sepal.Length, y = Petal.Length, color = Species)) + geom_point() +
stat_contour(aes(x = iris$Sepal.Length, y = iris$Petal.Length , z = iris$spec), data = lda_pred)
da_predict = data.frame(contour_data, y = as.numeric(predict(lda_iris, contour_data)$class))
#Do prediction to actual data
contour_data = expand.grid(iris[,1:2])
da_predict = data.frame(contour_data, y = as.numeric(predict(lda_iris, contour_data)$class))
rm(da_predict)
lda_predict = data.frame(contour_data, y = as.numeric(predict(lda_iris, contour_data)$class))
iris %>% ggplot(aes(x = Sepal.Length, y = Petal.Length, color = Species)) + geom_point() +
stat_contour(aes(x = iris$Sepal.Length, y = iris$Petal.Length , z = iris$spec), data = lda_pred)
rm(lda_pred)
lda_predict = data.frame(contour_data, y = as.numeric(predict(lda_iris, contour_data)$class))
View(lda_predict)
#Do prediction to actual data
lda_predict = data.frame(iris[,1:2], y = as.numeric(predict(lda_iris, contour_data)$class))
#Do prediction to actual data
lda_predict = data.frame(iris[,1:2], Species = as.numeric(predict(lda_iris, iris[,1:2])$class))
View(lda_predict)
iris %>% ggplot(aes(x = Sepal.Length, y = Petal.Length, color = Species)) + geom_point() +
stat_smooth(aes(x = iris$Sepal.Length, y = iris$Petal.Length , z = iris$Species), data = lda_pred)
iris %>% ggplot(aes(x = Sepal.Length, y = Petal.Length, color = Species)) + geom_point() +
stat_smooth(aes(x = iris$Sepal.Length, y = iris$Petal.Length , z = iris$Species), data = lda_predict)
rm(lda_predict)
iris %>% ggplot(aes(Sepal.Length, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
iris %>% ggplot(aes(Sepal.Length, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
geom_segment(aes(x = 0, xend = 8, y = 0, yend = 6)) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
iris %>% ggplot(aes(Sepal.Length, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
geom_segment(aes(x = 4, xend = 8, y = 0, yend = 6)) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
iris %>% ggplot(aes(Sepal.Length, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
geom_segment(aes(x = 4, xend = 8, y = 0, yend = 5.2)) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
iris %>% ggplot(aes(Sepal.Length, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
geom_segment(aes(x = 4.3, xend = 8, y = 0, yend = 5.2)) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
iris %>% ggplot(aes(Sepal.Length, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
geom_segment(aes(x = 4.3, xend = 7.2, y = 0, yend = 5.2)) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
lda_iris$scaling
# Plot of the discriminant function line
iris %>% ggplot(aes(Sepal.Length, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
geom_segment(aes(x = 4.3, xend = 7.2, y = 0, yend = 8.2)) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
# Plot for discriminant between 2 categories
plot(lda_iris)
# Plot for discriminant between 2 categories
plot(lda_iris, col = as.integer(iris$Species))
# Plot for discriminant between 2 categories
plot(lda_iris dimen)
# Plot for discriminant between 2 categories
plot(lda_iris)
lda_iris$xlevels
str(lda_iris$xlevels)
unlist(lda_iris$xlevels)
lda_iris$call
lda_iris$scaling
partimat(Species ~. , data = iris[,1:3], method = "lda")
iris %>% ggplot(aes(Sepal.Length, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
partimat(Species ~. , data = iris[,1:3], method = "lda")
# Plot of the discriminant function line
iris %>% ggplot(aes(Sepal.Length, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
geom_segment(aes(x = 4.3, xend = 7.2, y = 2.5, yend = 3.5)) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
# Plot of the discriminant function line
iris %>% ggplot(aes(Sepal.Length, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
geom_segment(aes(x = -4.3, xend = -7.2, y = 2.5, yend = 3.5)) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
# Plot of the discriminant function line
iris %>% ggplot(aes(Sepal.Length, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
geom_segment(aes(x = 5.5, xend = 6.5, y = 1, yend = 5)) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
# Plot of the discriminant function line
iris %>% ggplot(aes(Sepal.Length, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
geom_segment(aes(x = 4.3, xend = 7.2, y = 2.5, yend = 3.5)) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
iris %>% ggplot(aes(Sepal.Length, Petal.Length), color = Species) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
geom_segment(aes(x = 5.5, xend = 6.5, y = 1, yend = 3)) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
# Get the coordinate for lda plot
predict(lda_iris
# Get the coordinate for lda plot
predict(lda_iris)
# Get the coordinate for lda plot
predict(lda_iris)
# Get the coordinate for lda plot
plot(predict(lda_iris)$posterior)
partimat(Species ~. , data = iris[,1:3], method = "lda")
# Get the coordinate for lda plot
plot(predict(lda_iris)$posterior)
# Get the coordinate for lda plot
plot(lda_iris
# Get the coordinate for lda plot
plot(lda_iris)
# Get the coordinate for lda plot
plot(lda_iris)
ggplot(aes(x = predict(lda_iris)$x, y =0)) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec)
ggplot(aes(x = predict(lda_iris)$x, y =0)) + geom_point(alpha = 1.8, size = 2.5)
ggplot(aes(x = predict(lda_iris)$x, y =0)) + geom_point()
iris$pred = predict(lda_iris)$x
View(iris)
iris %>% ggplot(aes(x = pred, y = 0)) + geom_point()
iris %>% ggplot(aes(x = pred, y = 0)) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Feature_1', y="Feature_2")
iris %>% ggplot(aes(x = pred, y = 0)) + geom_point(alpha = 1.8, size = 2.5, col = iris$spec) +
scale_color_manual(values = c('black', 'red', 'green')) + labs(x='Linear_Disc1', y=" ")
plot(lda_iris)

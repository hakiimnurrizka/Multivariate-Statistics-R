std_adapt1 = std_adapt[,c(3,4,9,11,14)]
tbl = structable(std_adapt1)
tbl #Very sparse contingency table
mc.std = MCA(std_adapt1, graph = F)
fviz_screeplot(mc.std, addlabels = T) #First 2 dim represent 35% total inertia (information)
fviz_mca_var(mc.std, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = T)#Plot for each categories in each variables, color shows the quality of representation
#Lets check the distance for each variables' categories
get_mca_var(mc.std)
#Lets check the distance for each variables' categories
mc.std1 = get_mca_var(mc.std)
mc.std1$v.test
mc.std1$coord
dist(mc.std1$coord)
dist(mc.std1$coord[,c(1:3,11:13)])
dist(mc.std1$coord[c(1:3,11:13),])
#From the distance matrix above, we can see a trend where the higher the level of education, relative
#adaptability seems to lean towards higher adaptability level
dist(mc.std1$coord[c(4:5,11:13),])
fviz_mca_var(mc.std, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = T)#Plot for each categories in each variables, color shows the quality of representation
View(std_adapt1)
#Lets use a simple CA first to analyze pair of two variables : adaptability level and education level,
#adaptability level and institution type for the school
sc.std1 = CA(std_adapt1[,c(1,5)])
#Lets use a simple CA first to analyze pair of two variables : adaptability level and education level,
#adaptability level and institution type for the school
sc.std1 = CA(table(std_adapt1[,c(1,5)]))
sc.std2 = CA(table(std_adapt1[,c(2,5)]))
sc.std2 = CA(table(std_adapt1[,c(2,5)]))
sc.std2 = CA(table(std_adapt1[,c(2,5)]), graph = T)
#Lets use a simple CA first to analyze pair of two variables : adaptability level and education level,
#adaptability level and institution type for the school
sc.std1 = CA(table(std_adapt1[,c(1,5)]), graph = F)
sc.std2 = CA(table(std_adapt1[,c(2,5)]), graph = F)
fviz_screeplot(sc.std1, addlabels = T)
fviz_screeplot(sc.std2, addlabels = T)
table(std_adapt1[,c(2,5)])
fviz_mca_var(sc.std1, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = T)
fviz_mca_var(sc.std1, repel = T)
fviz_ca_biplot(sc.std1, repel = T)
fviz_ca_biplot(sc.std2, repel = T)
fviz_ca(sc.std2, repel = T)
#Lets use a simple CA first to analyze pair of two variables : adaptability level and education level
rm(sc.std2)
#Lets use a simple CA first to analyze pair of two variables : adaptability level and education level
sc.std1 = CA(table(std_adapt1[,c(1,5)]), graph = F)
fviz_screeplot(sc.std1, addlabels = T)
fviz_ca_biplot(sc.std1, repel = T)
sc.std1 = get_ca(sc.std1)
sc.std1$coord
sc.std1$inertia
#Lets compare distance with distance matrix
sc.std1 = get_ca_col(sc.std1) #Extract CA result on column
sc.std2 = get_ca_row(sc.std1) #Extract CA result on row
sc.std2
sc.std2 = get_ca_row(sc.std1) #Extract CA result on row
sc.std1$coord
#Lets use a simple CA first to analyze pair of two variables : adaptability level and education level
sc.std1 = CA(table(std_adapt1[,c(1,5)]), graph = F)
fviz_screeplot(sc.std1, addlabels = T) #First dimension represent 88% total inertia (information)
fviz_ca_biplot(sc.std1, repel = T)
#Lets compare distance with distance matrix
sc.std1 = get_ca_col(sc.std1) #Extract CA result on column
sc.std2 = get_ca_row(sc.std1) #Extract CA result on row
#Lets use a simple CA first to analyze pair of two variables : adaptability level and education level
sc.std1 = CA(table(std_adapt1[,c(1,5)]), graph = F)
fviz_screeplot(sc.std1, addlabels = T) #First dimension represent 88% total inertia (information)
fviz_ca_biplot(sc.std1, repel = T)
#Lets compare distance with distance matrix
sc.std2 = get_ca_col(sc.std1) #Extract CA result on column
sc.std3 = get_ca_row(sc.std1) #Extract CA result on row
sc.std3$coord
sc.std2$coord
coord.sc = rbind(sc.std2$coord, sc.std3$coord)
dist(coord.sc)#Distance between adaptability level and education level
dist(mc.std1$coord[c(1:3,11:13),])#Distance between adaptability level and education level
##Multiple correlation
#We'll visualize the correlation within each set between each pair then for both sets at once
pairs.panels(gluc_1)
library(psych)
library(MVN)
library(CCA)
library(CCP)
library(ggplot2)
pairs.panels(glucose) #complete correlation matrix
##Multiple correlation
#We'll visualize the correlation within each set between each pair then for both sets at once
pairs.panels(gluc_1)
pairs.panels(gluc_2)
#Correlation above used pearson correlation coefficient, which means it is only appropriate IF
#the data has normal distribution.
#This assumption is also needed in canonical correlation, the difference is we need a multivariate
#normal assumption instead of univariate one.
#Let's check normality for each variables AND multivariate normality for each set
mvn(gluc_1) #multivariate normality and 2 individuals normality is not accepted
##Canonical correlation
#First we'll use the function from default and CCA then we proceed with breaking down the formula.
cancor(gluc_1, gluc_2) #default function from stats library
can.gluc$cor #Canonical correlation, sorted from highest value
mlm.gluc$coefficients #beta matrix
summary(mlm.gluc)
View(std_adapt)
chisq.test(table(std_adapt[,c(7,9)]))
View(iris)
anova(lm1.iris2)
lm1.iris2 = lm(Sepal.Width ~ Species*soil)
attach(iris)
lm1.iris2 = lm(Sepal.Width ~ Species*soil)
anova(lm1.iris2)
plot(mlm.gluc)
summary(mlm.gluc)
##Multiple correlation
#We'll visualize the correlation within each set between each pair then for both sets at once
pairs.panels(gluc_1)
lm.gluc = lm( V6 ~ V1 + V2 + V3, data = glucose)
summary(lm.gluc)
plot(mlm.gluc)
plot(lm.gluc)
library(readxl)
coi_dosen <- read_excel("DATA - responden COI.xlsx",
sheet = "Dosen_IRT", col_types = c("skip",
"skip", "skip", "skip", "skip", "skip",
"skip", "numeric", "skip", "skip",
"skip", "skip", "skip", "numeric",
"skip", "skip", "skip", "skip", "numeric",
"skip", "skip", "skip", "skip", "skip",
"numeric", "skip", "skip", "skip",
"skip", "skip", "numeric"))
View(coi_dosen)
hier.coidosen = hclust(dist(coi_dosen), method = "complete")
hier.coidosen
plot(hier.coidosen)
fviz_nbclust(coi_dosen, hcut)
install.packages("factoextra")
install.packages("cluster")
###COI Analysis###
library(factoextra)
library(cluster)
library(dplyr)
library(ggplot2)
fviz_nbclust(coi_dosen, hcut)
# Decide by dendogram we have 2 clusters
clusterCut.dosen = cutree(hier.coidosen, 2)
coi_dosen$cluster.hier = clusterCut.dosen
clusterCut.dosen
# Decide by dendogram we have 2 clusters
cutree(hier.coidosen, 2)
coi_dosen %>% group_by(cluster.hier) %>% summarise(mean.kompetensi.komputer = mean(`Kompetensi Komputer`))
coi_dosen %>% group_by(cluster.hier) %>%
summarise(mean.kompetensi.komputer = mean(`Kompetensi Komputer`),
mean.kompetensi.sosial = mean(`Kompetensi Sosialisasi`),
mean.kompetensi.komunikasi = mean(`Kompetensi Komunikasi`),
mean.hasrat.moocs = mean(`Hasrat MOOCS`),
mean.daring = mean(`Kompetensi Komunikasi Daring`))
coi_dosen %>% group_by(cluster.hier) %>%
summarise(mean.komputer = mean(`Kompetensi Komputer`),
mean.sosial = mean(`Kompetensi Sosialisasi`),
mean.komunikasi = mean(`Kompetensi Komunikasi`),
mean.moocs = mean(`Hasrat MOOCS`),
mean.daring = mean(`Kompetensi Komunikasi Daring`))
#K Means
fviz_nbclust(coi_dosen, kmeans, method = "wss", k.max = 6)
fviz_nbclust(coi_dosen, kmeans, method = "silhouette", k.max = 6)
gap_stat.coidosen = clusGap(coi_dosen, FUN = kmeans, nstart = 25,
K.max = 6, B = 50)
fviz_gap_stat(gap_stat.coidosen)
gap_stat.coidosen = clusGap(coi_dosen, FUN = kmeans, nstart = 25,
K.max = 6, B = 100)
fviz_gap_stat(gap_stat.coidosen)
gap_stat.coidosen = clusGap(coi_dosen, FUN = kmeans, nstart = 25,
K.max = 6, B = 1000)
fviz_gap_stat(gap_stat.coidosen)
#KMeans seems to agree on 2 clusters as well
km_coidosen = kmeans(coi_dosen, iter.max = 1000, centers = 2, nstart = 100)
fviz_cluster(km_coidosen, stand = F, geom = "point")
fviz_cluster(km_coidosen, coi_dosen, stand = F, geom = "point")
km_coidosen$cluster
coi_dosen$cluster.km = km_coidosen$cluster
coi_dosen %>% group_by(cluster.km) %>%
summarise(mean.komputer = mean(`Kompetensi Komputer`),
mean.sosial = mean(`Kompetensi Sosialisasi`),
mean.komunikasi = mean(`Kompetensi Komunikasi`),
mean.moocs = mean(`Hasrat MOOCS`),
mean.daring = mean(`Kompetensi Komunikasi Daring`))
##Mahasiswa##
coi_mahasiswa = read_excel("DATA - responden COI.xlsx",
sheet = "Mahasiswa_IRT", col_types = c("skip",
"skip", "skip", "skip", "skip", "skip",
"skip", "numeric", "skip", "skip",
"skip", "skip", "skip", "numeric",
"skip", "skip", "skip", "skip", "numeric",
"skip", "skip", "skip", "skip", "skip",
"numeric", "skip", "skip", "skip",
"skip", "skip", "numeric"))
View(coi_mahasiswa)
#Hierarchical Clustering
hier.coimhs = hclust(dist(coi_mahasiswa), method = "complete")
fviz_nbclust(coi_mahasiswa, hcut)
# Decide by dendogram we have 2 clusters
clusterCut.mhs = cutree(hier.coimhs, 2)
coi_mahasiswa$cluster.hier = clusterCut.mhs
coi_mahasiswa %>% group_by(cluster.hier) %>%
summarise(mean.komputer = mean(`Kompetensi Komputer`),
mean.sosial = mean(`Kompetensi Sosialisasi`),
mean.komunikasi = mean(`Kompetensi Komunikasi`),
mean.moocs = mean(`Hasrat MOOCS`),
mean.daring = mean(`Kompetensi Komunikasi Daring`))
#K Means
fviz_nbclust(coi_mahasiswa, kmeans, method = "wss", k.max = 6)
fviz_nbclust(coi_mahasiswa, kmeans, method = "silhouette", k.max = 6)
gap_stat.coidosen = clusGap(coi_mahasiswa, FUN = kmeans, nstart = 25,
K.max = 6, B = 1000)
fviz_gap_stat(gap_stat.coidosen)
gap_stat.coimhs = clusGap(coi_mahasiswa, FUN = kmeans, nstart = 25,
K.max = 6, B = 1000)
fviz_gap_stat(gap_stat.coimhs)
#KMeans seems to agree on 2 clusters as well
km_coimhs = kmeans(coi_mhs, iter.max = 1000, centers = 2, nstart = 100)
fviz_cluster(km_coimhs, coi_mahasiswa, stand = F, geom = "point")
#KMeans seems to agree on 2 clusters as well
km_coimhs = kmeans(coi_mahasiswa, iter.max = 1000, centers = 2, nstart = 100)
fviz_cluster(km_coimhs, coi_mahasiswa, stand = F, geom = "point")
fviz_nbclust(coi_mahasiswa, kmeans, method = "silhouette", k.max = 6)
coi_mahasiswa$cluster.km = km_coimhs$cluster
coi_mahasiswa %>% group_by(cluster.km) %>%
summarise(mean.komputer = mean(`Kompetensi Komputer`),
mean.sosial = mean(`Kompetensi Sosialisasi`),
mean.komunikasi = mean(`Kompetensi Komunikasi`),
mean.moocs = mean(`Hasrat MOOCS`),
mean.daring = mean(`Kompetensi Komunikasi Daring`))
###COI Analysis###
library(factoextra)
library(cluster)
library(ggplot2)
library(dplyr)
library(psych)
#Reliability and validity
alpha(coi_dosen)
coi_dosen2 = read_excel("DATA - responden COI.xlsx",
sheet = "Dosen_IRT", col_types = c("skip",
"numeric", "numeric", "numeric", "numeric", "numeric",
"numeric", "skip", "numeric", "numeric",
"numeric", "numeric", "numeric", "skip",
"numeric", "numeric", "numeric", "numeric", "skip",
"numeric", "numeric", "numeric", "numeric", "numeric",
"skip", "numeric", "numeric", "numeric",
"numeric", "numeric", "skip"))
library(readxl)
coi_dosen2 = read_excel("DATA - responden COI.xlsx",
sheet = "Dosen_IRT", col_types = c("skip",
"numeric", "numeric", "numeric", "numeric", "numeric",
"numeric", "skip", "numeric", "numeric",
"numeric", "numeric", "numeric", "skip",
"numeric", "numeric", "numeric", "numeric", "skip",
"numeric", "numeric", "numeric", "numeric", "numeric",
"skip", "numeric", "numeric", "numeric",
"numeric", "numeric", "skip"))
#Reliability and validity
alpha(coi_dosen2)
colnames(coi_dosen2) = c("a1", "a2", "a3", "a4", "a5", "a6", "b1", "b2", "b3", "b4", "b5", "c1", "c2", "c3", "c4",
"d1", "d2", "d3", "d4", "d5", "e1", "e2", "e3", "e4", "e5")
#Reliability and validity
alpha(coi_dosen2)
#Reliability and validity
KMO(coi_dosen2)#Good reliability score
library(lavaan)
library(semTools)
install.packages("semTools")
install.packages("semPlot")
library(semTools)
library(semPlot)
model1 = "
komp =~ a1 + a2 + a3 + a4 + a5 + a6
sos =~ b1 + b2 + b3 + b4 + b5
komu =~ c1 + c2 + c3 + c4
moocs =~ d1 + d2 + d3 + d4 +d5
daring =~ e1 + e2 + e3 + e4 + e5"
cfact = cfa(model1, data=coi_dosen2 ,std.lv=TRUE)
summary(cfact, fit.measures = T, standardized = T)
library(corrplot)
#CFA
cor.plot(coi_dosen)
model1 = "
komp =~ a1 + a2 + a3 + a4 + a5 + a6
sos =~ b1 + b2 + b3 + b4 + b5
komu =~ c1 + c2 + c3 + c4
moocs =~ d1 + d2 + d3 + d4 +d5
daring =~ e1 + e2 + e3 + e4 + e5
komp ~~ 0*sos
komp ~~ 0*komu
komp ~~ 0*moocs
komp ~~ 0*daring"
cfact = cfa(model1, data=coi_dosen2 ,std.lv=TRUE)
cfact = cfa(model1, data=coi_dosen2 ,std.lv=TRUE, auto.cov.lv.x = F)
summary(cfact, fit.measures = T, standardized = T)
model1 = "
komp =~ a1 + a2 + a3 + a4 + a5 + a6
sos =~ b1 + b2 + b3 + b4 + b5
komu =~ c1 + c2 + c3 + c4
moocs =~ d1 + d2 + d3 + d4 +d5
daring =~ e1 + e2 + e3 + e4 + e5
siap =~ 1*sos + 1*komu + 1*moocs + 1*daring
siap~~siap"
cfact = cfa(model1, data=coi_dosen2)
summary(cfact, fit.measures = T, standardized = T)
model1 = "
komp =~ a1 + a2 + a3 + a4 + a5 + a6
sos =~ b1 + b2 + b3 + b4 + b5
komu =~ c1 + c2 + c3 + c4
moocs =~ d1 + d2 + d3 + d4 +d5
daring =~ e1 + e2 + e3 + e4 + e5
siap =~ a*sos + a*komu + a*moocs + a*daring
komp ~~ 0*siap"
cfact = cfa(model1, data=coi_dosen2, std.lv=TRUE, auto.cov.lv.x=FALSE)
summary(cfact, fit.measures = T, standardized = T)
semPaths(cfact, 'path', 'std', style = 'lisrel',
edge.color = 'black', intercepts = F)
semPaths(cfact, 'path', 'std', style = 'lisrel',
edge.color = 'black', intercepts = F)
alpha(coi_dosen2)#Good reliability score
coi_mahasiswa2 = read_excel("DATA - responden COI.xlsx",
sheet = "Mahasiswa_IRT", col_types = c("skip", "numeric", "numeric", "numeric",
"numeric", "numeric", "numeric", "skip",
"numeric", "numeric", "numeric", "numeric",
"numeric", "skip", "numeric", "numeric",
"numeric", "numeric", "skip",
"numeric", "numeric", "numeric", "numeric", "numeric",
"skip", "numeric", "numeric", "numeric",
"numeric", "numeric", "skip"))
colnames(coi_mahasiswa2) = c("a1", "a2", "a3", "a4", "a5", "a6", "b1", "b2", "b3", "b4", "b5", "c1", "c2", "c3", "c4",
"d1", "d2", "d3", "d4", "d5", "e1", "e2", "e3", "e4", "e5")
#Reliability and validity
KMO(coi_mahasiswa2)
alpha(coi_mahasiswa2)#Good reliability score
#CFA
cor.plot(coi_mahasiswa)
model2 = "
komp =~ a1 + a2 + a3 + a4 + a5 + a6
sos =~ b1 + b2 + b3 + b4 + b5
komu =~ c1 + c2 + c3 + c4
moocs =~ d1 + d2 + d3 + d4 +d5
daring =~ e1 + e2 + e3 + e4 + e5
siap =~ komp + sos + komu + moocs + daring"
cfact2 = cfa(model2, data=coi_dosen2, std.lv=TRUE, auto.cov.lv.x=FALSE)
summary(cfact2, fit.measures = T, standardized = T)
semPaths(cfact2, 'path', 'std', style = 'lisrel',
edge.color = 'black', intercepts = F)
model2 = "
komp =~ a1 + a2 + a3 + a4 + a5 + a6
sos =~ b1 + b2 + b3 + b4 + b5
komu =~ c1 + c2 + c3 + c4
moocs =~ d1 + d2 + d3 + d4 +d5
daring =~ e1 + e2 + e3 + e4 + e5
siap =~ a*komp + a*sos + a*komu + a*moocs + a*daring"
cfact2 = cfa(model2, data=coi_dosen2, std.lv=TRUE, auto.cov.lv.x=FALSE)
summary(cfact2, fit.measures = T, standardized = T)
cfact = cfa(model1, data=coi_dosen2, std.lv=TRUE, auto.cov.lv.x=FALSE)
summary(cfact, fit.measures = T, standardized = T)
#CFA
cor.plot(coi_dosen[,1:5])
model1 = "
komp =~ a1 + a2 + a3 + a4 + a5 + a6
sos =~ b1 + b2 + b3 + b4 + b5
komu =~ c1 + c2 + c3 + c4
moocs =~ d1 + d2 + d3 + d4 +d5
daring =~ e1 + e2 + e3 + e4 + e5
siap =~ a*sos + a*komu + a*moocs + a*daring
komp ~~ 0*siap"
cfact = cfa(model1, data=coi_dosen2, std.lv=TRUE, auto.cov.lv.x=FALSE)
summary(cfact, fit.measures = T, standardized = T)
semPaths(cfact, 'path', 'std', style = 'lisrel',
edge.color = 'black', intercepts = F)
#Reliability and validity
KMO(coi_mahasiswa2)
#Reliability and validity
KMO(coi_dosen2)
semPaths(cfact2, 'path', 'std', style = 'lisrel',
edge.color = 'black', intercepts = F)
model2 = "
komp =~ a1 + a2 + a3 + a4 + a5 + a6
sos =~ b1 + b2 + b3 + b4 + b5
komu =~ c1 + c2 + c3 + c4
moocs =~ d1 + d2 + d3 + d4 +d5
daring =~ e1 + e2 + e3 + e4 + e5
siap =~ komp + sos + komu + moocs + daring
siap~~siap"
cfact2 = cfa(model2, data=coi_dosen2, std.lv=TRUE, auto.cov.lv.x=FALSE)
summary(cfact2, fit.measures = T, standardized = T)
semPaths(cfact2, 'path', 'std', style = 'lisrel',
edge.color = 'black', intercepts = F)
coi_dosen3 = read_excel("DATA - responden COI.clean.xlsx",  sheet = "DOSEN (filter)")
library(readxl)
coi_dosen3 = read_excel("DATA - responden COI.clean.xlsx",  sheet = "DOSEN (filter)")
View(coi_dosen3)
#Chi square
chisq.test(coi_dosen3$`Pengalaman mengikuti MOOCs sebelumnya`, coi_dosen3$Kat_komp)
chisq.test(coi_dosen3$`Pengalaman mengikuti MOOCs sebelumnya`, coi_dosen3$Kat_sos)
chisq.test(coi_dosen3$`Pengalaman mengikuti MOOCs sebelumnya`, coi_dosen3$Kat_komu)
chisq.test(coi_dosen3$`Pengalaman mengikuti MOOCs sebelumnya`, coi_dosen3$Kat_MOOCS)
chisq.test(coi_dosen3$`Pengalaman mengikuti MOOCs sebelumnya`, coi_dosen3$Kat_komp)
kruskal.test(coi_dosen$`Kompetensi Komputer`~coi_dosen3$`Pengalaman mengikuti MOOCs sebelumnya`)
chisq.test(coi_dosen3$`Pengalaman mengikuti MOOCs sebelumnya`, coi_dosen3$Kat_daring)
chisq.test(coi_dosen3$`Pengalaman memfasilitasi pembelajaran daring di MOOCs sebelumnya`, coi_dosen3$Kat_komp)
chisq.test(coi_dosen3$`Pengalaman memfasilitasi pembelajaran daring di MOOCs sebelumnya`, coi_dosen3$Kat_sos)
chisq.test(coi_dosen3$`Pengalaman memfasilitasi pembelajaran daring di MOOCs sebelumnya`, coi_dosen3$Kat_komu)
chisq.test(coi_dosen3$`Pengalaman memfasilitasi pembelajaran daring di MOOCs sebelumnya`, coi_dosen3$Kat_MOOCS)
chisq.test(coi_dosen3$`Pengalaman memfasilitasi pembelajaran daring di MOOCs sebelumnya`, coi_dosen3$Kat_daring)
###multivariate normal distribution###
library(mvtnorm)
library(MVN)
library(JWileymisc)
library(MASS)
library(dplyr)
library(ellipse)
library(rgl)
##Simulating mutivariate normal (MN) distribution based on correlation/covariance matrix, means, and standard deviation
set.seed(100) #setting seed for pseudo-random initiation
#in a case where we have the covariance matrix, it is then become a straightforward method to simulate
#MN distribution. Meanwhile in case we have correlation matrix, we use the previous equation to transform
#correlation matrix into covariance matrix.
#library jwileymisc provide the function to simplify this transformation
v = matrix(c(1,.152,.096,.043,.109,
.152,1,.400,-.016,.297,
.096,.400,1,.092,.382,
.043,-.016,.092,1,.103,
.109,.297,.382,.103,1),5,5) #correlation matrix
sigma = c(.4421,1.0880,8.5073,.4700,1.1249) #vector of standard deviation
cov.matrix = cor2cov(v, sigma) #convert correlation into covariance matrix
#after getting the covariance matrix, we can use mvrnorm to simulate the MN distribution based on
#previous covariance matrix
#in addition we also have to provide the means vector
mu = c(.7337,2.7300,46.9970,2.6002,1.7491)
sim.mn.data = data.frame(mvrnorm(n = 500, Sigma = cov.matrix, mu = mu)) #simulated data with total 500 observations
str(sim.mn.data)
sim.mn.data = rename(sim.mn.data, low = X1, high = X2, throw = X3, dodge = X4, make = X5)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#since we are limited in 3dimensional representation, we are limited to only use until first 3 variables
#of the simulated data from before.
#it is guaranteed that marginal distributions from MN is always normal(see Methods_of_Multivariate_Analysis-_3rd_Edition Rencher & Christensen )
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,3], n = 150) #calculate kernel density estimate
image(sim.mn.kernel) #heatmap based on the kernel estimate
contour(sim.mn.kernel, add = T) #contour plot added above the heatmap
#next is the "clean" version of bivariate normal distribution which using ellipse function
#in addition, confidence intervals is also added
rho = cor(sim.mn.data[,1:2])
y_on_x = lm(high ~ low, sim.mn.data)    # Regression Y ~ X
x_on_y = lm(low ~ high, sim.mn.data)    # Regression X ~ Y
plot_legend = c("99% CI green", "95% CI red","90% CI blue",
"Y on X black", "X on Y brown")
plot(sim.mn.data[,1:2], xlab = "low", ylab = "high",
col = "dark blue",
main = "Bivariate Normal with Confidence Intervals")
lines(ellipse(rho), col="red")       # ellipse() from ellipse package
lines(ellipse(rho, level = .99), col="green")
lines(ellipse(rho, level = .90), col="blue")
abline(y_on_x)
abline(x_on_y, col="brown")
legend(3,1,legend=plot_legend,cex = .5, bty = "n")
#3d representation
persp(sim.mn.kernel, phi = 45, theta = 30, shade = .1, border = NA) # from base graphics package
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#Visualizing the simulated data can be ilustratedusing the following 1 dimension subset of previously generated matrix data.
hist(sim.mn.data[,1])
##Simulating mutivariate normal (MN) distribution based on correlation/covariance matrix, means, and standard deviation
set.seed(100) #setting seed for pseudo-random initiation
sim.mn.data = data.frame(mvrnorm(n = 5000, Sigma = cov.matrix, mu = mu)) #simulated data with total 5000 observations
str(sim.mn.data)
sim.mn.data = rename(sim.mn.data, low = X1, high = X2, throw = X3, dodge = X4, make = X5)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#Visualizing the simulated data can be ilustratedusing the following 1 dimension subset of previously generated matrix data.
hist(sim.mn.data[,1])
hist(sim.mn.data$high)
hist(sim.mn.data$throw)
View(sim.mn.data)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#Visualizing the simulated data can be ilustratedusing the following 1 dimension subset of previously generated matrix data.
hist(sim.mn.data$low)#var 1
hist(sim.mn.data$high)#var 2
hist(sim.mn.data$throw)#var 3
hist(sim.mn.data$dodge)#var 4
hist(sim.mn.data$make)#var 5
#since we are limited in 3dimensional representation, we are limited to only use until first 3 variables
#of the simulated data from before.
#it is guaranteed that marginal distributions from MN is always normal(see Methods_of_Multivariate_Analysis-_3rd_Edition Rencher & Christensen )
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,3], n = 150) #calculate kernel density estimate
image(sim.mn.kernel) #heatmap based on the kernel estimate
contour(sim.mn.kernel, add = T) #contour plot added above the heatmap
plot_legend = c("99% CI green", "95% CI red","90% CI blue",
"Y on X black", "X on Y brown")
plot(sim.mn.data[,1:2], xlab = "low", ylab = "high",
col = "dark blue",
main = "Bivariate Normal with Confidence Intervals")
lines(ellipse(rho), col="red")       # ellipse() from ellipse package
lines(ellipse(rho, level = .99), col="green")
lines(ellipse(rho, level = .90), col="blue")
#3d representation
persp(sim.mn.kernel, phi = 45, theta = 30, shade = .1, border = NA) # from base graphics package
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#Visualizing the simulated data can be ilustratedusing the following 1 dimension subset of previously generated matrix data.
hist(sim.mn.data$low)#var 1

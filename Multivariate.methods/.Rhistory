#Let's check normality for each variables AND multivariate normality for each set
mvn(gluc_1)
mvn(gluc_2) #multivariate normality and 2 individuals normality is not accepted
mvn(glucose)
cor(gluc_1)
cor(gluc_1, method = "spearman")
##Canonical correlation
#First we'll use the function from default and psych then we proceed with breaking down the formula.
cancor(gluc_1, gluc_2)
cc(gluc_1, gluc_2)
install.packages("CCA")
library(CCA)
cc(gluc_1, gluc_2)
plt.cc(gluc_1, gluc_2)
can.gluc = cc(gluc_1, gluc_2)
plt.cc(can.gluc)
plt.cc(can.gluc)
plt.cc(can.gluc, var.label = TRUE)
can.gluc$cor
#The plots above show the closeness for each variable and individual
#The next section, the correlations are tested for their significances
comput(gluc_1, gluc_2, can.gluc)
library(CCP)
install.packages("CCP")
library(CCP)
#The plots above show the closeness for each variable and individual
#The next section, the correlations are tested for their significances
p.asym(can.gluc$cor, dim(gluc_1)[1], length(gluc_1), length(gluc_2))#Inputs are : correlation coef,
#The plots above show the closeness for each variable and individual
#The next section, the correlations are tested for their significances
p.asym(can.gluc$cor, dim(gluc_1)[1], length(gluc_1), length(gluc_2),
tstat = "Wilks")#Inputs are : correlation coef,
#The plots above show the closeness for each variable and individual
#The next section, the correlations are tested for their significances
p.asym(can.gluc$cor, dim(gluc_1)[1], length(gluc_1), length(gluc_2),
tstat = "Hotelling")#Inputs are : correlation coef, num of observations+variables
#The result above shows that only when canonical correlation is built upon 3 dimensions, the correlations
#are significance
det(cor(glucose))
#The result above shows that only when canonical correlation is built upon 3 dimensions, the correlations
#are significance
det(cor(glucose))/(det(gluc_1)*det(gluc_2))
#The result above shows that only when canonical correlation is built upon 3 dimensions, the correlations
#are significance
det(cor(glucose))/(det(cor(gluc_1))*det(cor(gluc_2))
#The result above shows that only when canonical correlation is built upon 3 dimensions, the correlations
#are significance
det(cor(glucose))/(det(cor(gluc_1))*det(cor(gluc_2)))
#The result above shows that only when canonical correlation is built upon 3 dimensions, the correlations
#are significance
det(cor(glucose))/(det(cor(gluc_1))*det(cor(gluc_2)))
#The result above shows that only when canonical correlation is built upon 3 dimensions, the correlations
#are significance. This is equivalent with test for overall significance using 3 dimensions
#Now we discuss about the coefficient for canonical correlation
can.gluc$xcoef
can.gluc$ycoef
##Canonical correlation
#First we'll use the function from default and CCA then we proceed with breaking down the formula.
cancor(gluc_1, gluc_2) #default function from stats library
##Canonical correlation
#First we'll use the function from default and CCA then we proceed with breaking down the formula.
cancor(gluc_1, gluc_2, xcenter = FALSE, ycenter = FALSE) #default function from stats library
##Canonical correlation
#First we'll use the function from default and CCA then we proceed with breaking down the formula.
cancor(gluc_1, gluc_2) #default function from stats library
#The result above shows that only when canonical correlation is built upon 3 dimensions, the correlations
#are significance. This is equivalent with test for overall significance using 3 dimensions
#Now we discuss about the coefficient for canonical correlation
can.gluc$xcoef
can.gluc$ycoef
#Interpretationfor above can be seen similar to that of regression.
#So for this case, we have 2 sets of regression equations (which is actually the canonical variate) represented
#by table. For each set, we have 3 equations represented by collumns while row is our predictor (in this case
#is the variables themselves). Thus, we can interpret each value just like regression (i.e : each unit
#change of V1 will decrease first equation of set 1 of canonical variate by 0.065)
#We can also used the standardized version of canonical coefficient to see the relative effect of each variable
#and removing the difference in size/scale between variables.
diag(sqrt(diag(cov(gluc_1))))
#Interpretationfor above can be seen similar to that of regression.
#So for this case, we have 2 sets of regression equations (which is actually the canonical variate) represented
#by table. For each set, we have 3 equations represented by collumns while row is our predictor (in this case
#is the variables themselves). Thus, we can interpret each value just like regression (i.e : each unit
#change of V1 will decrease first equation of set 1 of canonical variate by 0.065)
#We can also used the standardized version of canonical coefficient to see the relative effect of each variable
#and removing the difference in size/scale between variables.
diag(sqrt(diag(cov(gluc_1))))%*%can.gluc$xcoef
barplot(can.gluc$cor, main = "Canonical correlation", col ="blue" )
par(mfrow = c(1,1))
barplot(can.gluc$cor, main = "Canonical correlation", col ="blue" )
library(ggplot2)
can.coef = data.frame(rho = can.gluc$cor, dim = c("rho1", "rho2", "rho3"))
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar()
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity") + labs(title = "Canonical correlation")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "bin") + labs(title = "Canonical correlation")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity", fill = "blue") + labs(title = "Canonical correlation")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity", fill = "steel blue") + labs(title = "Canonical correlation")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity", width = 0.9, fill = "steel blue") +
labs(title = "Canonical correlation")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity", width = 0.5, fill = "steel blue") +
labs(title = "Canonical correlation")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity", width = 0.5, fill = "steel blue") +
labs(title = "Canonical correlation", col = "red")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity", width = 0.5, fill = "steel blue") +
labs(title = "Canonical correlation", colour = "cylinders")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity", width = 0.5, fill = "steel blue") +
labs(title = "Canonical correlation", colour = "red")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity", width = 0.5, fill = "steel blue") +
labs(colour = "red")
ggplot(can.coef, aes(x = dim, y = rho)) + geom_bar(stat = "identity", width = 0.5, fill = "steel blue") +
labs(title = "Canonical correlation")
#Interpretationfor above can be seen similar to that of regression.
#So for this case, we have 2 sets of regression equations (which is actually the canonical variate) represented
#by table. For each set, we have 3 equations represented by collumns while row is our predictor (in this case
#is the variables themselves). Thus, we can interpret each value just like regression (i.e : each unit
#change of V1 will decrease first variate of set 1 of canonical variate by 0.065)
#We can also used the standardized version of canonical coefficient to see the relative effect of each variable
#and removing the difference in size/scale between variables.
diag(sqrt(diag(cov(gluc_1))))%*%can.gluc$xcoef
##Using matrix operation to get canonical correlation
#Next, lets break down the formula.
#First, we get the correlation matrix of the data
cor(gluc_1)
cor(gluc_2)
rxy = cor(gluc_1, gluc_2)
ryx = cor(gluc_2, gluc_1)
##Using matrix operation to get canonical correlation
#Next, lets break down the formula.
#First, we get the correlation matrix of the data
rxx = cor(gluc_1)
ryy = cor(gluc_2)
cor(glucose)
rxy
#Following formula from Rencher (2012), we are going to compute multiplication of inverse and the partition
#matrix above. That is : Rxx^-1 * Rxy * Ryy^-1 * Ryx
rho.mat = rxx^-1
rho.mat
#Following formula from Rencher (2012), we are going to compute multiplication of inverse and the partition
#matrix above. That is : Rxx^-1 * Rxy * Ryy^-1 * Ryx
rho.mat = rxx^-1%*%rxy%*%ryy^-1%*%ryx
rho.mat
#Eigen values of the resulting matrix is the squared canonical correlations that we are interested in
eigen(rho.mat)
#Following formula from Rencher (2012), we are going to compute multiplication of inverse and the partition
#matrix above. That is : Rxx^-1 * Rxy * Ryy^-1 * Ryx
rho.mat = solve(rxx)%*%rxy%*%solve(ryy)%*%ryx
#Eigen values of the resulting matrix is the squared canonical correlations that we are interested in
eigen(rho.mat)
#Eigen values of the resulting matrix is the squared canonical correlations that we are interested in
eigen(rho.mat)$values
#Eigen values of the resulting matrix is the squared canonical correlations that we are interested in
sq.rho = eigen(rho.mat)$values #squared canonical correlations
sqrt(sq.rho)
#Testing overall significance can also be made based only upon correlation matrix
wilk.gl = det(cor(glucose))/(det(cor(gluc_1))*det(cor(gluc_2)))
wf = length(glucose)-(dim(gluc_1)[1]+dim(gluc_2)[1]+3)/2
tf = sqrt((dim(gluc_1)[1]^2*dim(gluc_2)[1]^2-4)/(dim(gluc_1)[1]+dim(gluc_2)[1]-5))
tf = sqrt((dim(gluc_1)[1]^2*dim(gluc_2)[1]^2-4)/(dim(gluc_1)[1]^2+dim(gluc_2)[1]^2-5))
df1 = dim(gluc_1)[1]*dim(gluc_2)[1]
wf = length(glucose)-(dim(gluc_1)[2]+dim(gluc_2)[2]+3)/2
tf = sqrt((dim(gluc_1)[2]^2*dim(gluc_2)[2]^2-4)/(dim(gluc_1)[2]^2+dim(gluc_2)[2]^2-5))
df1 = dim(gluc_1)[2]*dim(gluc_2)[2]
df2 = wf*tf-df1/2+1
length(glucose)
wf = dim(glucose)[1]-(dim(gluc_1)[2]+dim(gluc_2)[2]+3)/2
wf = dim(glucose)[1]-(dim(gluc_1)[2]+dim(gluc_2)[2]+3)/2
tf = sqrt((dim(gluc_1)[2]^2*dim(gluc_2)[2]^2-4)/(dim(gluc_1)[2]^2+dim(gluc_2)[2]^2-5))
df1 = dim(gluc_1)[2]*dim(gluc_2)[2]
df2 = wf*tf-df1/2+1
ap.f = (1-wilk.gl^(1/tf))*df2/(wilk.gl^(1/tf)*df1)
pf(ap.f, df1, df2)
qf(ap.f, df1, df2)
wilk.gl
p.asym(can.gluc$cor, dim(gluc_1)[1], length(gluc_1), length(gluc_2),
tstat = "Wilks")
ap.f = (1-wilk.gl^(1/tf))*df2/(wilk.gl^(1/tf)*df1)
pf(ap.f, df1, df2, lower.tail = FALSE)
#We can also use the rho values instead of correlation matrix
wilk.gl = sum(1-eigen(sq.rho))
1-eigen(sq.rho)
1-eigen(sq.rho)$values
eigen(sq.rho)$values
#We can also use the rho values instead of correlation matrix
wilk.gl = sum(1-sq.rho)
#Testing overall significance can also be made based only upon correlation matrix
wilk.gl = det(cor(glucose))/(det(cor(gluc_1))*det(cor(gluc_2)))
#We can also use the rho values instead of correlation matrix
wilk.gl = prod(1-sq.rho)
#Testing overall significance can also be made based only upon correlation matrix
wilk.gl = det(cor(glucose))/(det(cor(gluc_1))*det(cor(gluc_2)))
pillai.gl = sum(sq.rho)
hotel.gl = sum(sq.rho/(1-sq.rho))
roy.gl = max(sq.rho)
p.asym(can.gluc$cor, dim(gluc_1)[1], length(gluc_1), length(gluc_2),
tstat = "Pillai")
p.asym(can.gluc$cor, dim(gluc_1)[1], length(gluc_1), length(gluc_2),
tstat = "Roy")
#For roy test, it is basically only testing significance with 1st dimension.
#Extension for testing succeeding dimensions can be made by modifying the formula based on eigen values
wilk.gl2 = prod(1-sq.rho[,-1])
#For roy test, it is basically only testing significance with 1st dimension.
#Extension for testing succeeding dimensions can be made by modifying the formula based on eigen values
wilk.gl2 = prod(1-sq.rho[-1])
#From the result above, it can be seen that the highest canonical correlation is close to .5
#The next section, the correlations are tested for their significances
p.asym(can.gluc$cor, dim(gluc_1)[1], length(gluc_1), length(gluc_2),
tstat = "Hotelling")#Inputs are : correlation coef, num of observations+variables
#Lets compare with our previous function using tstat wilks
p.asym(can.gluc$cor, dim(gluc_1)[1], length(gluc_1), length(gluc_2),
tstat = "Wilks")
matrix(c(1,.5,1,.5), byrow = T, ncol = 2)
matrix(c(1,.5,1,.5), ncol = 2)
matrix(c(1,.5,.5,1), ncol = 2)
matrix(c(.4,.3,.2,.25), ncol = 2)
matrix(c(1,.6,.6,1), ncol = 2)
matrix(c(.4,.3,.2,.25), byrow = T, ncol = 2)
### Multivariate Regression ###
#Extension from multiple linear regression where we have more than 1 target/dependent variables
#At this point, regression should have been very familiar for almost everyone so lets jump right into modelling
#We'll model glucose data with first 3 variables as x and the others are the target
mlm.gluc = lm(glucose[,1:3] ~ glucose[,4:6])
### Multivariate Regression ###
#Extension from multiple linear regression where we have more than 1 target/dependent variables
#At this point, regression should have been very familiar for almost everyone so lets jump right into modelling
#We'll model glucose data with first 3 variables as x and the others are the target
mlm.gluc = lm(cbind(glucose[,1:3]) ~ glucose[,4:6])
### Multivariate Regression ###
#Extension from multiple linear regression where we have more than 1 target/dependent variables
#At this point, regression should have been very familiar for almost everyone so lets jump right into modelling
#We'll model glucose data with first 3 variables as x and the others are the target
mlm.gluc = lm(formula = cbind(V1, V2, V3) ~ V4 + V5 + V6, data = glucose)
summary(mlm.gluc)
### Multivariate Regression ###
#Extension from multiple linear regression where we have more than 1 target/dependent variables
#At this point, regression should have been very familiar for almost everyone so lets jump right into modelling
#We'll model glucose data with first 3 variables as x and the others are the target
mlm.gluc = lm(formula = cbind(V4, V5, V6) ~ V1 + V2 + V3, data = glucose)
summary(mlm.gluc)
mlm.gluc$coefficients
#Overall test : alternative hypothesis that at least one element of beta matrix is not equal to 0 (of course
#except the beta zero)
wilk.mlgl = det(t(glucose[,4:6])%*%glucose[,4:6])
t(glucose[,4:6])%*%glucose[,4:6]
#Overall test : alternative hypothesis that at least one element of beta matrix is not equal to 0 (of course
#except the beta zero)
glucose1 = as.matrix(glucose)
wilk.mlgl = det(t(glucose1[,4:6])%*%glucose1[,4:6])
#Overall test : alternative hypothesis that at least one element of beta matrix is not equal to 0 (of course
#except the beta zero)
beta1.mlgl =  as.matrix(mlm.gluc$coefficients[2:4,])
View(beta1.mlgl)
wilk.mlgl = det(t(glucose1[,4:6])%*%glucose1[,4:6]-t(beta1.mlgl)%*%t(glucose1[,1:3]%*%glucose1[,4:6])
wilk.mlgl = det(t(glucose1[,4:6])%*%glucose1[,4:6]-t(beta1.mlgl)%*%t(glucose1[,1:3])%*%glucose1[,4:6])
wilk.mlgl = det(t(glucose1[,4:6])%*%glucose1[,4:6]-t(beta1.mlgl)%*%t(glucose1[,1:3])%*%glucose1[,4:6])
xgl = glucose1[,1:3]
ygl = glucose1[,4:6]
dim(glucose1)
ngl = dim(glucose1)[1]
mean(y)
mean(ygl)
apply(ygl, 2, mean)
ybar.gl = as.matrix(apply(ygl, 2, mean))
wilk.mlgl = det(t(y)%*%y-t(beta1.mlgl)%*%t(x)%*%y)/det(t(y)%*%y-ngl*ybar.gl%*%t(ybar.gl))
wilk.mlgl = det(t(ygl)%*%ygl-t(beta1.mlgl)%*%t(xgl)%*%ygl)/det(t(ygl)%*%ygl-ngl*ybar.gl%*%t(ybar.gl))
wilk.mlgl = det(cov(glucose1))/(det(cov(xgl))*det(cov(ygl)))
solve(t(xgl)%*%xgl)%*%t(xgl)%*%ygl
mlm.gluc$coefficients #beta matrix
chem <- read.table("D:/My Drive/Github/Multivariate-Statistics-R/T10_1_CHEM.DAT", quote="\"", comment.char="")
View(chem)
lm(formula = cbind(V2, V3, V4) ~ V5 + V6 + V7, data = chem)
xche = as.matrix(chem[,5:7])
yche = as.matrix(chem[,2:4])
solve(t(xche)%*%xche)%*%t(xche)%*%yche
xche
xche = as.matrix(1,chem[,5:7])
xche
length(chem)
dim(chem)[1]
xche = as.matrix(cbind(rep(1, dim(chem)[1]), chem[,5:7]))
xche
solve(t(xche)%*%xche)%*%t(xche)%*%yche
lm(formula = cbind(V2, V3, V4) ~ V5 + V6 + V7, data = chem)
xgl = cbind(rep(1, dim(glucose1)[1]), glucose1[,1:3])
View(xgl)
xgl = cbind(1 = rep(1, dim(glucose1)[1]), glucose1[,1:3])
xgl = cbind('1' = rep(1, dim(glucose1)[1]), glucose1[,1:3])
solve(t(xgl)%*%xgl)%*%t(xgl)%*%ygl
summary(mlm.gluc)
mlm.gluc$coefficients #beta matrix
solve(t(xgl)%*%xgl)%*%t(xgl)%*%ygl
#Overall test : alternative hypothesis that at least one element of beta matrix is not equal to 0 (of course
#except the beta zero)
beta1.mlgl =  as.matrix(mlm.gluc$coefficients[2:4,])
ybar.gl = as.matrix(apply(ygl, 2, mean))
ngl = dim(glucose1)[1]
wilk.mlgl = det(t(ygl)%*%ygl-t(beta1.mlgl)%*%t(xgl)%*%ygl)/det(t(ygl)%*%ygl-ngl*ybar.gl%*%t(ybar.gl))
beta1.mlgl
#Overall test : alternative hypothesis that at least one element of beta matrix is not equal to 0 (of course
#except the beta zero)
beta1.mlgl =  as.matrix(mlm.gluc$coefficients[1:4,])
ybar.gl = as.matrix(apply(ygl, 2, mean))
ngl = dim(glucose1)[1]
wilk.mlgl = det(t(ygl)%*%ygl-t(beta1.mlgl)%*%t(xgl)%*%ygl)/det(t(ygl)%*%ygl-ngl*ybar.gl%*%t(ybar.gl))
wilk.mlgl = det(cov(glucose1))/(det(cov(xgl))*det(cov(ygl)))
wilk.mlgl = det(t(ygl)%*%ygl-t(beta1.mlgl)%*%t(xgl)%*%ygl)/det(t(ygl)%*%ygl-ngl*ybar.gl%*%t(ybar.gl))
#Overall test : alternative hypothesis that at least one element of beta matrix is not equal to 0 (of course
#except the beta zero)
beta1.mlgl =  as.matrix(mlm.gluc$coefficients)
ybar.gl = as.matrix(apply(ygl, 2, mean))
ngl = dim(glucose1)[1]
egl = det(t(ygl)%*%ygl-t(beta1.mlgl)%*%t(xgl)%*%ygl)
egl = t(ygl)%*%ygl-t(beta1.mlgl)%*%t(xgl)%*%ygl
hgl = t(beta1.mlgl)%*%t(xgl)%*%ygl-ngl*ybar.gl%*%t(ybar.gl)
eh = solve(egl)%*%hgl
eigen(eh)
prod(1/(1+eigen(eh)$values))
wilk.mlgl = det(cov(glucose1))/(det(cov(xgl[,3:4]))*det(cov(ygl)))
xgl[,3:4]
wilk.mlgl = det(cov(glucose1))/(det(cov(xgl[,2:4]))*det(cov(ygl)))
library(FactoMineR)
library(factoextra)
std_adapt <- read.csv("D:/My Drive/Github/Multivariate-Statistics-R/std_adapt.csv")
View(std_adapt)
str(std_adapt)
std_adapt = apply(std_adapt, 2, as.factor)
str(std_adapt)
##Data
# Data used to illustrate this method is from kaggle : students level of adaptability in online education
std_adapt = read.csv("~/Multivariate-Statistics-R/std_adapt.csv")
std_adapt <- read.csv("D:/My Drive/Github/Multivariate-Statistics-R/std_adapt.csv")
str(std_adapt)
std_adapt = apply(std_adapt, 1, as.factor)
std_adapt <- read.csv("D:/My Drive/Github/Multivariate-Statistics-R/std_adapt.csv")
table(std_adapt[,1:2])
# We can present the above data in a contingency table
table(std_adapt[,1:4])
table(std_adapt[,4:8])
table(std_adapt[,4:7])
##CA
# To analyze with CA, we first choose which variables to be included, lets focus on : age, education.level,
#internet type, class duration, and adaptivity level
std_adapt1 = std_adapt[,c(2,3,9,11,14)]
tbl = as.table(as.matrix(std_adapt1))
tbl
balloonplot(t(tbl), main = 'Aspect of students adaptability for online educatio',
xlab = '', ylab = '', label = FALSE, show.margins = FALSE)
library(gplots)
balloonplot(t(tbl), main = 'Aspect of students adaptability for online educatio',
xlab = '', ylab = '', label = FALSE, show.margins = FALSE)
std_adapt1.rownames = std_adapt[,14]
##CA
# To analyze with CA, we first choose which variables to be included, lets focus on : age, education.level,
#internet type, class duration, and adaptivity level
std_adapt1 = std_adapt[,c(2,3,9,11)]
std_adapt1.rownames = std_adapt[,14]
row.names(std_adapt1) = std_adapt1.rownames
##CA
# To analyze with CA, we first choose which variables to be included, lets focus on : age, education.level,
#internet type, class duration, and adaptivity level
std_adapt1 = std_adapt[,c(2,3,9,11,14)]
rm(std_adapt1.rownames)
tbl
library(vcd)
structable(std_adapt[,4:7])
tbl = structable(std_adapt1)
tbl
structable(std_adapt[,c(4:7,14)]) #4-way : location x load x institution x IT
balloonplot(t(tbl), main = 'Aspect of students adaptability for online educatio',
xlab = '', ylab = '', label = FALSE, show.margins = FALSE)
chisq.test(tbl)
chisq.test(table(std_adapt1))
CA(tbl)
CA(table(std_adapt1))
CA(table(std_adapt1), ncp = 2)
CA(table(std_adapt1[,2:4]), ncp = 2)
tbl #Very sparse contingency table
CA(table(std_adapt1[,1:2]))
MCA(std_adapt1)
mc.std = MCA(std_adapt1, graph = F)
fviz_screeplot(mc.std)
fviz_screeplot(mc.std, addlabels = T)
fviz_mca_biplot(mc.std, repel = T)
fviz_mca_var(mc.std, repel = T)
fviz_mca_var(mc.std, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = T)#Plot for each categories in each variables
##CA
# To analyze with CA, we first choose which variables to be included, lets focus on : education.level,
#institution, internet type, class duration, and adaptivity level
std_adapt1 = std_adapt[,c(3,4,9,11,14)]
tbl = structable(std_adapt1)
tbl #Very sparse contingency table
mc.std = MCA(std_adapt1, graph = F)
fviz_screeplot(mc.std, addlabels = T) #First 2 dim represent 35% total inertia (information)
fviz_mca_var(mc.std, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = T)#Plot for each categories in each variables, color shows the quality of representation
#Lets check the distance for each variables' categories
get_mca_var(mc.std)
#Lets check the distance for each variables' categories
mc.std1 = get_mca_var(mc.std)
mc.std1$v.test
mc.std1$coord
dist(mc.std1$coord)
dist(mc.std1$coord[,c(1:3,11:13)])
dist(mc.std1$coord[c(1:3,11:13),])
#From the distance matrix above, we can see a trend where the higher the level of education, relative
#adaptability seems to lean towards higher adaptability level
dist(mc.std1$coord[c(4:5,11:13),])
fviz_mca_var(mc.std, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = T)#Plot for each categories in each variables, color shows the quality of representation
View(std_adapt1)
#Lets use a simple CA first to analyze pair of two variables : adaptability level and education level,
#adaptability level and institution type for the school
sc.std1 = CA(std_adapt1[,c(1,5)])
#Lets use a simple CA first to analyze pair of two variables : adaptability level and education level,
#adaptability level and institution type for the school
sc.std1 = CA(table(std_adapt1[,c(1,5)]))
sc.std2 = CA(table(std_adapt1[,c(2,5)]))
sc.std2 = CA(table(std_adapt1[,c(2,5)]))
sc.std2 = CA(table(std_adapt1[,c(2,5)]), graph = T)
#Lets use a simple CA first to analyze pair of two variables : adaptability level and education level,
#adaptability level and institution type for the school
sc.std1 = CA(table(std_adapt1[,c(1,5)]), graph = F)
sc.std2 = CA(table(std_adapt1[,c(2,5)]), graph = F)
fviz_screeplot(sc.std1, addlabels = T)
fviz_screeplot(sc.std2, addlabels = T)
table(std_adapt1[,c(2,5)])
fviz_mca_var(sc.std1, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = T)
fviz_mca_var(sc.std1, repel = T)
fviz_ca_biplot(sc.std1, repel = T)
fviz_ca_biplot(sc.std2, repel = T)
fviz_ca(sc.std2, repel = T)
#Lets use a simple CA first to analyze pair of two variables : adaptability level and education level
rm(sc.std2)
#Lets use a simple CA first to analyze pair of two variables : adaptability level and education level
sc.std1 = CA(table(std_adapt1[,c(1,5)]), graph = F)
fviz_screeplot(sc.std1, addlabels = T)
fviz_ca_biplot(sc.std1, repel = T)
sc.std1 = get_ca(sc.std1)
sc.std1$coord
sc.std1$inertia
#Lets compare distance with distance matrix
sc.std1 = get_ca_col(sc.std1) #Extract CA result on column
sc.std2 = get_ca_row(sc.std1) #Extract CA result on row
sc.std2
sc.std2 = get_ca_row(sc.std1) #Extract CA result on row
sc.std1$coord
#Lets use a simple CA first to analyze pair of two variables : adaptability level and education level
sc.std1 = CA(table(std_adapt1[,c(1,5)]), graph = F)
fviz_screeplot(sc.std1, addlabels = T) #First dimension represent 88% total inertia (information)
fviz_ca_biplot(sc.std1, repel = T)
#Lets compare distance with distance matrix
sc.std1 = get_ca_col(sc.std1) #Extract CA result on column
sc.std2 = get_ca_row(sc.std1) #Extract CA result on row
#Lets use a simple CA first to analyze pair of two variables : adaptability level and education level
sc.std1 = CA(table(std_adapt1[,c(1,5)]), graph = F)
fviz_screeplot(sc.std1, addlabels = T) #First dimension represent 88% total inertia (information)
fviz_ca_biplot(sc.std1, repel = T)
#Lets compare distance with distance matrix
sc.std2 = get_ca_col(sc.std1) #Extract CA result on column
sc.std3 = get_ca_row(sc.std1) #Extract CA result on row
sc.std3$coord
sc.std2$coord
coord.sc = rbind(sc.std2$coord, sc.std3$coord)
dist(coord.sc)#Distance between adaptability level and education level
dist(mc.std1$coord[c(1:3,11:13),])#Distance between adaptability level and education level
##Multiple correlation
#We'll visualize the correlation within each set between each pair then for both sets at once
pairs.panels(gluc_1)
library(psych)
library(MVN)
library(CCA)
library(CCP)
library(ggplot2)
pairs.panels(glucose) #complete correlation matrix
##Multiple correlation
#We'll visualize the correlation within each set between each pair then for both sets at once
pairs.panels(gluc_1)
pairs.panels(gluc_2)
#Correlation above used pearson correlation coefficient, which means it is only appropriate IF
#the data has normal distribution.
#This assumption is also needed in canonical correlation, the difference is we need a multivariate
#normal assumption instead of univariate one.
#Let's check normality for each variables AND multivariate normality for each set
mvn(gluc_1) #multivariate normality and 2 individuals normality is not accepted
##Canonical correlation
#First we'll use the function from default and CCA then we proceed with breaking down the formula.
cancor(gluc_1, gluc_2) #default function from stats library
can.gluc$cor #Canonical correlation, sorted from highest value
mlm.gluc$coefficients #beta matrix
summary(mlm.gluc)
View(std_adapt)
chisq.test(table(std_adapt[,c(7,9)]))
View(iris)
anova(lm1.iris2)
lm1.iris2 = lm(Sepal.Width ~ Species*soil)
attach(iris)
lm1.iris2 = lm(Sepal.Width ~ Species*soil)
anova(lm1.iris2)
plot(mlm.gluc)
summary(mlm.gluc)
##Multiple correlation
#We'll visualize the correlation within each set between each pair then for both sets at once
pairs.panels(gluc_1)
lm.gluc = lm( V6 ~ V1 + V2 + V3, data = glucose)
summary(lm.gluc)
plot(mlm.gluc)
plot(lm.gluc)

apply(z, 2,mean)
cov(z)
#using equation 3.62 and 3.64
mult311 = matrix(c(1,1,1,2,-3,2,-1,-2,-3), nrow = 3)
mult311
#using equation 3.62 and 3.64
mult311 = matrix(c(1,1,1,2,-3,2,-1,-2,-3), nrow = 3, byrow = T)
mult311
mult311*%*apply(calcium, 2, mean)
#using equation 3.62 and 3.64
mean.calc = matrix(apply(calcium, 2, mean), ncol = 1)
mult311*%*mean.calc
mult311*%*mean.calc
mult311%*%mean.calc
apply(z, 2,mean)
sd.calc = matrix(cov(calcium))
sd.calc = matrix(cov(calcium), ncol = 3)
sd.calc
cov(calcium)
mult311%*%sd.calc%*%t(mult311)
cov(z)
#correlation matrix
diag(cov(z))
#correlation matrix
matrix(diag(cov(z)))
#correlation matrix
matrix(diag(diag(cov(z))))
#correlation matrix
matrix(diag(diag(cov(z))), ncol = 3)
#correlation matrix
matrix(diag(diag(cov(z))), ncol = 3)^-1
#correlation matrix
solve(matrix(diag(diag(cov(z))), ncol = 3))%*%cov(z)%*%solve(matrix(diag(diag(cov(z)))))
#correlation matrix
solve(matrix(diag(diag(cov(z))), ncol = 3))%*%cov(z)%*%solve(matrix(diag(diag(cov(z))), ncol = 3))
cor(z)
solve(matrix(diag(diag(cov(z))), ncol = 3))
matrix(diag(diag(cov(z))), ncol = 3)
mult311%*%sd.calc%*%t(mult311) #sd/covariance matrix
matrix(sqrt(diag(diag(cov(z)))), ncol = 3)
#correlation matrix
solve(matrix(sqrt(diag(diag(cov(z)))), ncol = 3))%*%cov(z)%*%solve(matrix(sqrt(diag(diag(cov(z)))), ncol = 3))
cor(z)
##3.20
mult320 = matrix(c(2,3,-1,4,-2,-1,4,-2,3,-2,-1,3), ncol = 4, byrow = T)
mult320
##3.20
mean.bone = matrix(apply(bone, 2, mean), ncol = 1)
sd.bone = matrix(cov(bone), ncol = 3)
sd.bone = matrix(cov(bone), ncol = 4)
mult320%*%mean.bone
mult320%*%sd.bone%*%t(mult320)
sd.bone
mult311%*%mean.calc #mean
apply(z, 2,mean)
mult311%*%sd.calc%*%t(mult311) #sd/covariance matrix
cov(z)
covz.bone = mult320%*%sd.bone%*%t(mult320)
solve(matrix(sqrt(diag(diag(covz.bone))), ncol = 3))%*%covz.bone%*%solve(matrix(sqrt(diag(diag(covz.bone))), ncol = 3))
cor(mult320%*%bone)
##3.20
bone = matrix(bone, ncol = 4)
View(bone)
bone = read.table("T3_7_BONE.DAT", quote="\"", comment.char="")
bone = bone[,-1]
##3.20
bone = as.matrix(bone)
cor(mult320%*%bone)
mult320%*%bone
bone%*%t(mult320)
cor(bone%*%t(mult320))
solve(matrix(sqrt(diag(diag(covz.bone))), ncol = 3))%*%covz.bone%*%solve(matrix(sqrt(diag(diag(covz.bone))), ncol = 3))
##3.22
apply(glucose, 2, mean)
cov(glucose)
View(glucose)
#build the partition
partition = function(x, rowsep, colsep, ...){
colmissing <- missing(colsep)
rowmissing <- missing(rowsep)
if (rowmissing && colmissing) {
stop("Atleast one of rowsep or colsep args must be specified")
}
if (!rowmissing) {
if (sum(rowsep) != NROW(x)) {
stop("rowsep must sum to the number of columns in x")
}
if (!is.numeric(rowsep)) {
stop("the rowsep vector must be numeric")
}
}
if (!colmissing) {
if (sum(colsep) != NCOL(x)) {
stop("colsep must sum to the number of rows in x")
}
if (!is.numeric(colsep)) {
stop("the colsep vector must be numeric")
}
}
if (!rowmissing) {
set <- lapply(split(seq(NROW(x)), rep(seq(along.with = rowsep),
times = rowsep)), function(index) x[index, , drop = FALSE])
}
else {
set <- NULL
}
if (!colmissing) {
FUN <- function(x) lapply(split(seq(NCOL(x)), rep(seq(along.with = colsep),
times = colsep)), function(index) x[, index, drop = FALSE])
if (is.null(set)) {
FUN(x)
}
else {
lapply(set, FUN)
}
}
else {
set
}
}
partition(glucose, rowsep = c(3,3))
partition(glucose, colsep = c(3,3))
part.gluc = partition(glucose, colsep = c(3,3))
apply(part.gluc$'1', 2,mean)
apply(part.gluc$'2', 2, mean) #second partition mean vectors
part1 = part.gluc$'1'
rm(part1)
part1.gluc = part.gluc$'1'
part2.gluc = part.gluc$'2'
part1.gluc
cov(part1.gluc)
cov(part1.gluc, part2.gluc)
cov(part1.gluc)
cov(part2.gluc, part1.gluc)
cov(part1.gluc)
cov(part1.gluc, part2.gluc)
cov(part2.gluc, part1.gluc)
cov(part2.gluc)
install.packages("JWileymisc")
###multivariate normal distribution###
library(mvtnorm)
library(MVN)
library(JWileymisc)
library(MASS)
library(dplyr)
#in a case where we have the covariance matrix, it is then become a straightforward method to simulate
#MN distribution. Meanwhile in case we have correlation matrix, we use the previous equation to transform
#correlation matrix into covariance matrix.
#library jwileymisc provide the function to simplify this transformation
v = matrix(c(1,.152,.096,.043,.109,.152,1,.400,-.016,.297,.096,.400,1,.092,.382,.043,-.016,.092,1,.103,
.109,.297,.382,.103,1),5,5)
sigma<-c(.4421,1.0880,8.5073,.4700,1.1249) #vector of standard deviation
cor2cov(v, sigma)
cov.matrix = cor2cov(v, sigma)
##Simulating mutivariate normal (MN) distribution based on correlation/covariance matrix, means, and standard deviation
set.seed(100) #setting seed for pseudo-random initiation
#after getting the covariance matrix, we can use mvrnorm to simulate the MN distribution based on
#previous covariance matrix
#in addition we also have to provide the means vector
mu = c(.7337,2.7300,46.9970,2.6002,1.7491)
mvrnorm(cov.matrix, mu)
mvrnorm(n = 2, cov.matrix, mu)
mvrnorm(n = 2, cov.matrix, mu, 5,5)
mvrnorm(n = 200, cov.matrix, mu, 5,5)
mvrnorm(n = 3747, cov.matrix, mu, 5,5)
cov.matrix
mu
mvrnorm(cov.matrix, mu, 5,5)
mvrnorm(cov.matrix, mu)
mvrnorm(Sigma = cov.matrix, mu)
mvrnorm(Sigma = cov.matrix, mu = mu)
mvrnorm(Sigma = cov.matrix, mu = mu)
mvrnorm(Sigma = cov.matrix, mu = mu)
mvrnorm(Sigma = cov.matrix, mu = mu)
mvrnorm(n = 5, Sigma = cov.matrix, mu = mu)
mvrnorm(n = 500, Sigma = cov.matrix, mu = mu) #simulated data with total 500 observations
sim.mn.data = mvrnorm(n = 500, Sigma = cov.matrix, mu = mu) #simulated data with total 500 observations
str(sim.mn.data)
names(sim.mn.data)
sim.mn.data = data.frame(mvrnorm(n = 500, Sigma = cov.matrix, mu = mu)) #simulated data with total 500 observations
str(sim.mn.data)
sim.mn.data = rename(sim.mn.data, low = X1, high = X2, throw = X3, dodge = X4, make = X5)
View(sim.mn.data)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,2], n = 50) #calculate kernel density estimate
sim.mn.kernel$z
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,2], n = 5000) #calculate kernel density estimate
image(sim.mn.kernel)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,2], n = 50) #calculate kernel density estimate
image(sim.mn.kernel)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,2], n = 50) #calculate kernel density estimate
image(sim.mn.kernel)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,2], n = 100) #calculate kernel density estimate
image(sim.mn.kernel)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,2], n = 150) #calculate kernel density estimate
image(sim.mn.kernel)
contour(sim.mn.data, add = T)
contour(sim.mn.kernel, add = T)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,3], n = 150) #calculate kernel density estimate
image(sim.mn.kernel)
contour(sim.mn.kernel, add = T)
library(ellipse)
rho = cor(sim.mn.data[,1:2])
y_on_x = lm(bivn[,2] ~ bivn[,1])    # Regression Y ~ X
sim.mn.data
y_on_x = lm(sim.mn.data, high ~ low)    # Regression Y ~ X
y_on_x = lm(high ~ low, sim.mn.data)    # Regression Y ~ X
x_on_y = lm(low ~ high, sim.mn.data)    # Regression X ~ Y
plot_legend = c("99% CI green", "95% CI red","90% CI blue",
"Y on X black", "X on Y brown")
plot(sim.mn.data[,1:2], xlab = "low", ylab = "high",
col = "dark blue",
main = "Bivariate Normal with Confidence Intervals")
lines(ellipse(rho), col="red")       # ellipse() from ellipse package
lines(ellipse(rho, level = .99), col="green")
lines(ellipse(rho, level = .90), col="blue")
abline(y_on_x)
abline(x_on_y, col="brown")
legend(3,1,legend=plot_legend,cex = .5, bty = "n")
#next is the bivariate normal distribution which
rho
ellipse(rho)
#3d representation
persp(sim.mn.kernel[,1:2], phi = 45, theta = 30, shade = .1, border = NA) # from base graphics package
#3d representation
persp(sim.mn.kernel, phi = 45, theta = 30, shade = .1, border = NA) # from base graphics package
library(rgl)
#using interactive plot from RGL library
color2 = heat.colors(length(sim.mn.kernel$z))[rank(sim.mn.kernel$z)]
persp3d(x=sim.mn.kernel, col = color2)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#since we are limited in 3dimensional representation, we use only first 2 variables of the simulated
#data from before.
#it is guaranteed that marginal distributions from MN is always normal(see Methods_of_Multivariate_Analysis-_3rd_Edition Rencher & Christensen )
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,3], n = 150) #calculate kernel density estimate
persp3d(x=sim.mn.kernel, col = color2)
##Multivariate normality test
#there are several methods in which each of them has unique characteristics and may be better off used
#on certain type of data.
#mardia
mvn(iris, mvnTest = "mardia")
#perspective andn contour plot
mvn(iris[,2:3], mvnTest = "hz", multivariatePlot = "persp")
##4.24
hematol = read.table("T4_3_HEMATOL.DAT")
###rename the data as you wish, in here we rename to "hematol"
View(hematol)
View(probe)
## 4.21
probe = read.table("T3_6_PROBE.DAT")
probe = probe[,-1]
qqnorm(probe$V2)
qqnorm(probe$V2)
qqnorm(probe$V3)
qqnorm(probe$V4)
qqnorm(probe$V5)
qqnorm(probe$V6)
library(moments)
# skewness and kurtosis
skewness(probe)
kurtosis(probe)
#kurtosis<3 means the distribution is flatter than the normal distribution while kurtosis>3 will have
#higher peak than normal distribution
###Skewness and Kurtosis Representation###
set.seed(5)
# normal
x = rnorm(1000, 0,1)
hist(x, main="Normal: Symmetrical", freq=FALSE)
lines(density(x), col='red', lwd=3)
abline(v = c(mean(x),median(x)),  col=c("green", "blue"), lty=c(2,2), lwd=c(3, 3))
# exponential (positive skewness)
x = rexp(1000,1)
hist(x, main="Exponential: Positive Skew", freq=FALSE)
lines(density(x), col='red', lwd=3)
abline(v = c(mean(x),median(x)),  col=c("green", "blue"), lty=c(2,2), lwd=c(3, 3))
# beta (negative skewness)
x= rbeta(10000,5,2)
hist(x, main="Beta: Negative Skew", freq=FALSE)
lines(density(x), col='red', lwd=3)
abline(v = c(mean(x),median(x)),  col=c("green", "blue"), lty=c(2,2), lwd=c(3, 3))
agostino.test(probe)
agostino.test(probe$V2)
#d'agostino test
agostino.test
#d'agostino test
agostino.test(probe$V2)
agostino.test(probe$V3)
agostino.test(probe$V4)
agostino.test(probe$V5)
agostino.test(probe$V6)
#lin-mudholkar test
lin.mudholkar.normality.test.simple = function(sample.r
,sample.size
,method = c("lin-mudholkar-1980")
,alternative = c("two.sided","less","greater")
,conf.level = 0.95
) {
validate.htest.alternative(alternative = alternative)
n = sample.size
r = sample.r
Y = -11.7/n + 55.06/n^2
S = sqrt(3/n - 7.324/n^2 + 53.005/n^3)
a = 24/Y - 3
c = .5 * log((1+r)/(1-r))
b = (-24 * c) / (S*Y)
if (n < 4) {
roots = rep(NA,3)
} else {
roots = polyroot(c(b,a,0,1))
}
roots[which(Im(roots) == 0)]
roots = Re(roots)
roots = roots[order(abs(roots))]
z = roots[1]
p.value = if (alternative[1] == "two.sided") {
tmp = pnorm(z)
min(tmp,1-tmp)*2
} else if (alternative[1] == "greater") {
pnorm(z,lower.tail = FALSE)
} else if (alternative[1] == "less") {
pnorm(z,lower.tail = TRUE)
} else {
NA
}
retval = list(data.name   = "input data",
statistic   = z,
estimate    = c(sample.size = n,
r = r, b = b, a = a, S = S, Y = Y, root.1 = roots[1],
root.2 = roots[2],root.3 = roots[3]),
parameter   = 0,
p.value     = p.value,
null.value  = 0,
alternative = alternative[1],
method      = "Lin-Mudholkar Normality Test"
)
names(retval$statistic) = "z statistic"
names(retval$null.value) = "z"
names(retval$parameter) = "null hypothesis z"
class(retval) = "htest"
retval
}
lin.mudholkar.normality.test.simple(probe$V2, 10)
install.packages("BurStMisc")
#lin-mudholkar test
validate.htest.alternative <- function(alternative, allowed = c("two.sided", "less", "greater")) {
if (length(alternative) < 1) {
stop("Empty alternative parameter")
}
if (length(alternative) > 1) {
alternative <- alternative[1]
}
if (is.na(alternative)) {
stop("alternative parameter is NA")
}
if (!is.character(alternative)) {
stop("alternative parameter is not a character string")
}
if (!(alternative %in% allowed)) {
stop(paste("alternative parameter is invalid. Use", paste(shQuote(allowed, type="cmd"), collapse = ", ") ))
}
}
lin.mudholkar.normality.test.simple = function(sample.r
,sample.size
,method = c("lin-mudholkar-1980")
,alternative = c("two.sided","less","greater")
,conf.level = 0.95)
{
validate.htest.alternative(alternative = alternative)
n = sample.size
r = sample.r
Y = -11.7/n + 55.06/n^2
S = sqrt(3/n - 7.324/n^2 + 53.005/n^3)
a = 24/Y - 3
c = .5 * log((1+r)/(1-r))
b = (-24 * c) / (S*Y)
if (n < 4) {
roots = rep(NA,3)
} else {
roots = polyroot(c(b,a,0,1))
}
roots[which(Im(roots) == 0)]
roots = Re(roots)
roots = roots[order(abs(roots))]
z = roots[1]
p.value = if (alternative[1] == "two.sided") {
tmp = pnorm(z)
min(tmp,1-tmp)*2
} else if (alternative[1] == "greater") {
pnorm(z,lower.tail = FALSE)
} else if (alternative[1] == "less") {
pnorm(z,lower.tail = TRUE)
} else {
NA
}
retval = list(data.name   = "input data",
statistic   = z,
estimate    = c(sample.size = n,
r = r, b = b, a = a, S = S, Y = Y, root.1 = roots[1],
root.2 = roots[2],root.3 = roots[3]),
parameter   = 0,
p.value     = p.value,
null.value  = 0,
alternative = alternative[1],
method      = "Lin-Mudholkar Normality Test"
)
names(retval$statistic) = "z statistic"
names(retval$null.value) = "z"
names(retval$parameter) = "null hypothesis z"
class(retval) = "htest"
retval
}
lin.mudholkar.normality.test.simple(probe$V2, 10)
lin.mudholkar.normality.test.simple(probe$V2)
lin.mudholkar.normality.test.simple(probe$V2, 1000)
lin.mudholkar.normality.test.simple(probe$V2, 11)
lin.mudholkar.normality.test.simple(probe$V2, 1)
lin.mudholkar.normality.test.simple(probe$V2, 2)
lin.mudholkar.normality.test.simple(probe$V2, 3)
lin.mudholkar.normality.test.simple(probe$V3, 3)
lin.mudholkar.normality.test.simple(probe$V4, 3)
lin.mudholkar.normality.test.simple(probe$V4, 4)
lin.mudholkar.normality.test.simple(probe$V4, 2)
lin.mudholkar.normality.test.simple(probe$V5, 2)
lin.mudholkar.normality.test.simple(probe$V6, 2)
lin.mudholkar.normality.test.simple(probe$V6, 1)
lin.mudholkar.normality.test.simple <- function(sample.r
,sample.size
,method = c("lin-mudholkar-1980")
,alternative = c("two.sided","less","greater")
,conf.level = 0.95
) {
validate.htest.alternative(alternative = alternative)
n <- sample.size
r <- sample.r
Y <- -11.7/n + 55.06/n^2
S <- sqrt(3/n - 7.324/n^2 + 53.005/n^3)
a <- 24/Y - 3
c <- .5 * log((1+r)/(1-r))
b <- (-24 * c) / (S*Y)
if (n < 4) {
roots <-rep(NA,3)
} else {
roots <- polyroot(c(b,a,0,1))
}
roots[which(Im(roots) == 0)]
roots <- Re(roots)
roots <- roots[order(abs(roots))]
z <- roots[1]
p.value <- if (alternative[1] == "two.sided") {
tmp<-pnorm(z)
min(tmp,1-tmp)*2
} else if (alternative[1] == "greater") {
pnorm(z,lower.tail = FALSE)
} else if (alternative[1] == "less") {
pnorm(z,lower.tail = TRUE)
} else {
NA
}
retval<-list(data.name   = "input data",
statistic   = z,
estimate    = c(sample.size = n
,r = r
,b = b
,a = a
,S = S
,Y = Y
,root.1 = roots[1]
,root.2 = roots[2]
,root.3 = roots[3]
),
parameter   = 0,
p.value     = p.value,
null.value  = 0,
alternative = alternative[1],
method      = "Lin-Mudholkar Normality Test"
)
names(retval$statistic) <- "z statistic"
names(retval$null.value) <- "z"
names(retval$parameter) <- "null hypothesis z"
class(retval)<-"htest"
retval
}
lin.mudholkar.normality.test.simple(probe$V6, 1)
lin.mudholkar.normality.test.simple(probe$V6, 0)
lin.mudholkar.normality.test.simple(probe$V6, 0)
lin.mudholkar.normality.test.simple(probe$V6, 4)
lin.mudholkar.normality.test.simple(probe$V6, 5)
lin.mudholkar.normality.test.simple(probe$V6, 6)
lin.mudholkar.normality.test.simple(probe$V6, 3)
lin.mudholkar.normality.test.simple(probe$V6, 2)

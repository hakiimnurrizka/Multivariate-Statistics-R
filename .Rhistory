library(RColorBrewer)
###data
data("iris")
iris = as.matrix(iris[, 1:4])
###plot data
scatterplotMatrix(iris)
##profile plot
makeProfilePlot <- function(mylist,names)
{
require(RColorBrewer)
# find out how many variables we want to include
numvariables <- length(mylist)
# choose 'numvariables' random colours
colours <- brewer.pal(numvariables,"Set1")
# find out the minimum and maximum values of the variables:
mymin <- 1e+20
mymax <- 1e-20
for (i in 1:numvariables)
{
vectori <- mylist[[i]]
mini <- min(vectori)
maxi <- max(vectori)
if (mini < mymin) { mymin <- mini }
if (maxi > mymax) { mymax <- maxi }
}
# plot the variables
for (i in 1:numvariables)
{
vectori <- mylist[[i]]
namei <- names[i]
colouri <- colours[i]
if (i == 1) { plot(vectori,col=colouri,type="l",ylim=c(mymin,mymax)) }
else         { points(vectori, col=colouri,type="l")                                     }
lastxval <- length(vectori)
lastyval <- vectori[length(vectori)]
text((lastxval-10),(lastyval),namei,col="black",cex=0.6)
}
}
iris_list = list(iris[,1],iris[,2],iris[,3],iris[,4])
makeProfilePlot(iris_list, names(iris[1,]))
##Covariance and correlation on multivariate variable
#scatterplot data
scatterplotMatrix(iris)
#from the scatterplot, subjectively, there are pairs of variables which shown a trend of covary
#covary on multivariate data can be described as : tendency to create pattern relative to each variables mean
#for instance : petal length and petal width which create a similar data point spread around each of their means
#to help on looking on this pattern, profile plot can be used
#profile plot
makeProfilePlot = function(list,name){
require(RColorBrewer)
# finding how many variables to include
numvar = length(list)
# choose 'numvar' random colours
colours = brewer.pal(numvar,"Set1")
# finding the minimum and maximum values of the variables:
mymin = 1e+20
mymax = 1e-20
for (i in 1:numvar)
{
vectori <- list[[i]]
mini = min(vectori)
maxi = max(vectori)
if (mini < mymin) { mymin = mini }
if (maxi > mymax) { mymax = maxi }
}
# plot the variables
for (i in 1:numvar)
{
vectori <- list[[i]]
namei <- name[i]
colouri <- colours[i]
if (i == 1) { plot(vectori,col=colouri,type="l",ylim=c(mymin,mymax)) }
else         { points(vectori, col=colouri,type="l")                                     }
lastxval <- length(vectori)
lastyval <- vectori[length(vectori)]
text((lastxval-10),(lastyval),namei,col="black",cex=0.6)
}
}
#from the scatterplot, subjectively, there are pairs of variables which shown a trend of covary
#covary on multivariate data can be described as : tendency to create pattern relative to each variables mean
#for instance : petal length and petal width which create a similar data point spread around each of their means
#to help on looking on this pattern, profile plot can be used
#profile plot
Profile_Plot = function(list,name){
require(RColorBrewer)
# finding how many variables to include
numvar = length(list)
# choose 'numvar' random colours
colours = brewer.pal(numvar,"Set1")
# finding the minimum and maximum values of the variables:
mymin = 1e+20
mymax = 1e-20
for (i in 1:numvar)
{
vectori <- list[[i]]
mini = min(vectori)
maxi = max(vectori)
if (mini < mymin) { mymin = mini }
if (maxi > mymax) { mymax = maxi }
}
# plot the variables
for (i in 1:numvar)
{
vectori <- list[[i]]
namei <- name[i]
colouri <- colours[i]
if (i == 1) { plot(vectori,col=colouri,type="l",ylim=c(mymin,mymax)) }
else         { points(vectori, col=colouri,type="l")                                     }
lastxval <- length(vectori)
lastyval <- vectori[length(vectori)]
text((lastxval-10),(lastyval),namei,col="black",cex=0.6)
}
}
rm(makeProfilePlot())
rm(makeProfilePlot)
Profile_Plot(iris_list, names(iris[1,]))
###mean n cov
##mean per collumn
n = dim(iris)[1]
j = as.matrix(rep(1, dim(iris)[1]))
mean_iris = 1/n*t(iris)%*%j #  calculate means of all columns
mean_iris
apply(iris, 2, mean)
##cov matrix
mean_matrix = matrix(data = 1, nrow = n)%*%cbind(mean_iris[[1]], mean_iris[[2]], mean_iris[[3]], mean_iris[[4]])
head(mean_matrix)
cov = 1/(n-1)*t(iris-mean_matrix)%*%(iris-mean_matrix)
cov
cov(iris)
##cor matrix
diag(cov)
d = diag(diag(cov)^(-1/2))
d
cor = d%*%cov%*%d
cor
#from the scatterplot, subjectively, there are pairs of variables which shown a trend of covary
#covary on multivariate data can be described as : tendency to create pattern relative to each variables mean
#for instance : petal length and petal width which create a similar data point spread around each of their means
plot(iris[,3], iris[,4])
iris
###Characterizing and displaying multivariate data###
library(car)
library(RColorBrewer)
cor(iris)#simplified using function from stats library
##apply to food texture data
read.csv('food-texture.csv')
##apply to food texture data
food_text = read.csv('food-texture.csv')
food_text = food_text[,-1]
#scatterplot food texture data
scatterplotMatrix(food_text)
plot(food_text[,1], food_text[,2])
food_text-list = list(food_text)
food_text_list = list(food_text)
food_text_list
Profile_Plot(food_text_list)
food_text_list = list(food_text[,1], food_text[,2], food_text[,3], food_text[,4], food_text[,5])
Profile_Plot(food_text_list)
food_text
food_text[1,]
names(food_text)
Profile_Plot(food_text_list, names(food_text))
food_text_list = list(food_text[,1], food_text[,3], food_text[,4], food_text[,5])
Profile_Plot(food_text_list, names(food_text))
food_text_list = list(food_text[,1], food_text[,2], food_text[,3], food_text[,4], food_text[,5])
Profile_Plot(food_text_list, names(food_text))
food_text_list.2 = list(food_text[,1], food_text[,3], food_text[,4], food_text[,5])
Profile_Plot(food_text_list2, names(food_text))#density has high value's gap than the rest
Profile_Plot(food_text_list.2, names(food_text))#density has high value's gap than the rest
Profile_Plot(food_text_list.2, names(food_text[,-2]))#density has high value's gap than the rest
food_text_list.3 = list(food_text[,1], food_text[,3], food_text[,4])
Profile_Plot(food_text_list.3, names(food_text[,-2||-5]))#hardness has high value's gap than the rest
Profile_Plot(food_text_list.3, names(food_text[,-2&&-5]))#hardness has high value's gap than the rest
Profile_Plot(food_text_list.3, names(food_text[,-c(2,5)]))#hardness has high value's gap than the rest
#Mean and sd
sapply(food_text, mean)
apply(food_text, 2, mean)
#Mean and sd
lapply(food_text, mean)
sapply(food_text, sd)
#covariance and correlation matrix
cov(food_text)
cor(food_text)
calcium <- read.table("G:/My Drive/Github/Multivariate-Statistics-R/T3_4_CALCIUM.DAT", quote="\"", comment.char="")
View(calcium)
###Exercise from Methods_of_Multivariate_Analysis-_3rd_Edition Rencher & Christensen###
###chapter 3
calcium <- read.table("T3_4_CALCIUM.DAT", quote="\"", comment.char="")
rm(calcium)
###Exercise from Methods_of_Multivariate_Analysis-_3rd_Edition Rencher & Christensen###
###chapter 3
calcium = read.table("T3_4_CALCIUM.DAT", quote="\"", comment.char="")
bone = read.table("T3_7_BONE.DAT", quote="\"", comment.char="")
glucose = read.table("T3_9_GLUCOSE.DAT", quote="\"", comment.char="")
glucose = read.table("T3_9_GLUCOSE.DAT", quote="\"", comment.char="")
glucose = read.table("T3_9_GLUCOSE.DAT", quote="\"", comment.char="")
glucose <- read.table("G:/My Drive/Github/Multivariate-Statistics-R/T3_9_GLUCOSE.DAT", quote="\"", comment.char="")
View(glucose)
glucose = read.table("T3_9_GLUCOSE.DAT", quote="\"", comment.char="")
###Exercise from Methods_of_Multivariate_Analysis-_3rd_Edition Rencher & Christensen###
###chapter 3
calcium = read.table("T3_4_CALCIUM.DAT", quote="\"", comment.char="")
bone = read.table("T3_7_BONE.DAT", quote="\"", comment.char="")
glucose = read.table("T3_9_GLUCOSE.DAT", quote="\"", comment.char="")
probe = read.table("T3_6_PROBE.DAT", quote="\"", comment.char="")
cov(probe)
probe = probe[,-1]
View(probe)
cov(probe)
## 3.11
#covariance matrix
cov(calcium)
View(glucose)
View(bone)
bone = bone[,-1]
det(cov(calcium))
tr(cov(calcium))
trace(cov(calcium))
install.packages("fBasics")
###Exercise from Methods_of_Multivariate_Analysis-_3rd_Edition Rencher & Christensen###
library(fBasics)
tr(cov(calcium))
## 3.11
cov(calcium)#covariance matrix
View(calcium)
calcium = calcium[,-1]
## 3.11
cov(calcium)#covariance matrix
det(cov(calcium))#generalized sample variance
## 3.17
z = as.matrix(sapply(calcium, sum))
z
## 3.17
z = as.matrix(apply(calcium,1, sum))
z
## 3.17
z = as.matrix(c(apply(calcium,1, sum), 2*calcium[,1]-3*calcium[,2]+2*calcium[,3]))
z
## 3.17
z = matrix(c(apply(calcium,1, sum), 2*calcium[,1]-3*calcium[,2]+2*calcium[,3], -calcium[,1]-2*calcium[,2]-3*calcium[,3]),
ncol = 3)
z
sapply(z, mean)
apply(z, 2,mean)
cov(z)
#using equation 3.62 and 3.64
mult311 = matrix(c(1,1,1,2,-3,2,-1,-2,-3), nrow = 3)
mult311
#using equation 3.62 and 3.64
mult311 = matrix(c(1,1,1,2,-3,2,-1,-2,-3), nrow = 3, byrow = T)
mult311
mult311*%*apply(calcium, 2, mean)
#using equation 3.62 and 3.64
mean.calc = matrix(apply(calcium, 2, mean), ncol = 1)
mult311*%*mean.calc
mult311*%*mean.calc
mult311%*%mean.calc
apply(z, 2,mean)
sd.calc = matrix(cov(calcium))
sd.calc = matrix(cov(calcium), ncol = 3)
sd.calc
cov(calcium)
mult311%*%sd.calc%*%t(mult311)
cov(z)
#correlation matrix
diag(cov(z))
#correlation matrix
matrix(diag(cov(z)))
#correlation matrix
matrix(diag(diag(cov(z))))
#correlation matrix
matrix(diag(diag(cov(z))), ncol = 3)
#correlation matrix
matrix(diag(diag(cov(z))), ncol = 3)^-1
#correlation matrix
solve(matrix(diag(diag(cov(z))), ncol = 3))%*%cov(z)%*%solve(matrix(diag(diag(cov(z)))))
#correlation matrix
solve(matrix(diag(diag(cov(z))), ncol = 3))%*%cov(z)%*%solve(matrix(diag(diag(cov(z))), ncol = 3))
cor(z)
solve(matrix(diag(diag(cov(z))), ncol = 3))
matrix(diag(diag(cov(z))), ncol = 3)
mult311%*%sd.calc%*%t(mult311) #sd/covariance matrix
matrix(sqrt(diag(diag(cov(z)))), ncol = 3)
#correlation matrix
solve(matrix(sqrt(diag(diag(cov(z)))), ncol = 3))%*%cov(z)%*%solve(matrix(sqrt(diag(diag(cov(z)))), ncol = 3))
cor(z)
##3.20
mult320 = matrix(c(2,3,-1,4,-2,-1,4,-2,3,-2,-1,3), ncol = 4, byrow = T)
mult320
##3.20
mean.bone = matrix(apply(bone, 2, mean), ncol = 1)
sd.bone = matrix(cov(bone), ncol = 3)
sd.bone = matrix(cov(bone), ncol = 4)
mult320%*%mean.bone
mult320%*%sd.bone%*%t(mult320)
sd.bone
mult311%*%mean.calc #mean
apply(z, 2,mean)
mult311%*%sd.calc%*%t(mult311) #sd/covariance matrix
cov(z)
covz.bone = mult320%*%sd.bone%*%t(mult320)
solve(matrix(sqrt(diag(diag(covz.bone))), ncol = 3))%*%covz.bone%*%solve(matrix(sqrt(diag(diag(covz.bone))), ncol = 3))
cor(mult320%*%bone)
##3.20
bone = matrix(bone, ncol = 4)
View(bone)
bone = read.table("T3_7_BONE.DAT", quote="\"", comment.char="")
bone = bone[,-1]
##3.20
bone = as.matrix(bone)
cor(mult320%*%bone)
mult320%*%bone
bone%*%t(mult320)
cor(bone%*%t(mult320))
solve(matrix(sqrt(diag(diag(covz.bone))), ncol = 3))%*%covz.bone%*%solve(matrix(sqrt(diag(diag(covz.bone))), ncol = 3))
##3.22
apply(glucose, 2, mean)
cov(glucose)
View(glucose)
#build the partition
partition = function(x, rowsep, colsep, ...){
colmissing <- missing(colsep)
rowmissing <- missing(rowsep)
if (rowmissing && colmissing) {
stop("Atleast one of rowsep or colsep args must be specified")
}
if (!rowmissing) {
if (sum(rowsep) != NROW(x)) {
stop("rowsep must sum to the number of columns in x")
}
if (!is.numeric(rowsep)) {
stop("the rowsep vector must be numeric")
}
}
if (!colmissing) {
if (sum(colsep) != NCOL(x)) {
stop("colsep must sum to the number of rows in x")
}
if (!is.numeric(colsep)) {
stop("the colsep vector must be numeric")
}
}
if (!rowmissing) {
set <- lapply(split(seq(NROW(x)), rep(seq(along.with = rowsep),
times = rowsep)), function(index) x[index, , drop = FALSE])
}
else {
set <- NULL
}
if (!colmissing) {
FUN <- function(x) lapply(split(seq(NCOL(x)), rep(seq(along.with = colsep),
times = colsep)), function(index) x[, index, drop = FALSE])
if (is.null(set)) {
FUN(x)
}
else {
lapply(set, FUN)
}
}
else {
set
}
}
partition(glucose, rowsep = c(3,3))
partition(glucose, colsep = c(3,3))
part.gluc = partition(glucose, colsep = c(3,3))
apply(part.gluc$'1', 2,mean)
apply(part.gluc$'2', 2, mean) #second partition mean vectors
part1 = part.gluc$'1'
rm(part1)
part1.gluc = part.gluc$'1'
part2.gluc = part.gluc$'2'
part1.gluc
cov(part1.gluc)
cov(part1.gluc, part2.gluc)
cov(part1.gluc)
cov(part2.gluc, part1.gluc)
cov(part1.gluc)
cov(part1.gluc, part2.gluc)
cov(part2.gluc, part1.gluc)
cov(part2.gluc)
install.packages("JWileymisc")
###multivariate normal distribution###
library(mvtnorm)
library(MVN)
library(JWileymisc)
library(MASS)
library(dplyr)
#in a case where we have the covariance matrix, it is then become a straightforward method to simulate
#MN distribution. Meanwhile in case we have correlation matrix, we use the previous equation to transform
#correlation matrix into covariance matrix.
#library jwileymisc provide the function to simplify this transformation
v = matrix(c(1,.152,.096,.043,.109,.152,1,.400,-.016,.297,.096,.400,1,.092,.382,.043,-.016,.092,1,.103,
.109,.297,.382,.103,1),5,5)
sigma<-c(.4421,1.0880,8.5073,.4700,1.1249) #vector of standard deviation
cor2cov(v, sigma)
cov.matrix = cor2cov(v, sigma)
##Simulating mutivariate normal (MN) distribution based on correlation/covariance matrix, means, and standard deviation
set.seed(100) #setting seed for pseudo-random initiation
#after getting the covariance matrix, we can use mvrnorm to simulate the MN distribution based on
#previous covariance matrix
#in addition we also have to provide the means vector
mu = c(.7337,2.7300,46.9970,2.6002,1.7491)
mvrnorm(cov.matrix, mu)
mvrnorm(n = 2, cov.matrix, mu)
mvrnorm(n = 2, cov.matrix, mu, 5,5)
mvrnorm(n = 200, cov.matrix, mu, 5,5)
mvrnorm(n = 3747, cov.matrix, mu, 5,5)
cov.matrix
mu
mvrnorm(cov.matrix, mu, 5,5)
mvrnorm(cov.matrix, mu)
mvrnorm(Sigma = cov.matrix, mu)
mvrnorm(Sigma = cov.matrix, mu = mu)
mvrnorm(Sigma = cov.matrix, mu = mu)
mvrnorm(Sigma = cov.matrix, mu = mu)
mvrnorm(Sigma = cov.matrix, mu = mu)
mvrnorm(n = 5, Sigma = cov.matrix, mu = mu)
mvrnorm(n = 500, Sigma = cov.matrix, mu = mu) #simulated data with total 500 observations
sim.mn.data = mvrnorm(n = 500, Sigma = cov.matrix, mu = mu) #simulated data with total 500 observations
str(sim.mn.data)
names(sim.mn.data)
sim.mn.data = data.frame(mvrnorm(n = 500, Sigma = cov.matrix, mu = mu)) #simulated data with total 500 observations
str(sim.mn.data)
sim.mn.data = rename(sim.mn.data, low = X1, high = X2, throw = X3, dodge = X4, make = X5)
View(sim.mn.data)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,2], n = 50) #calculate kernel density estimate
sim.mn.kernel$z
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,2], n = 5000) #calculate kernel density estimate
image(sim.mn.kernel)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,2], n = 50) #calculate kernel density estimate
image(sim.mn.kernel)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,2], n = 50) #calculate kernel density estimate
image(sim.mn.kernel)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,2], n = 100) #calculate kernel density estimate
image(sim.mn.kernel)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,2], n = 150) #calculate kernel density estimate
image(sim.mn.kernel)
contour(sim.mn.data, add = T)
contour(sim.mn.kernel, add = T)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,3], n = 150) #calculate kernel density estimate
image(sim.mn.kernel)
contour(sim.mn.kernel, add = T)
library(ellipse)
rho = cor(sim.mn.data[,1:2])
y_on_x = lm(bivn[,2] ~ bivn[,1])    # Regression Y ~ X
sim.mn.data
y_on_x = lm(sim.mn.data, high ~ low)    # Regression Y ~ X
y_on_x = lm(high ~ low, sim.mn.data)    # Regression Y ~ X
x_on_y = lm(low ~ high, sim.mn.data)    # Regression X ~ Y
plot_legend = c("99% CI green", "95% CI red","90% CI blue",
"Y on X black", "X on Y brown")
plot(sim.mn.data[,1:2], xlab = "low", ylab = "high",
col = "dark blue",
main = "Bivariate Normal with Confidence Intervals")
lines(ellipse(rho), col="red")       # ellipse() from ellipse package
lines(ellipse(rho, level = .99), col="green")
lines(ellipse(rho, level = .90), col="blue")
abline(y_on_x)
abline(x_on_y, col="brown")
legend(3,1,legend=plot_legend,cex = .5, bty = "n")
#next is the bivariate normal distribution which
rho
ellipse(rho)
#3d representation
persp(sim.mn.kernel[,1:2], phi = 45, theta = 30, shade = .1, border = NA) # from base graphics package
#3d representation
persp(sim.mn.kernel, phi = 45, theta = 30, shade = .1, border = NA) # from base graphics package
library(rgl)
#using interactive plot from RGL library
color2 = heat.colors(length(sim.mn.kernel$z))[rank(sim.mn.kernel$z)]
persp3d(x=sim.mn.kernel, col = color2)
##visualizing the multivariate normal data
#after generating MN data, we can visualize the data
#since we are limited in 3dimensional representation, we use only first 2 variables of the simulated
#data from before.
#it is guaranteed that marginal distributions from MN is always normal(see Methods_of_Multivariate_Analysis-_3rd_Edition Rencher & Christensen )
#2d contour
sim.mn.kernel = kde2d(sim.mn.data[,1], sim.mn.data[,3], n = 150) #calculate kernel density estimate
persp3d(x=sim.mn.kernel, col = color2)
##Multivariate normality test
#there are several methods in which each of them has unique characteristics and may be better off used
#on certain type of data.
#mardia
mvn(iris, mvnTest = "mardia")
#perspective andn contour plot
mvn(iris[,2:3], mvnTest = "hz", multivariatePlot = "persp")
##4.24
hematol = read.table("T4_3_HEMATOL.DAT")
###rename the data as you wish, in here we rename to "hematol"
View(hematol)
